# 데이터 정규화의 필요성과 통계 분석에서의 역할

**작성일**: 2025.09.09  
**목적**: 통계 분석에서 데이터 정규화가 필요한 이유와 그 역할에 대한 체계적 설명

---

## 1. 데이터 정규화의 정의와 목적

데이터 정규화(Data Normalization) 또는 피처 스케일링(Feature Scaling)은 서로 다른 단위와 규모를 가진 변수들을 동일한 범위로 변환하여, 각 변수가 가진 정보와 패턴이 동등한 조건에서 분석에 기여하도록 보장하는 과정이다.

변수 간의 값 범위에 큰 차이가 존재하면, 알고리즘은 더 중요해서가 아니라 단순히 값이 크다는 이유만으로 해당 변수에 더 높은 가중치를 부여하게 된다. 이로 인해 분석 결과가 왜곡되거나 모델의 성능이 저하될 수 있다.

---

## 2. 신뢰성 보증: "Garbage In, Garbage Out" 원칙

데이터 분석의 제1 원칙은 "Garbage In, Garbage Out"이다. 분석 모델이 아무리 정교하더라도, 입력되는 데이터의 품질이 낮으면 그 결과는 신뢰할 수 없다.

데이터 전처리 과정은 분석에 사용된 데이터가 품질 문제가 없음을 증명하는 데이터 품질 보증(Data Quality Assurance) 과정을 기록하는 역할을 한다. 결측값이나 데이터 불일치가 없었음을 확인하고, 모든 데이터를 일관된 기준으로 처리했음을 보여줌으로써 최종 분석 결과의 신뢰도를 근본적으로 뒷받침한다.

---

## 3. 재현성 확보: 과학적 방법론의 핵심

분석의 재현성은 과학적 방법론의 핵심 요건이다. 다른 연구자가 동일한 원본 데이터와 동일한 절차를 따랐을 때, 동일한 결과를 얻을 수 있어야 한다.

데이터 전처리 과정은 원본 데이터(raw data)를 실제 분석에 사용된 데이터(analysis-ready data)로 변환하기 위해 수행한 모든 단계를 기록하는 과정이다. 이 과정이 없다면, 다른 사람들은 데이터를 어떻게 처리했는지 알 수 없으므로 연구 결과를 검증하거나 재현하는 것이 불가능해진다.

---

## 4. 통계 모델의 가정 충족 및 왜곡 방지

### 4.1 통계 모델의 가정 충족

대부분의 통계 모델은 수학적 공식에 기반하며, 그 공식이 올바르게 작동하기 위한 몇 가지 가정을 전제로 한다. 만약 데이터가 이 가정을 충족하지 못하면, 모델의 계산 결과 자체를 신뢰할 수 없게 된다.

피어슨 상관분석의 주요 가정은 다음과 같다:

- **선형성(Linearity)**: 두 변수 간의 관계가 직선적이어야 함
- **정규성(Normality)**: 각 변수의 데이터가 정규분포를 따라야 함  
- **등분산성(Homoscedasticity)**: 데이터가 흩어져 있는 정도가 전체적으로 비슷해야 함

원본 데이터가 이 가정들을 충족하지 않을 경우, 전처리 단계에서 로그 변환이나 제곱근 변환 등을 적용하여 데이터를 선형적이고 정규성을 띠도록 보정할 수 있다.

### 4.2 단위와 스케일 차이로 인한 왜곡 방지

분석에 사용되는 변수들의 단위와 값의 범위가 크게 다르면, 통계적 계산 과정에서 단순히 숫자가 큰 변수가 다른 변수보다 더 중요하다고 잘못 판단될 수 있다. 값의 크기가 실제 패턴을 압도해 버리는 현상이 발생한다.

**거리 계산에서의 왜곡**

많은 통계 분석은 데이터 포인트 간의 거리를 계산하는 과정을 포함한다. 유클리드 거리 공식은 각 변수 차이의 제곱합으로 계산되는데, 이 과정에서 값의 범위가 큰 변수는 작은 변수에 비해 거리 계산에 불균형적으로 큰 영향을 미친다.

예를 들어, 두 지역의 유사성을 측정할 때 '인구수'(수십만 단위)와 '범죄율'(한 자릿수 단위)을 함께 사용한다면, 범죄율의 실제 차이가 더 중요함에도 불구하고 인구수의 차이가 거리 계산을 지배하게 된다.

**최적화 과정에서의 비효율성**

통계 모델은 일반적으로 오차를 최소화하는 최적해를 찾아가는 과정을 거친다. 변수 간 스케일이 다르면 이 최적화 과정에서 탐색 공간이 불균등하게 형성된다. 

스케일이 큰 변수 방향으로는 조금만 움직여도 오차가 크게 변하지만, 스케일이 작은 변수 방향으로는 많이 움직여야 오차 변화가 감지된다. 이로 인해 최적화 알고리즘이 비효율적인 경로를 따라 해를 탐색하게 되어 수렴 속도가 현저히 느려지거나, 최적해를 찾지 못하고 부분 최적해에 머물 수 있다.

**계산 정밀도의 손실**

컴퓨터의 부동소수점 연산에서는 매우 큰 수와 매우 작은 수가 함께 연산될 때 정밀도 손실이 발생할 수 있다. 스케일이 크게 다른 변수들이 동시에 계산에 참여하면, 작은 값들의 변화가 큰 값들에 의해 상쇄되어 실제로는 중요한 정보가 무시될 수 있다.

### 4.3 정규화를 통한 해결

Min-Max 정규화는 모든 변수의 값을 0과 1 사이의 동일한 범위로 재조정한다. 이를 통해 모든 변수가 동등한 스케일 위에서 오직 데이터의 패턴과 관계만으로 평가받도록 하여, 특정 변수의 단위나 크기 때문에 결과가 왜곡되는 것을 방지한다.

### 4.4 이상치로 인한 왜곡 방지

데이터셋에 다른 값들과 동떨어진 극단적인 값인 이상치가 존재하면, 특히 피어슨 상관계수는 그 값에 크게 영향을 받아 전체적인 관계를 잘못 표현할 수 있다.

데이터 전처리 단계에서 박스플롯 같은 시각화 기법을 통해 이러한 이상치를 식별하고, 이를 제거하거나 다른 값으로 대체하는 등의 처리를 한다. 이를 통해 모델이 대부분의 데이터가 보여주는 일반적인 경향성을 학습하도록 하여 결과의 안정성과 일반화 가능성을 높인다.

---

## 7. 결론

데이터 정규화는 단순한 기술적 절차가 아니라, 분석 결과의 신뢰성과 타당성을 보장하기 위한 필수적인 과정이다. 이는 데이터의 품질을 보증하고, 분석의 재현성을 확보하며, 통계 모델의 가정을 충족시키고, 다양한 형태의 왜곡을 방지하는 역할을 수행한다.

특히 변수 간의 스케일 차이로 인한 왜곡을 방지하여, 모든 변수가 공평한 조건에서 분석에 기여할 수 있도록 보장하는 것이 정규화의 핵심 목적이다.