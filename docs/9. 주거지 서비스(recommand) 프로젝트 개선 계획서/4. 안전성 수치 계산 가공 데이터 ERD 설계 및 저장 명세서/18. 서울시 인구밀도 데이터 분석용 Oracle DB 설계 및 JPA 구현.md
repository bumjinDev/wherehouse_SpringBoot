# 서울시 인구밀도 통계 데이터 분석용 처리 Component (2차 작업 - 1차 파일 연동)

## 1. Oracle 분석용 테이블 DDL

```sql
-- 기존 테이블과 데이터를 완전히 삭제
DROP TABLE ANALYSIS_POPULATION_DENSITY CASCADE CONSTRAINTS PURGE;
DROP SEQUENCE SEQ_ANALYSIS_POPULATION_DENSITY;

-- 분석용 인구밀도 통계 테이블 생성 (1차 파일 구조 기반)
CREATE TABLE ANALYSIS_POPULATION_DENSITY (
    ID                      NUMBER,                     -- 제한 없음
    DISTRICT_NAME           VARCHAR2(4000),             -- 1차 파일과 동일한 필드명
    YEAR                    NUMBER,                     -- 제한 없음
    POPULATION_COUNT        NUMBER,                     -- 제한 없음
    AREA_SIZE               NUMBER,                     -- 제한 없음
    POPULATION_DENSITY      NUMBER                      -- 제한 없음
);

-- 시퀀스 생성
CREATE SEQUENCE SEQ_ANALYSIS_POPULATION_DENSITY 
    START WITH 1 
    INCREMENT BY 1 
    NOCACHE
    NOCYCLE;
```

## 2. JPA Entity (분석용)

```java
package com.WhereHouse.AnalysisStaticData.Population.Entity;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import java.math.BigDecimal;

@Entity
@Table(name = "ANALYSIS_POPULATION_DENSITY")
@Data
@NoArgsConstructor
@AllArgsConstructor
@Builder
public class AnalysisPopulationDensity {

    @Id
    @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = "analysis_population_seq")
    @SequenceGenerator(name = "analysis_population_seq", sequenceName = "SEQ_ANALYSIS_POPULATION_DENSITY", allocationSize = 1)
    @Column(name = "ID")
    private Long id;

    @Column(name = "DISTRICT_NAME", length = 100)
    private String districtName;  // 1차 파일과 동일한 필드명

    @Column(name = "YEAR")
    private Integer year;

    @Column(name = "POPULATION_COUNT")
    private Long populationCount;

    @Column(name = "AREA_SIZE", precision = 15, scale = 5)
    private BigDecimal areaSize;

    @Column(name = "POPULATION_DENSITY", precision = 15, scale = 5)
    private BigDecimal populationDensity;
}
```

## 3. Repository (분석용)

```java
package com.WhereHouse.AnalysisStaticData.Population.Repository;

import com.WhereHouse.AnalysisStaticData.Population.Entity.AnalysisPopulationDensity;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Query;
import org.springframework.stereotype.Repository;
import java.util.List;
import java.util.Optional;

@Repository
public interface AnalysisPopulationDensityRepository extends JpaRepository<AnalysisPopulationDensity, Long> {
    
    Optional<AnalysisPopulationDensity> findByDistrictName(String districtName);
    
    List<AnalysisPopulationDensity> findAllByOrderByPopulationDensityDesc();
    
    boolean existsByDistrictName(String districtName);
    
    @Query("SELECT COUNT(a) FROM AnalysisPopulationDensity a")
    long countAnalysisData();
    
    @Query("SELECT a.districtName, a.populationDensity FROM AnalysisPopulationDensity a ORDER BY a.populationDensity DESC")
    List<Object[]> findDistrictDensityRanking();
}
```

## 4. 인구밀도 데이터 분석용 처리 Component (2차 작업 - 1차 파일 연동)

```java
package com.WhereHouse.AnalysisStaticData.Population.Processor;

import com.WhereHouse.AnalysisStaticData.Population.Entity.AnalysisPopulationDensity;
import com.WhereHouse.AnalysisStaticData.Population.Repository.AnalysisPopulationDensityRepository;
// 1차 파일의 원본 데이터 접근을 위한 import
import com.WhereHouse.APITest.Population.Entity.PopulationDensity;
import com.WhereHouse.APITest.Population.Repository.PopulationDensityRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;

import java.util.List;

/**
 * 인구밀도 데이터 분석용 테이블 생성 처리 컴포넌트 (2차 작업 - 1차 파일 연동)
 * 
 * 1차 파일에서 생성된 POPULATION_DENSITY 테이블의 데이터를 조회하여
 * 분석 전용 ANALYSIS_POPULATION_DENSITY 테이블로 복사하는 작업을 수행한다.
 * 
 * 주요 기능:
 * - 1차 파일에서 로딩된 원본 인구밀도 통계 데이터 조회 및 검증
 * - CREATED_AT 필드 제외한 모든 인구밀도 통계 필드 복사
 * - 1차 파일의 DISTRICT_NAME 필드를 그대로 활용
 * - 분석용 테이블 데이터 품질 검증
 * 
 * @author Safety Analysis System
 * @since 1.0
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class PopulationDataProcessor {

    // 1차 파일의 원본 인구밀도 통계 테이블 접근을 위한 Repository
    private final PopulationDensityRepository originalPopulationRepository;
    
    // 분석용 인구밀도 통계 테이블 접근을 위한 Repository  
    private final AnalysisPopulationDensityRepository analysisPopulationRepository;

    /**
     * 인구밀도 데이터 분석용 테이블 생성 메인 프로세스 (2차 작업 - 1차 파일 연동)
     * 
     * 작업 순서:
     * 1. 기존 분석용 데이터 존재 여부 확인
     * 2. 1차 파일에서 로딩된 원본 인구밀도 데이터 조회 및 검증
     * 3. 데이터 변환 및 분석용 테이블 저장
     * 4. 데이터 품질 검증 및 결과 로깅
     */
    @Transactional
    public void processAnalysisPopulationData() {
        log.info("=== 인구밀도 데이터 분석용 테이블 생성 작업 시작 (2차 작업 - 1차 파일 연동) ===");
        
        // Step 1: 기존 분석용 데이터 중복 처리 방지를 위한 존재 여부 확인
        long existingAnalysisDataCount = analysisPopulationRepository.count();
        if (existingAnalysisDataCount > 0) {
            log.info("분석용 인구밀도 데이터가 이미 존재합니다 (총 {} 개). 작업을 스킵합니다.", existingAnalysisDataCount);
            return;
        }

        // Step 2: 1차 파일에서 로딩된 원본 인구밀도 통계 데이터 조회 및 검증
        List<PopulationDensity> originalPopulationDataList = originalPopulationRepository.findAll();
        if (originalPopulationDataList.isEmpty()) {
            log.warn("원본 인구밀도 통계 데이터가 존재하지 않습니다. 먼저 1차 파일의 PopulationDensityDataLoader를 통해 데이터를 로드해주세요.");
            return;
        }
        
        log.info("1차 파일에서 로딩된 원본 인구밀도 통계 데이터 {} 개 구 발견", originalPopulationDataList.size());

        // Step 3: 데이터 변환 및 저장 작업 수행
        int successfulConversionCount = 0;  // 성공적으로 변환된 데이터 개수
        int failedConversionCount = 0;      // 변환 실패한 데이터 개수

        for (PopulationDensity originalPopulationData : originalPopulationDataList) {
            try {
                // 원본 데이터를 분석용 엔티티로 변환 (1차 파일 구조 기반)
                AnalysisPopulationDensity analysisTargetPopulationData = convertToAnalysisEntity(originalPopulationData);
                
                // 분석용 테이블에 데이터 저장
                analysisPopulationRepository.save(analysisTargetPopulationData);
                successfulConversionCount++;
                
                log.debug("분석용 데이터 생성 완료: {} 구 (인구밀도: {} 명/㎢)", 
                    originalPopulationData.getDistrictName(), originalPopulationData.getPopulationDensity());

            } catch (Exception dataConversionException) {
                log.error("분석용 데이터 생성 실패 - 구명: {}, 오류: {}", 
                    originalPopulationData.getDistrictName(), dataConversionException.getMessage());
                failedConversionCount++;
            }
        }

        // Step 4: 변환 작업 결과 로깅
        log.info("인구밀도 데이터 분석용 테이블 생성 작업 완료 - 성공: {} 개, 실패: {} 개", 
            successfulConversionCount, failedConversionCount);
        
        // Step 5: 최종 데이터 검증 및 품질 확인
        performFinalDataValidation();
        
        log.info("=== 인구밀도 데이터 분석용 테이블 생성 작업 종료 (2차 작업 - 1차 파일 연동) ===");
    }
    
    /**
     * 1차 파일의 원본 인구밀도 통계 엔티티를 분석용 엔티티로 변환
     * 
     * CREATED_AT 필드를 제외한 모든 인구밀도 통계 필드를 복사한다.
     * 1차 파일의 DISTRICT_NAME 필드를 그대로 활용한다.
     * 
     * @param originalPopulationData 1차 파일의 원본 인구밀도 통계 엔티티
     * @return 분석용 인구밀도 통계 엔티티
     */
    private AnalysisPopulationDensity convertToAnalysisEntity(PopulationDensity originalPopulationData) {
        return AnalysisPopulationDensity.builder()
            // 1차 파일의 DISTRICT_NAME 필드 그대로 활용
            .districtName(originalPopulationData.getDistrictName())       // 자치구명
            .year(originalPopulationData.getYear())                       // 통계 연도
            
            // 인구밀도 통계 (1차 파일 구조와 동일)
            .populationCount(originalPopulationData.getPopulationCount()) // 인구 수
            .areaSize(originalPopulationData.getAreaSize())               // 면적 (㎢)
            .populationDensity(originalPopulationData.getPopulationDensity()) // 인구밀도 (명/㎢)
            .build();
    }
    
    /**
     * 분석용 데이터의 최종 검증 및 품질 확인
     * 
     * 작업 내용:
     * - 전체 데이터 개수 확인
     * - 구별 인구밀도 순위 상위 5개 로깅
     * - 데이터 검증 과정에서 발생하는 오류 처리
     */
    private void performFinalDataValidation() {
        try {
            // 최종 저장된 분석용 데이터 개수 확인
            long finalAnalysisDataCount = analysisPopulationRepository.count();
            log.info("최종 분석용 인구밀도 데이터 저장 완료: {} 개 구", finalAnalysisDataCount);
            
            // 구별 인구밀도 순위 조회 및 로깅 (피어슨 상관분석 검증용)
            List<Object[]> districtDensityRankingList = analysisPopulationRepository.findDistrictDensityRanking();
            log.info("서울시 구별 인구밀도 순위 (상위 5개구):");
            
            districtDensityRankingList.stream()
                .limit(5)
                .forEach(rankingRow -> {
                    String districtName = (String) rankingRow[0];           // 구 이름
                    Object densityObj = rankingRow[1];                      // 인구밀도
                    log.info("  {} : {} 명/㎢", districtName, densityObj);
                });
                
        } catch (Exception dataValidationException) {
            log.error("분석용 데이터 검증 과정에서 오류가 발생했습니다: {}", 
                dataValidationException.getMessage(), dataValidationException);
        }
    }
}
```

## 5. Main 처리 클래스 (2차 작업 - 1차 파일 연동)

```java
package com.WhereHouse.AnalysisStaticData.Main;

import com.WhereHouse.AnalysisStaticData.Population.Processor.PopulationDataProcessor;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.CommandLineRunner;
import org.springframework.stereotype.Component;

@Component
@RequiredArgsConstructor
@Slf4j
public class AnalysisDataProcessor implements CommandLineRunner {

    private final PopulationDataProcessor populationDataProcessor;
    // 향후 18개 ERD별 프로세서 추가 예정

    @Override
    public void run(String... args) throws Exception {
        log.info("=== 안전성 분석용 데이터 처리 시작 (2차 작업 - 1차 파일 연동) ===");
        
        try {
            // 1. 1차 파일에서 로딩된 인구밀도 데이터를 분석용으로 처리
            log.info("1. 인구밀도 데이터 분석용 테이블 생성 시작 (2차 작업 - 1차 파일 연동)");
            populationDataProcessor.processAnalysisPopulationData();
            
            // 향후 추가될 18개 ERD별 데이터 처리
            
        } catch (Exception e) {
            log.error("안전성 분석용 데이터 처리 중 오류 발생", e);
            throw e;
        }
        
        log.info("=== 안전성 분석용 데이터 처리 완료 (2차 작업 - 1차 파일 연동) ===");
    }
}
```

## 6. application.yml 설정 추가

```yaml
# 기존 1차 작업 설정에 추가
app:
  csv:
    population-density-path: classpath:인구밀도_202509011.csv
  analysis:
    enable-population-processing: true  # 분석용 데이터 처리 활성화

# Spring 설정
spring:
  application:
    name: seoul-population-density-api
    
  # 데이터베이스 설정 (1차 작업과 동일)
  datasource:
    url: jdbc:oracle:thin:@localhost:1521/XE
    username: ${DB_USERNAME:your_username}
    password: ${DB_PASSWORD:your_password}
    driver-class-name: oracle.jdbc.OracleDriver
    
  # JPA 설정 (1차 작업과 동일)
  jpa:
    hibernate:
      ddl-auto: validate
    show-sql: ${SHOW_SQL:false}
    properties:
      hibernate:
        format_sql: true
        dialect: org.hibernate.dialect.Oracle12cDialect
        
# 로깅 설정 (분석용 로그 추가)
logging:
  level:
    root: INFO
    com.WhereHouse.APITest.Population: ${LOG_LEVEL:INFO}
    com.WhereHouse.AnalysisStaticData: ${ANALYSIS_LOG_LEVEL:INFO}
    org.hibernate.SQL: ${SQL_LOG_LEVEL:WARN}
```

## 7. 실행 순서 및 검증

### 실행 순서
1. **1차 작업**: `PopulationDensityDataLoader`가 CSV에서 데이터를 `POPULATION_DENSITY`에 로딩
2. **2차 작업**: `PopulationDataProcessor`가 원본 데이터를 `ANALYSIS_POPULATION_DENSITY`에 복사

### 데이터 검증
```sql
-- 1차 파일 데이터 확인
SELECT COUNT(*) FROM POPULATION_DENSITY;
SELECT DISTRICT_NAME, POPULATION_DENSITY FROM POPULATION_DENSITY ORDER BY POPULATION_DENSITY DESC;

-- 2차 작업 결과 확인
SELECT COUNT(*) FROM ANALYSIS_POPULATION_DENSITY;
SELECT DISTRICT_NAME, POPULATION_DENSITY FROM ANALYSIS_POPULATION_DENSITY ORDER BY POPULATION_DENSITY DESC;

-- 데이터 일치성 검증
SELECT 
    (SELECT COUNT(*) FROM POPULATION_DENSITY) as ORIGINAL_COUNT,
    (SELECT COUNT(*) FROM ANALYSIS_POPULATION_DENSITY) as ANALYSIS_COUNT
FROM DUAL;
```

## 8. Maven Dependencies (pom.xml 추가 항목)

```xml
<!-- 1차 작업 의존성과 동일 -->
<dependency>
    <groupId>com.opencsv</groupId>
    <artifactId>opencsv</artifactId>
    <version>5.8</version>
</dependency>

<dependency>
    <groupId>com.oracle.database.jdbc</groupId>
    <artifactId>ojdbc8</artifactId>
    <scope>runtime</scope>
</dependency>

<dependency>
    <groupId>org.projectlombok</groupId>
    <artifactId>lombok</artifactId>
    <optional>true</optional>
</dependency>

<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-jpa</artifactId>
</dependency>

<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-web</artifactId>
</dependency>
```

## 9. 주요 수정사항 요약

### 1차 파일 연동을 위한 수정
- **테이블명 통일**: 1차 작업의 `POPULATION_DENSITY` 테이블 참조
- **Entity 연동**: 1차 파일의 `PopulationDensity` 엔티티 직접 활용
- **Repository 연동**: 1차 파일의 `PopulationDensityRepository` 직접 활용
- **패키지 구조**: 분석용 패키지(`AnalysisStaticData`)로 분리
- **데이터 매핑**: 1차 파일의 필드 구조에 맞게 변환 로직 수정
- **로깅 메시지**: 2차 작업 - 1차 파일 연동임을 명시하는 메시지로 변경

이제 1차 작업에서 CSV 데이터를 `POPULATION_DENSITY` 테이블에 로딩한 후, 2차 작업에서 해당 데이터를 `ANALYSIS_POPULATION_DENSITY` 테이블로 복사하는 완전한 워크플로우가 구성되었습니다.