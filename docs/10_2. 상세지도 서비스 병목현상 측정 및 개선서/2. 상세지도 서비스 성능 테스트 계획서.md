# 위치 분석 서비스 성능 테스트 계획서
## Performance Test Plan

**문서 정보**
- 문서 유형: 성능 테스트 계획서 (Performance Test Plan)
- 문서 버전: 2.0
- 작성일: 2025-01-23
- 작성자: 정범진
- 프로젝트: Wherehouse 위치 기반 정보 분석 서비스 성능 최적화
- 참조 문서: 위치 분석 서비스 성능 테스트 명세서 v2.1

---

## 목차

1. [테스트 개요](#1-테스트-개요)
2. [테스트 방법론](#2-테스트-방법론)
3. [테스트 환경 및 도구](#3-테스트-환경-및-도구)
4. [테스트 시나리오](#4-테스트-시나리오)
5. [데이터 수집 및 분석 방법](#5-데이터-수집-및-분석-방법)
6. [일정 및 산출물](#6-일정-및-산출물)

---

## 1. 테스트 개요

### 1.1. 테스트 목적

**명세서 기반 실행:**
본 계획서는 "위치 분석 서비스 성능 테스트 명세서 v2.1"에 정의된 측정 요구사항을 실제 환경에서 실행하기 위한 구체적인 방법을 기술한다.

**핵심 목표:**
1. 명세서에 정의된 R-01 ~ R-07 각 단계의 실제 실행 시간 측정
2. B-01 ~ B-05 예상 병목 지점의 실제 영향도 검증
3. Success Criteria(평균 500ms, 95th 1,000ms, 50 TPS) 달성 여부 확인
4. 병목 개선을 위한 데이터 기반 의사결정 근거 확보

---

### 1.2. 테스트 범위

**측정 대상:**
- API: `POST /api/location-analysis`
- 서비스: `LocationAnalysisServiceImpl.analyzeLocation()`
- 측정 단위: R-Step(7개) 및 Action(세부 동작)

**측정 제외:**
- 프론트엔드 렌더링 시간
- 네트워크 지연 시간 (클라이언트 ↔ 서버)
- 카카오맵 API 서버 내부 처리 시간

---

### 1.3. 테스트 전제조건

**필수 구현 사항:**
1. PerformanceLogger 유틸리티 구현 완료
2. TraceIdFilter 구현 및 적용 완료
3. 모든 R-Step에 계측 코드 삽입 완료
4. Result DTO 전체 구현 완료
5. JSON 로그 출력 설정 완료

**데이터 준비:**
1. CCTV 데이터: 최소 10,000건 이상
2. 파출소 데이터: 전국 단위 데이터
3. 검거율 데이터: 서울시 전 구 데이터
4. Redis 캐시: 초기 비움 상태 및 워밍업 상태 모두 준비

**환경 준비:**
1. 개발 서버 또는 스테이징 서버 (운영 환경과 유사한 스펙)
2. Oracle 21c + Redis 준비
3. 네트워크 안정성 확보

---

## 2. 테스트 방법론

### 2.1. Top-Down 프로파일링 (거시 분석)

**목적:**
시스템 전체의 성능 특성을 파악하고 핵심 병목 구간을 식별

**절차:**

**1단계: 계측 코드 삽입**
- R-01 ~ R-07 각 메서드에 PerformanceLogger 적용
- START/END 로그가 정확히 기록되는지 검증
- 예시:
```java
PerformanceLogger perfLogger = PerformanceLogger.start(
    "R-03", "performDatabaseQuery", "Service",
    "LocationAnalysisServiceImpl", "performDatabaseQuery"
);
try {
    R03DbResult result = performDatabaseQuery(cacheResult);
    perfLogger.setResultData(result);
    return result;
} finally {
    perfLogger.end();
}
```

**2단계: 단일 요청 검증 테스트**
- 부하 생성 없이 단일 API 호출 수행
- 로그 파일에서 해당 traceId의 모든 로그 추출
- R-01 ~ R-07이 모두 순차적으로 기록되는지 확인
- duration_ms 값이 합리적인지 육안 검증

**3단계: 부하 테스트 수행**
- 시나리오별 부하 생성 (상세는 4장 참조)
- 모든 PERFORMANCE 로그를 파일에 저장
- 최소 100개 이상의 요청 로그 수집

**4단계: 데이터 집계 및 분석**
- Python/Pandas를 사용한 로그 파싱
- Step별 평균/중앙값/95th percentile 계산
- 전체 응답 시간 대비 각 Step의 비율 계산
- 결과 예시:
```
Step     평균(ms)  비율(%)  95th(ms)
R-01     8         0.4%     12
R-02     45        2.2%     95
R-03     120       5.9%     250
R-04     380       18.6%    520
R-05     1,450     71.0%    2,100  ← 병목 확정
R-06     15        0.7%     25
R-07     25        1.2%     50
```

**5단계: 병목 Step 확정**
- 명세서의 병목 식별 원칙 적용:
  - 상대적 비중: 전체의 20% 이상 차지
  - 절대적 지연: 기능 복잡도 대비 과도한 시간
  - 개선 가능성: 아키텍처/쿼리 개선 여지
- 위 예시에서는 R-05가 명확한 병목

---

### 2.2. Drill-Down 분석 (미시 분석)

**목적:**
Top-Down에서 식별된 병목 Step 내부의 근본 원인 규명

**절차:**

**1단계: 병목 Step 내부 Action 분석**
- 해당 Step의 모든 Action 로그 추출
- Action별 duration_ms 집계
- 예시 (R-05의 경우):
```
Action                        평균(ms)  비율(%)
Filter_CCTV                  5         0.3%
DB_findNearestPoliceStation  1,420     97.9%  ← 미시 병목 확정
Filter_Amenity               25        1.7%
```

**2단계: 근본 원인 분류**

**I/O 병목 (DB 쿼리):**
- DB 쿼리 실행 계획 분석
- 실행 계획 분석 항목:
  - Full Table Scan 여부
  - 인덱스 사용 여부
  - Rows examined vs Rows returned
  - 조인 방식 및 임시 테이블 사용
- 분석 결과 예시:
```
실행 계획 확인 결과:
- TABLE ACCESS FULL on POLICE_OFFICE_GEO (2,345 rows)
- 원인: 공간 인덱스 미사용, 공간 거리 함수 계산
```

**I/O 병목 (Redis):**
- Redis SLOWLOG 확인
- 명령별 실행 시간 분석
- N+1 패턴 여부 확인 (for 루프 내 단일 GET/SET)

**I/O 병목 (외부 API):**
- API별 평균 응답 시간 집계
- 타임아웃 발생 빈도 확인
- 순차 vs 병렬 실행 패턴 분석

**CPU 병목:**
- 반복문 횟수 및 시간 복잡도 분석
- 데이터 크기별 선형성 확인

**(선택) 추가 프로파일링:**
- VisualVM 등 프로파일러를 활용한 Hotspot 메서드 식별
- CPU 병목이 명확하지 않을 경우에만 수행

**3단계: 개선 가설 수립**
- 측정 데이터 기반으로 구체적 개선 방안 도출
- 예시:
  - B-01: 공간 인덱스 추가 시 Full Scan 제거 → 예상 1,420ms → 50ms
  - B-03: Redis MGET 사용 시 RTT 9회 → 1회 → 예상 110ms → 15ms

**4단계: 개선 우선순위 결정**
- 영향도 (현재 소요 시간) × 개선 가능성 (기술적 난이도)
- 투입 리소스 대비 효과 분석

---

### 2.3. 분석 핵심 원칙

**측정 기반 의사결정:**
- "파출소 쿼리가 느릴 것 같다" (X)
- "파출소 쿼리가 평균 1,420ms 소요되며 전체의 71%를 차지한다" (O)

**가설 검증:**
- 추측으로 개선 작업 시작하지 않음
- 반드시 측정 → 분석 → 가설 → 개선 → 재측정 순서 준수

**재현 가능성:**
- 모든 측정은 traceId로 추적 가능
- 동일 조건 재실행 시 유사한 결과 보장

---

## 3. 테스트 환경 및 도구

### 3.1. 테스트 환경

**서버 환경:**
```
OS: Windows 11 Pro
CPU: Intel Core i5-11400H (6 Core, 12 Thread, 2.70GHz)
Memory: 32GB
Storage: 512GB SSD
```

**애플리케이션 스택:**
```
Java: OpenJDK 17
Spring Boot: 3.2.x
Oracle: 21c
Redis
```

**DB 데이터 규모:**
```
CCTV: 10,000 ~ 50,000건
파출소: 1,000 ~ 2,000건
검거율: 25건 (서울시 구)
```

---

### 3.2. 테스트 도구

**부하 생성:**
- **도구**: Apache JMeter 5.6
- **역할**: HTTP 요청 생성 및 응답 시간 측정
- **설정**: Thread Group, HTTP Request Sampler, Listeners

**로그 수집:**
- **도구**: Logback + JSON Encoder
- **역할**: 구조화된 JSON 로그 출력
- **설정**: `logback-spring.xml`에 별도 appender 정의

**시스템 모니터링:**
- **도구**: htop, iostat, vmstat
- **역할**: CPU, Memory, Disk I/O 모니터링
- **주기**: 1초 간격 샘플링

**데이터 분석:**
- **도구**: Python 3.10 + Pandas + Matplotlib
- **역할**: 로그 파싱 및 시각화
- **스크립트**: `performance_analyzer.py` (별도 개발)

---

### 3.3. JMeter 테스트 설정

**Thread Group 설정:**
```
Number of Threads (users): 시나리오별 상이
Ramp-Up Period: 60초 (부하 테스트)
Loop Count: 10회 (단일 시나리오) / Infinite (부하 테스트)
Duration: 300초 (5분, 부하 테스트)
```

**HTTP Request 설정:**
```
Method: POST
Path: /api/location-analysis
Content-Type: application/json
Body:
{
    "latitude": 37.5665,
    "longitude": 126.9780,
    "radius": 500
}
```

**Listeners 설정:**
```
- Summary Report: 평균 응답 시간, TPS
- Response Time Graph: 시간대별 응답 시간
- View Results Tree: 개별 요청/응답 확인
```

---

## 4. 테스트 시나리오

### 4.1. 시나리오 1: 단일 요청 검증 (Smoke Test)

**목적:**
계측 코드가 정상 동작하는지 확인하고 로그 포맷 검증

**설정:**
- Users: 1
- 반복: 1회
- 캐시: Cold Start (비운 상태)

**절차:**
1. Redis FLUSHALL 명령으로 캐시 비움
2. JMeter로 단일 요청 실행
3. 애플리케이션 로그에서 해당 traceId 추출
4. R-01 ~ R-07 모든 Step의 START/END 로그 확인
5. resultData 필드의 JSON 구조 검증

**성공 기준:**
- 모든 Step의 START/END 쌍 존재
- traceId 일관성 유지
- duration_ms > 0
- resultData 필드 정상 직렬화

---

### 4.2. 시나리오 2: Cache Miss 스트레스 테스트

**목적:**
캐시 미스 시 최악의 응답 시간 측정 및 B-01 ~ B-05 병목 검증

**설정:**
- Users: 1
- 반복: 20회
- 캐시: 매 요청마다 FLUSHALL (강제 미스)

**절차:**
1. JMeter Loop Controller 사용
2. 각 반복 전 Redis FLUSHALL 실행 (BeanShell Sampler)
3. 20회 요청 수행
4. 모든 로그 수집

**측정 지표:**
- R-03 DB 조회 시간 (캐시 미스 → DB 조회 발생)
- R-05 파출소 쿼리 시간 (B-01 병목 측정)
- R-02 L2 캐시 조회 시간 (전부 미스 → 0ms에 가까움)

**예상 결과:**
- 평균 응답 시간: 1,500 ~ 2,500ms
- R-05가 전체의 60~80% 차지
- DB_findNearestPoliceStation이 1,000ms 초과

---

### 4.3. 시나리오 3: Cache Hit 최적 성능 테스트

**목적:**
캐시 히트 시 최선의 응답 시간 측정 및 목표 달성 가능성 확인

**설정:**
- Users: 1
- 반복: 20회
- 캐시: 동일 좌표 반복 요청 (L1 히트)

**절차:**
1. 동일 좌표로 첫 요청 실행 (캐시 워밍업)
2. 이후 19회 동일 요청 반복
3. 로그 수집

**측정 지표:**
- R-02 L1 캐시 히트 시간
- 전체 응답 시간 (R-03 ~ R-07 스킵)

**예상 결과:**
- 평균 응답 시간: 50 ~ 150ms
- R-02 L1_Cache_Get만 실행
- 목표 500ms 충분히 달성

**시사점:**
- 캐시 전략의 효과성 검증
- 최종 목표 달성 가능성 확인

---

### 4.4. 시나리오 4: 혼합 부하 테스트 (Mixed Workload)

**목적:**
실제 운영 환경을 모사한 캐시 히트/미스 혼합 상황에서의 안정성 검증

**설정:**
- Users: 10
- Ramp-Up: 60초
- Duration: 5분
- 캐시: 자연적인 히트/미스 발생
- 좌표: CSV 파일로 100개 좌표 준비 (서울시 랜덤)

**절차:**
1. JMeter CSV Data Set Config로 좌표 로드
2. 각 스레드는 CSV에서 순차적으로 좌표 선택
3. 5분간 지속 요청
4. 로그 수집

**측정 지표:**
- 평균 응답 시간
- 95th Percentile
- TPS
- 에러율
- 캐시 히트율 (L1, L2)

**예상 결과:**
- 평균 응답 시간: 800 ~ 1,500ms (캐시 히트율에 따라 변동)
- 95th: 2,000ms 내외
- TPS: 10 ~ 20 (현재 시스템)

**목표와의 Gap:**
- 목표: 평균 500ms, 95th 1,000ms, 50 TPS
- 현재: 미달성 → 개선 필요성 입증

---

### 4.5. 시나리오 5: 목표 TPS 달성 테스트

**목적:**
Success Criteria의 50 TPS 달성 가능 여부 확인

**설정:**
- Users: 50
- Ramp-Up: 60초
- Duration: 5분
- 캐시: 자연적 히트/미스
- 좌표: 랜덤 100개

**절차:**
1. 50명의 동시 사용자 시뮬레이션
2. 5분간 지속 부하
3. JMeter Summary Report에서 TPS 확인

**측정 지표:**
- Throughput (TPS)
- 에러율
- 평균 응답 시간

**성공 기준:**
- TPS ≥ 50
- 에러율 < 0.1%
- 평균 응답 시간 ≤ 500ms (개선 후)

**예상 결과 (개선 전):**
- TPS: 20 ~ 30 (미달)
- 에러율: < 1%
- 평균: 1,200ms (미달)

---

## 5. 데이터 수집 및 분석 방법

### 5.1. 로그 수집 절차

**1단계: 로그 파일 분리**
- 일반 애플리케이션 로그: `application.log`
- 성능 로그: `performance.log` (별도 appender)
- 필터 조건: `eventType == "PERFORMANCE"`

**2단계: 로그 저장**
- 포맷: 1줄 = 1개 JSON 객체 (NDJSON)
- 파일명: `performance_{timestamp}.log`
- 크기: 로테이션 100MB

**3단계: 로그 백업**
- 테스트 완료 후 즉시 별도 디렉토리로 복사
- 시나리오별로 폴더 분리
- 예: `logs/scenario1/`, `logs/scenario2/`

---

### 5.2. Python 분석 스크립트 구조

**기본 파싱:**

```python
import pandas as pd
import json

# 1. 로그 파일 읽기
logs = []
with open('performance.log', 'r') as f:
    for line in f:
        logs.append(json.loads(line))

df = pd.DataFrame(logs)

# 2. END 로그만 필터링 (duration 계산된 로그)
df_end = df[df['status'] == 'END'].copy()

# 3. Step별 집계
step_summary = df_end.groupby('step').agg({
    'duration_ms': ['mean', 'median', 'min', 'max', 'std'],
    'traceId': 'count'
}).round(2)

print(step_summary)
```

**트랜잭션 재구성:**

```python
# 특정 traceId의 전체 흐름 재구성
trace_id = 'a1b2c3d4'
trace = df[df['traceId'] == trace_id].sort_values('nanoTime')

# 시각화
import matplotlib.pyplot as plt

steps = trace[trace['status'] == 'END']['step']
durations = trace[trace['status'] == 'END']['duration_ms']

plt.barh(steps, durations)
plt.xlabel('Duration (ms)')
plt.title(f'Transaction Timeline: {trace_id}')
plt.show()
```

**병목 식별:**

```python
# 전체 응답 시간 대비 비율 계산
total_time = df_end[df_end['step'] == 'R-07']['duration_ms'].mean()

df_end['ratio'] = (df_end['duration_ms'] / total_time * 100).round(1)

# 20% 이상 차지하는 Step 식별
bottlenecks = df_end[df_end['ratio'] >= 20.0]
print(bottlenecks[['step', 'duration_ms', 'ratio']])
```

**95th Percentile 계산:**

```python
# Step별 95th percentile
df_end.groupby('step')['duration_ms'].quantile(0.95)
```

---

### 5.3. 시각화 및 보고서 생성

**생성할 차트:**

1. **Step별 평균 시간 막대 그래프**
   - X축: R-01 ~ R-07
   - Y축: 평균 duration_ms
   - 목적: 거시적 병목 시각화

2. **박스 플롯 (Box Plot)**
   - 각 Step의 분포 (최소/Q1/중앙/Q3/최대)
   - 목적: 편차 및 이상치 확인

3. **파이 차트**
   - 각 Step의 전체 시간 대비 비율
   - 목적: 상대적 비중 시각화

4. **타임라인 차트 (Gantt-style)**
   - 단일 요청의 Step별 시작/종료 시각
   - 목적: 순차/병렬 실행 패턴 확인

5. **히트맵 (Action 레벨)**
   - X축: Action 이름
   - Y축: 요청 번호
   - 색상: duration_ms
   - 목적: 미시 병목 패턴 분석

---

### 5.4. 분석 보고서 구성

**보고서 템플릿:**

```
1. 테스트 요약
   - 시나리오, 일시, 환경
   - 전체 요청 수, 성공/실패

2. 전체 성능 지표
   - 평균 응답 시간
   - 95th Percentile
   - TPS
   - 에러율
   - 목표 대비 달성 여부

3. Step별 분석
   - R-01 ~ R-07 각각의 평균/중앙/95th
   - 비율 및 병목 여부

4. Action별 상세 분석 (병목 Step만)
   - 예: R-05 내부의 Action 분석
   - DB_findNearestPoliceStation: 1,420ms (97.9%)

5. 병목 원인 규명
   - 쿼리 실행 계획
   - 인덱스 사용 여부
   - 코드 패턴 분석

6. 개선 권고사항
   - 우선순위별 개선 방안
   - 예상 효과 (시간 단축)

7. 부록
   - 로그 샘플
   - 분석 스크립트
```

---

## 6. 일정 및 산출물

### 6.1. 테스트 일정

| 단계 | 작업 내용 | 소요 기간 | 담당 |
|------|----------|----------|------|
| 1. 준비 | 계측 코드 구현 및 검증 | 3일 | 개발팀 |
| 2. 환경 구성 | 테스트 서버 셋업 및 데이터 준비 | 2일 | 인프라팀 |
| 3. Smoke Test | 시나리오 1 실행 및 로그 검증 | 1일 | 테스트팀 |
| 4. 부하 테스트 | 시나리오 2~5 실행 | 2일 | 테스트팀 |
| 5. 데이터 분석 | 로그 파싱 및 병목 식별 | 2일 | 분석팀 |
| 6. 보고서 작성 | 성능 분석 보고서 작성 | 2일 | 분석팀 |
| **총계** | | **12일** | |

---

### 6.2. 산출물

**테스트 수행 중:**
1. 성능 로그 파일 (시나리오별)
2. JMeter 테스트 결과 (Summary Report, Graphs)
3. 시스템 모니터링 데이터 (CPU, Memory, I/O)

**분석 완료 후:**
1. **성능 분석 보고서** (핵심 산출물)
   - 전체 성능 지표
   - Step별/Action별 분석
   - 병목 원인 및 개선 권고
2. Python 분석 스크립트
3. 시각화 차트 모음 (PNG/PDF)
4. 쿼리 실행 계획 분석 결과

---

### 6.3. 의사결정 포인트

**테스트 완료 후 결정 사항:**

1. **병목 개선 착수 여부**
   - 조건: 목표 대비 50% 이상 Gap
   - 결정: 병목 개선 착수 / 현상 유지

2. **개선 우선순위**
   - B-01 ~ B-05 중 영향도 순 정렬
   - 리소스 투입 계획 수립

3. **추가 테스트 필요성**
   - 특정 시나리오 재실행
   - 더 큰 부하 테스트 (100 VUs 이상)

---

## 7. 참조 문서

- 위치 분석 서비스 성능 테스트 명세서 v2.1
- JMeter 사용자 가이드
- Python Pandas 데이터 분석 가이드

---

## 8. 문서 버전 관리

| 버전 | 날짜 | 작성자 | 변경 내역 |
|------|------|--------|----------|
| 1.0 | 2025-01-21 | 정범진 | 초안 (분석 보고서 형태) |
| 2.0 | 2025-01-23 | 정범진 | 계획서로 전면 재작성 - 명세서 기반 실행 계획 수립 |

---

**문서 종료**