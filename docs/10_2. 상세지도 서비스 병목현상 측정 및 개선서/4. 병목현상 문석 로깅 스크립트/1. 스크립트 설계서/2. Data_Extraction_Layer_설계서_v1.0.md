# Data Extraction Layer 설계서 v1.0

**문서 정보**
- 문서 유형: 상세 설계 문서 (Detailed Design Document)
- 작성일: 2025-01-24
- 작성자: 정범진
- 프로젝트: Wherehouse 상세지도 서비스 성능 최적화
- 상위 문서: 성능 로그 파싱 스크립트 설계 문서 v2.0
- 목적: Data Extraction Layer (데이터 추출 계층) 구현 상세 명세

---

## 목차

1. [계층 개요](#1-계층-개요)
2. [Extractor 아키텍처](#2-extractor-아키텍처)
3. [공통 함수 라이브러리](#3-공통-함수-라이브러리)
4. [중간 파일 포맷 상세](#4-중간-파일-포맷-상세)
5. [단계별 Extractor 구현 가이드](#5-단계별-extractor-구현-가이드)
6. [에러 핸들링](#6-에러-핸들링)
7. [성능 최적화](#7-성능-최적화)

---

## 1. 계층 개요

### 1.1. 책임 (Responsibility)

Data Extraction Layer는 다음 작업을 담당한다:

1. **로그 파일 읽기**: NDJSON 형식의 `wherehouse.log` 파일 로드
2. **JSON 파싱**: 각 줄을 JSON 객체로 변환
3. **필터링**: 특정 step (예: R-01)에 해당하는 로그만 추출
4. **데이터 정제**: 불필요한 필드 제거, 데이터 타입 변환
5. **resultData 추출**: 중첩된 resultData 필드를 평탄화
6. **중간 파일 저장**: 정제된 데이터를 JSON 파일로 저장

### 1.2. 입력/출력

| 항목 | 내용 |
|------|------|
| **입력** | `logs/wherehouse.log` (NDJSON 형식) |
| **출력** | `results/r0X/r0X_parsed_data.json` (구조화된 JSON) |
| **의존성** | Python 3.10+, pandas, json |

### 1.3. 실행 주기

- **초기 실행**: 로그 파일 생성 직후 1회 실행
- **재실행**: 로그 파일 업데이트 시에만 재실행
- **주기**: 보고서 생성 계층과 독립적

### 1.4. Generation Layer와의 인터페이스

**계약 (Contract)**:
- Extractor는 **정해진 JSON 포맷**으로 데이터를 저장
- Generator는 **해당 포맷을 읽을 수 있다고 가정**
- 포맷 변경 시 **양측 모두 수정 필요**

**인터페이스 정의**:
```json
{
  "metadata": { ... },
  "logs": [ ... ]
}
```

*상세 포맷은 섹션 4 참조*

---

## 2. Extractor 아키텍처

### 2.1. 전체 처리 흐름

```
┌─────────────────────────────────────────┐
│ r0X_data_extractor.py                   │
├─────────────────────────────────────────┤
│                                         │
│  1. 설정 초기화                          │
│     - 로그 파일 경로                     │
│     - 출력 디렉토리                      │
│     - step 필터 (예: 'R-01')            │
│                                         │
│  2. 로그 파일 읽기                       │
│     parse_ndjson_log()                  │
│     ├─ 파일 열기 (UTF-8)                │
│     ├─ 라인별 JSON 파싱                 │
│     ├─ step 필터링                      │
│     └─ DataFrame 변환                   │
│                                         │
│  3. 데이터 정제                          │
│     clean_log_data()                    │
│     ├─ 중복 제거                        │
│     ├─ duration_ms 계산                 │
│     └─ 타입 변환                        │
│                                         │
│  4. resultData 추출                     │
│     extract_result_data()               │
│     ├─ status='END' 필터링              │
│     ├─ resultData 필드 파싱             │
│     └─ 평탄화 (flatten)                 │
│                                         │
│  5. 메타데이터 생성                      │
│     create_metadata()                   │
│     ├─ 총 로그 개수                     │
│     ├─ 추출 시각                        │
│     └─ 버전 정보                        │
│                                         │
│  6. JSON 파일 저장                       │
│     save_to_json()                      │
│     ├─ 디렉토리 생성                    │
│     ├─ JSON 직렬화                      │
│     └─ 파일 쓰기                        │
│                                         │
└─────────────────────────────────────────┘
```

### 2.2. 모듈 구조

```python
# r01_data_extractor.py

import json
import sys
from pathlib import Path
from datetime import datetime
sys.path.append(str(Path(__file__).parent.parent / 'common'))

from extractor_utils import (
    parse_ndjson_log,
    clean_log_data,
    extract_result_data,
    create_metadata,
    save_to_json
)

def main():
    # 1. 설정
    config = {
        'step': 'R-01',
        'log_file': 'logs/wherehouse.log',
        'output_dir': 'results/r01',
        'output_file': 'r01_parsed_data.json'
    }
    
    # 2. 로그 파싱
    print(f"[{config['step']}] 로그 파싱 시작...")
    logs = parse_ndjson_log(config['log_file'], config['step'])
    print(f"[{config['step']}] 파싱 완료: {len(logs)}개 로그")
    
    # 3. 데이터 정제
    print(f"[{config['step']}] 데이터 정제 중...")
    logs = clean_log_data(logs)
    
    # 4. resultData 추출
    print(f"[{config['step']}] resultData 추출 중...")
    logs = extract_result_data(logs, config['step'])
    
    # 5. 메타데이터 생성
    metadata = create_metadata(config, logs)
    
    # 6. JSON 저장
    output_path = Path(config['output_dir']) / config['output_file']
    save_to_json({'metadata': metadata, 'logs': logs}, output_path)
    
    print(f"[{config['step']}] 추출 완료: {output_path}")

if __name__ == '__main__':
    main()
```

---

## 3. 공통 함수 라이브러리

### 3.1. extractor_utils.py 구조

```python
# common/extractor_utils.py

import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

def parse_ndjson_log(log_file: str, step_filter: str) -> List[Dict]:
    """
    NDJSON 로그 파일을 파싱하여 특정 step의 로그만 추출
    
    Args:
        log_file: 로그 파일 경로
        step_filter: 필터링할 step (예: 'R-01')
    
    Returns:
        List[Dict]: 파싱된 로그 리스트
    
    Example:
        logs = parse_ndjson_log('logs/wherehouse.log', 'R-01')
        # [{'timestamp': '...', 'step': 'R-01', ...}, ...]
    """
    logs = []
    
    with open(log_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            
            try:
                log = json.loads(line)
                
                # PERFORMANCE 이벤트이고 해당 step인 경우만 추가
                if (log.get('eventType') == 'PERFORMANCE' and 
                    log.get('step') == step_filter):
                    logs.append(log)
                    
            except json.JSONDecodeError as e:
                print(f"Warning: JSON 파싱 실패 (Line {line_num}): {e}")
                continue
    
    return logs


def clean_log_data(logs: List[Dict]) -> List[Dict]:
    """
    로그 데이터 정제
    
    처리 내용:
    - duration_ms가 없으면 duration_ns에서 계산
    - 중복 제거 (동일 traceId + action + status)
    - 타입 변환 (duration을 float로)
    
    Args:
        logs: 파싱된 로그 리스트
    
    Returns:
        List[Dict]: 정제된 로그 리스트
    """
    cleaned = []
    seen = set()
    
    for log in logs:
        # duration_ms 계산
        if 'duration_ms' not in log and 'duration_ns' in log:
            log['duration_ms'] = log['duration_ns'] / 1_000_000
        
        # 중복 제거 키 생성
        dedup_key = (
            log.get('traceId'),
            log.get('action'),
            log.get('status')
        )
        
        if dedup_key not in seen:
            seen.add(dedup_key)
            cleaned.append(log)
    
    return cleaned


def extract_result_data(logs: List[Dict], step: str) -> List[Dict]:
    """
    resultData 필드 추출 (무손실 보존 원칙)
    
    Args:
        logs: 정제된 로그 리스트
        step: 단계 (예: 'R-01')
    
    Returns:
        List[Dict]: resultData가 원본 그대로 유지된 로그 리스트
    
    CRITICAL 원칙:
        - resultData의 모든 필드를 있는 그대로 보존
        - 평탄화, 필터링, 변환 일체 금지
        - R-04처럼 amenityResults에 수백 개 데이터가 있어도 전체 보존
        - Generator에서 필요한 것만 선택적으로 추출하도록 위임
    
    이유:
        - 중간 파일은 원본 데이터의 완전한 복사본 역할
        - 나중에 다른 분석이 필요하면 중간 파일에서 재추출 가능
        - 데이터 손실은 복구 불가능하므로 절대 필터링 금지
    """
    # 아무것도 하지 않음 - 원본 그대로 반환
    return logs


def create_metadata(config: Dict, logs: List[Dict]) -> Dict:
    """
    메타데이터 생성
    
    Args:
        config: 설정 딕셔너리
        logs: 로그 리스트
    
    Returns:
        Dict: 메타데이터
    """
    end_logs = [log for log in logs if log.get('status') == 'END']
    
    return {
        'step': config['step'],
        'log_file': config['log_file'],
        'extraction_time': datetime.now().isoformat(),
        'total_logs': len(logs),
        'end_logs': len(end_logs),
        'extractor_version': '1.0'
    }


def save_to_json(data: Dict, output_path: Path):
    """
    데이터를 JSON 파일로 저장
    
    Args:
        data: 저장할 데이터 (metadata + logs)
        output_path: 출력 파일 경로
    """
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
```

### 3.2. 함수별 책임

| 함수명 | 입력 | 출력 | 책임 |
|--------|------|------|------|
| `parse_ndjson_log()` | 로그 파일 경로, step | List[Dict] | NDJSON 파싱 + 필터링 |
| `clean_log_data()` | List[Dict] | List[Dict] | 중복 제거, 타입 변환 |
| `extract_result_data()` | List[Dict], step | List[Dict] | resultData 추출 (단계별 커스텀) |
| `create_metadata()` | config, logs | Dict | 메타데이터 생성 |
| `save_to_json()` | data, path | None | JSON 파일 쓰기 |

---

## 4. 중간 파일 포맷 상세

### 4.1. 전체 구조

```json
{
  "metadata": { ... },
  "logs": [ ... ]
}
```

### 4.2. metadata 필드

| 필드명 | 타입 | 설명 | 예시 |
|--------|------|------|------|
| step | string | 단계 ID | "R-01" |
| log_file | string | 원본 로그 파일 경로 | "logs/wherehouse.log" |
| extraction_time | string | 추출 시각 (ISO 8601) | "2025-01-24T10:30:00.123456" |
| total_logs | int | 전체 로그 개수 (START + END) | 150 |
| end_logs | int | END 로그 개수 (통계 대상) | 75 |
| extractor_version | string | Extractor 버전 | "1.0" |

**예시**:
```json
{
  "metadata": {
    "step": "R-01",
    "log_file": "logs/wherehouse.log",
    "extraction_time": "2025-01-24T10:30:00.123456",
    "total_logs": 150,
    "end_logs": 75,
    "extractor_version": "1.0"
  }
}
```

### 4.3. logs 필드 (배열)

각 로그는 다음 구조를 갖는다:

**기본 필드** (모든 단계 공통):

| 필드명 | 타입 | 설명 | 예시 |
|--------|------|------|------|
| timestamp | string | 로그 생성 시각 (ISO 8601) | "2025-10-23T07:36:55.257Z" |
| nanoTime | long | System.nanoTime() | 21466959818700 |
| traceId | string | 요청 추적 ID | "27ec5af0" |
| thread | string | 실행 스레드명 | "http-nio-8185-exec-2" |
| eventType | string | 이벤트 타입 (항상 "PERFORMANCE") | "PERFORMANCE" |
| step | string | 처리 단계 | "R-01" |
| layer | string | 레이어 | "Service", "Utility" |
| class | string | 클래스명 | "LocationAnalysisServiceImpl" |
| method | string | 메서드명 | "calculate9BlockGrid" |
| action | string | 액션명 | "calculate9BlockGrid" |
| status | string | 로그 상태 | "START" 또는 "END" |
| duration_ns | long | 실행 시간 (나노초, END만 존재) | 3245000 |
| duration_ms | float | 실행 시간 (밀리초, END만 존재) | 3.245 |

**resultData 필드** (END 로그만 존재, 단계별로 다름):

```json
{
  "resultData": {
    "requestLatitude": 37.5663,
    "requestLongitude": 126.9779,
    "centerGeohashId": "wydm9qw",
    "nineBlockGeohashes": ["wydm9qw", "wydm9qy", "..."],
    "totalGridCount": 9,
    "isSuccess": true,
    "errorMessage": null
  }
}
```

### 4.4. 완전한 예시 (R-01)

```json
{
  "metadata": {
    "step": "R-01",
    "log_file": "logs/wherehouse.log",
    "extraction_time": "2025-01-24T10:30:00.123456",
    "total_logs": 150,
    "end_logs": 75,
    "extractor_version": "1.0"
  },
  "logs": [
    {
      "timestamp": "2025-10-23T07:36:55.257935200Z",
      "nanoTime": 21466959818700,
      "traceId": "27ec5af0",
      "thread": "http-nio-8185-exec-2",
      "eventType": "PERFORMANCE",
      "step": "R-01",
      "layer": "Service",
      "class": "LocationAnalysisServiceImpl",
      "method": "calculate9BlockGrid",
      "action": "calculate9BlockGrid",
      "status": "START"
    },
    {
      "timestamp": "2025-10-23T07:36:55.280120500Z",
      "nanoTime": 21466981941700,
      "traceId": "27ec5af0",
      "thread": "http-nio-8185-exec-2",
      "eventType": "PERFORMANCE",
      "step": "R-01",
      "layer": "Service",
      "class": "LocationAnalysisServiceImpl",
      "method": "calculate9BlockGrid",
      "action": "calculate9BlockGrid",
      "status": "END",
      "duration_ns": 3245000,
      "duration_ms": 3.245,
      "resultData": {
        "requestLatitude": 37.5663,
        "requestLongitude": 126.9779,
        "requestRadius": 500,
        "centerGeohashId": "wydm9qw",
        "nineBlockGeohashes": [
          "wydm9qw", "wydm9qy", "wydm9qz", "wydm9qx",
          "wydm9qr", "wydm9qq", "wydm9qm", "wydm9qt", "wydm9qv"
        ],
        "totalGridCount": 9,
        "isSuccess": true,
        "errorMessage": null
      }
    }
  ]
}
```

---

## 5. 단계별 Extractor 구현 가이드

### 5.1. R-01 ~ R-07 공통 템플릿

모든 Extractor는 동일한 구조를 따른다:

```python
# r0X_data_extractor.py

import json
import sys
from pathlib import Path
from datetime import datetime
sys.path.append(str(Path(__file__).parent.parent / 'common'))

from extractor_utils import (
    parse_ndjson_log,
    clean_log_data,
    extract_result_data,
    create_metadata,
    save_to_json
)

def main():
    # 설정 (단계별로 step, output_dir, output_file만 변경)
    config = {
        'step': 'R-0X',  # ← 변경
        'log_file': 'logs/wherehouse.log',
        'output_dir': 'results/r0X',  # ← 변경
        'output_file': 'r0X_parsed_data.json'  # ← 변경
    }
    
    # 2-6단계는 모든 Extractor에서 동일
    print(f"[{config['step']}] 로그 파싱 시작...")
    logs = parse_ndjson_log(config['log_file'], config['step'])
    print(f"[{config['step']}] 파싱 완료: {len(logs)}개 로그")
    
    print(f"[{config['step']}] 데이터 정제 중...")
    logs = clean_log_data(logs)
    
    print(f"[{config['step']}] resultData 추출 중...")
    logs = extract_result_data(logs, config['step'])
    
    metadata = create_metadata(config, logs)
    
    output_path = Path(config['output_dir']) / config['output_file']
    save_to_json({'metadata': metadata, 'logs': logs}, output_path)
    
    print(f"[{config['step']}] 추출 완료: {output_path}")

if __name__ == '__main__':
    main()
```

### 5.2. 단계별 차이점

**유일한 차이**: config의 `step`, `output_dir`, `output_file` 값만 변경

| 단계 | step | output_dir | output_file |
|------|------|-----------|-------------|
| R-01 | `'R-01'` | `'results/r01'` | `'r01_parsed_data.json'` |
| R-02 | `'R-02'` | `'results/r02'` | `'r02_parsed_data.json'` |
| ... | ... | ... | ... |
| R-07 | `'R-07'` | `'results/r07'` | `'r07_parsed_data.json'` |

**모든 로직은 공통 함수(extractor_utils.py)에서 처리하므로 코드 중복 없음**

---

## 6. 에러 핸들링

### 6.1. 로그 파싱 오류

**문제**: NDJSON 파일의 특정 줄이 잘못된 JSON 형식

**처리**:
```python
try:
    log = json.loads(line)
except json.JSONDecodeError as e:
    print(f"Warning: JSON 파싱 실패 (Line {line_num}): {e}")
    continue  # 해당 줄 스킵, 다음 줄 계속 처리
```

**원칙**: 일부 로그 파싱 실패는 전체 프로세스를 중단하지 않음

### 6.2. 파일 없음 오류

**문제**: 로그 파일이 존재하지 않음

**처리**:
```python
def parse_ndjson_log(log_file: str, step_filter: str) -> List[Dict]:
    if not Path(log_file).exists():
        raise FileNotFoundError(f"로그 파일을 찾을 수 없습니다: {log_file}")
    # ...
```

**원칙**: 치명적 오류는 즉시 예외 발생

### 6.3. resultData 누락 오류

**문제**: END 로그에 resultData 필드가 없음

**처리**:
```python
if log.get('status') == 'END' and 'resultData' not in log:
    print(f"Warning: resultData 누락 (traceId={log.get('traceId')})")
    log['resultData'] = {}  # 빈 객체로 대체
```

**원칙**: 데이터 품질 문제는 경고 출력 후 기본값 사용

---

## 7. 성능 최적화

### 7.1. 메모리 효율

**문제**: 대용량 로그 파일 (수만 줄) 처리 시 메모리 부족

**해결**: 라인별 스트리밍 처리 (이미 적용됨)
```python
# ✓ Good: 라인별 읽기
with open(log_file, 'r') as f:
    for line in f:
        log = json.loads(line)
        # 처리

# ✗ Bad: 전체 파일 읽기
with open(log_file, 'r') as f:
    content = f.read()  # 메모리 폭증
```

### 7.2. 실행 시간

**현재 성능** (예상):
- 로그 1,000개: ~0.5초
- 로그 10,000개: ~5초
- 로그 100,000개: ~50초

**최적화 불필요 이유**:
- Extractor는 1회만 실행
- Generator는 수십 번 재실행 가능하므로 Generator 최적화가 더 중요

---

## 8. 검증 방법

### 8.1. Extractor 출력 검증

```bash
# 1. 파일 생성 확인
ls -lh results/r01/r01_parsed_data.json

# 2. JSON 유효성 검사
python -m json.tool results/r01/r01_parsed_data.json > /dev/null
echo "JSON 유효성: $?"

# 3. 메타데이터 확인
cat results/r01/r01_parsed_data.json | jq '.metadata'

# 4. 로그 개수 확인
cat results/r01/r01_parsed_data.json | jq '.logs | length'

# 5. END 로그만 필터링
cat results/r01/r01_parsed_data.json | jq '.logs[] | select(.status == "END")'
```

### 8.2. 단위 테스트 (선택)

```python
# tests/test_extractor_utils.py

import pytest
from common.extractor_utils import parse_ndjson_log, clean_log_data

def test_parse_ndjson_log():
    logs = parse_ndjson_log('tests/sample.log', 'R-01')
    assert len(logs) > 0
    assert all(log['step'] == 'R-01' for log in logs)

def test_clean_log_data():
    logs = [
        {'traceId': 'a', 'action': 'test', 'status': 'END', 'duration_ns': 1000000},
        {'traceId': 'a', 'action': 'test', 'status': 'END', 'duration_ns': 1000000}  # 중복
    ]
    cleaned = clean_log_data(logs)
    assert len(cleaned) == 1  # 중복 제거
    assert cleaned[0]['duration_ms'] == 1.0  # ns → ms 변환
```

---

## 9. 변경 이력

| 버전 | 날짜 | 작성자 | 변경 내역 |
|------|------|--------|----------|
| 1.0 | 2025-01-24 | 정범진 | 초안 작성 - Data Extraction Layer 상세 설계 |

---

**문서 종료**
