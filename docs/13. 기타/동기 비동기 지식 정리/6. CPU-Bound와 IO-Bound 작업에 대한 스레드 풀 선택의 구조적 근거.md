# CPU-Bound와 I/O-Bound 작업에 대한 스레드 풀 선택의 구조적 근거

## 문서 개요

본 문서는 ForkJoinPool과 ThreadPoolExecutor의 내부 메커니즘을 데이터 구조, 작업 스케줄링 알고리즘, 그리고 하드웨어 최적화 관점에서 상세히 분석한다. "CPU-bound는 ForkJoinPool, I/O-bound는 ThreadPoolExecutor"라는 권장사항이 단순한 베스트 프랙티스가 아닌, 각 스레드 풀의 설계 목표와 작업 특성 간의 구조적 적합성에서 비롯됨을 증명한다.

---

## 1. 서론: 스레드 풀 선택이 단순한 관례가 아닌 이유

### 1.1 문제 제기

"CPU-bound는 ForkJoinPool, I/O-bound는 ThreadPoolExecutor"라는 권장사항은 Java 개발자들 사이에서 널리 알려져 있지만, 이 선택의 구조적 근거를 명확히 이해하는 경우는 드물다. 대부분의 설명은 "ForkJoinPool은 work-stealing을 사용한다", "I/O-bound는 많은 스레드가 필요하다" 정도의 표면적인 수준에 머문다.

### 1.2 핵심 질문

왜 CPU-bound 작업은 ForkJoinPool을 사용해야 하는가? 왜 I/O-bound 작업은 ThreadPoolExecutor를 사용해야 하는가? 이 질문에 대한 답은 각 스레드 풀의 **내부 작업 관리 메커니즘**과 **작업 패턴의 특성** 간의 구조적 적합성을 분석해야만 얻을 수 있다.

### 1.3 분석 방법론

본 문서는 다음 관점에서 두 스레드 풀을 비교 분석한다:

1. **데이터 구조**: Per-worker deque vs 공유 BlockingQueue
2. **작업 스케줄링**: Work-stealing vs FIFO
3. **동시성 제어**: Lock-free CAS vs Mutex-based blocking
4. **하드웨어 최적화**: 캐시 지역성, 컨텍스트 스위칭
5. **작업 특성**: 재귀적 분할, 블로킹 시간, 실행 시간 예측 가능성

---

## 2. 본론: ForkJoinPool과 ThreadPoolExecutor의 메커니즘 분석

### 2.1 ForkJoinPool의 내부 구조: Work-Stealing과 재귀적 작업 분할에 최적화

#### 2.1.1 ForkJoinPool의 핵심 데이터 구조

ForkJoinPool은 각 Worker 스레드마다 **개별 작업 큐(Deque, Double-Ended Queue)**를 유지한다. 이는 ThreadPoolExecutor의 단일 공유 큐와 근본적으로 다른 구조다.

**구조 다이어그램:**

```
ForkJoinPool 구조:

Worker Thread 1         Worker Thread 2         Worker Thread 3
┌──────────────┐       ┌──────────────┐       ┌──────────────┐
│  Work Deque  │       │  Work Deque  │       │  Work Deque  │
│ ┌──────────┐ │       │ ┌──────────┐ │       │ ┌──────────┐ │
│ │ Task A   │ │       │ │ Task D   │ │       │ │ Task G   │ │
│ │ Task B   │ │       │ │ Task E   │ │       │ │          │ │
│ │ Task C   │ │       │ │ Task F   │ │       │ │          │ │
│ └──────────┘ │       │ └──────────┘ │       │ └──────────┘ │
└──────────────┘       └──────────────┘       └──────────────┘
      ↓                       ↑                       ↑
   LIFO (Push/Pop)         FIFO (Steal)           FIFO (Steal)
   자신의 작업 처리         다른 Worker의 작업 훔치기
```

#### 2.1.2 작업 추가 메커니즘

**자신의 작업 생성 시:**
Worker 스레드가 새로운 작업(fork())을 생성하면, 자신의 deque의 **head(상단)**에 push한다. 자신의 다음 작업을 가져올 때는 deque의 **head에서 pop**한다 (LIFO - Last In First Out).

**다른 Worker의 작업 훔치기:**
다른 Worker가 작업을 훔쳐갈 때는 deque의 **tail(하단)에서 steal**한다 (FIFO - First In First Out).

#### 2.1.3 이 구조의 설계 의도

**의도 1: 재귀 분할 지원**

분할 정복(Divide and Conquer) 알고리즘에서 작업 A가 A1, A2로 분할되면, A1을 먼저 처리하고 완료한 후 A2를 처리하는 것이 메모리 접근 패턴상 유리하다. LIFO는 이를 자연스럽게 지원한다.

예시:
```
Task(0, 1000) 실행
→ Task(0, 500).fork()  // deque에 push
→ Task(500, 1000).compute()  // 즉시 실행
  → Task(500, 750).fork()  // deque에 push (위에 쌓임)
  → Task(750, 1000).compute()  // 즉시 실행
  ...

deque 상태: [Task(0, 500), Task(500, 750), ...]
다음 처리: Task(500, 750) (가장 최근에 생성된 작업)
```

각 Worker가 자신의 deque의 head만 접근(push/pop)하고, 다른 Worker는 tail만 접근(steal)하므로, 대부분의 경우 락 없이 동작할 수 있다. 이는 Compare-And-Swap(CAS) 같은 가벼운 동기화 메커니즘만으로 충분하다.

동시 접근 시나리오:
- Worker 1: 자신의 deque head에서 pop → 락 불필요
- Worker 2: Worker 1의 deque tail에서 steal → CAS만 필요
- 충돌 확률 낮음: head와 tail이 물리적으로 분리됨

#### 2.1.4 Work-Stealing 알고리즘의 동작

**의사 코드:**

```java
// ForkJoinPool의 핵심 로직
void runWorker() {
    while (true) {
        ForkJoinTask task = null;
        
        // 1단계: 자신의 deque에서 작업 가져오기 (LIFO)
        task = myDeque.pop(); // head에서 pop
        
        if (task == null) {
            // 2단계: 다른 Worker의 deque에서 작업 훔치기 (FIFO)
            for (WorkQueue q : allWorkerQueues) {
                task = q.steal(); // tail에서 steal
                if (task != null) break;
            }
        }
        
        if (task == null) {
            // 3단계: 모든 큐가 비었으면 대기 또는 종료
            waitForWork();
        } else {
            task.exec(); // 작업 실행
        }
    }
}
```

#### 2.1.5 Work-Stealing이 효율적인 시나리오

Work-stealing은 다음 조건에서 최대 효율을 발휘한다:

1. **작업이 재귀적으로 분할**되어 새로운 하위 작업을 계속 생성하는 경우
2. **각 작업의 실행 시간이 짧고 예측 가능**한 경우
3. **작업 간 데이터 지역성**이 존재하는 경우 (부모 작업과 자식 작업이 같은 데이터를 사용)

#### 2.1.6 CPU-Bound 작업이 이 패턴에 부합하는 이유

**예시: 큰 배열을 병렬 정렬하는 작업**

```java
class SortTask extends RecursiveAction {
    int[] array;
    int left, right;
    
    protected void compute() {
        if (right - left < THRESHOLD) {
            Arrays.sort(array, left, right); // CPU 집약적 작업
        } else {
            int mid = (left + right) / 2;
            SortTask leftTask = new SortTask(array, left, mid);
            SortTask rightTask = new SortTask(array, mid, right);
            
            leftTask.fork();  // Worker의 deque head에 push
            rightTask.compute(); // 현재 스레드에서 직접 실행
            leftTask.join();  // leftTask 완료 대기
        }
    }
}
```

**이 코드의 실행 흐름:**

1. Worker 1이 전체 배열 정렬 작업을 받음
2. 작업을 leftTask와 rightTask로 분할
3. leftTask를 자신의 deque에 push (나중을 위해)
4. rightTask를 즉시 실행 → 또다시 분할 → 더 작은 작업들이 deque에 계속 쌓임
5. Worker 2가 유휴 상태면 Worker 1의 deque tail에서 leftTask를 steal
6. 양쪽 Worker가 독립적으로 작업 분할과 처리 반복

**왜 이 메커니즘이 CPU-Bound에 최적인가:**

**이유 1: 작업 생성 비용 최소화**

작업 분할이 빈번하게 일어나므로, deque에 직접 push하는 것이 공유 큐에 넣는 것보다 훨씬 빠르다. 락 경쟁이 없기 때문이다.

**이유 2: 캐시 친화적**

Worker가 자신이 방금 만든 작업을 즉시 처리하므로, 작업과 관련된 데이터(배열의 특정 부분)가 CPU 캐시에 남아있다.

**이유 3: 동적 부하 분산**

어떤 Worker의 작업이 더 많이 분할되면, 유휴 Worker들이 자동으로 그쪽에서 작업을 가져간다.

**이유 4: 블로킹 없음**

모든 작업이 CPU-bound이므로 Worker 스레드가 블로킹되지 않고, work-stealing이 정상 작동한다.

#### 2.1.7 ForkJoinPool이 I/O-Bound 작업에 부적합한 구조적 이유

**I/O 작업의 특성:**
```java
// I/O 작업 예시
CompletableFuture.supplyAsync(() -> {
    return httpClient.send(request); // 여기서 스레드 블로킹
}, forkJoinPool);
```

**주의: CompletableFuture의 기본 실행자**

CompletableFuture는 executor를 명시하지 않으면 `ForkJoinPool.commonPool()`을 사용한다:
```java
// 명시적 executor 지정 없음 → ForkJoinPool.commonPool() 사용
CompletableFuture.supplyAsync(() -> httpClient.send(request));

// 위 코드는 다음과 동일:
CompletableFuture.supplyAsync(() -> httpClient.send(request), 
                               ForkJoinPool.commonPool());
```

이는 I/O 작업에 ForkJoinPool을 사용하게 되는 흔한 원인이다. 개발자가 의도하지 않았어도 기본 동작으로 인해 부적절한 스레드 풀이 선택된다. 따라서 I/O 작업에는 **반드시 명시적으로 ThreadPoolExecutor 기반 executor를 지정**해야 한다:
```java
ExecutorService ioExecutor = Executors.newFixedThreadPool(20);

// 올바른 I/O 작업 실행
CompletableFuture.supplyAsync(() -> httpClient.send(request), 
                               ioExecutor); // 명시적 지정 필수
```

**구조적 불일치:**

**불일치 1: 작업 분할이 일어나지 않음**

I/O 작업은 재귀적으로 분할되지 않는다. HTTP 요청 하나는 하나의 독립적인 작업이다. Worker의 deque는 사실상 빈 상태로 유지되므로, work-stealing이 작동할 기회가 없다.

**불일치 2: 긴 블로킹 시간**

Worker가 `httpClient.send()`에서 블로킹되면, 해당 스레드는 커널 I/O 대기 상태로 진입한다. 이 Worker의 deque에 다른 작업이 없으므로, 다른 Worker가 훔쳐갈 것도 없다. ForkJoinPool은 이 Worker가 블로킹되었는지 알 수 없으므로, 새 Worker를 생성하지 않는다.

**불일치 3: 스레드 수 고정의 문제**

ForkJoinPool의 기본 parallelism은 CPU 코어 수 (예: 4개)다. 일반적인 블로킹 I/O (Thread.sleep(), 기본 HTTP 클라이언트 등)는 ForkJoinPool이 이를 감지하지 못하므로, 4개 Worker가 모두 블로킹되면 새 작업을 처리할 스레드가 없다. CPU는 유휴 상태지만 작업은 큐에서 대기한다.

**불일치 4: Work-Stealing의 무용화**

Work-stealing은 "빠른 작업 처리 + 새로운 작업 생성"의 동적 균형을 위한 메커니즘이다. I/O 작업은 "느린 대기 + 작업 생성 없음"이므로, stealing할 가치 있는 작업이 없다.

**구체적 시나리오:**
```
시간 0ms: Worker 1-4가 각각 HTTP 요청 1-4를 시작
시간 1ms: 모든 Worker가 커널 I/O 대기 상태 진입
시간 50ms: 새로운 HTTP 요청 5-10이 ForkJoinPool의 제출 큐에 도착
시간 50ms: 모든 Worker가 여전히 블로킹 중이므로 요청 5-10은 처리되지 않음
시간 200ms: Worker 1의 HTTP 응답 도착 → Ready Queue → 스케줄러 선택 → CPU 할당
시간 201ms: Worker 1이 요청 1 처리 완료 후 요청 5를 가져와 시작
시간 202ms: Worker 1이 다시 I/O 블로킹

결과: 10개의 HTTP 요청을 처리하는데 400ms+ 소요 (실제로는 병렬 처리 가능한 작업임)
```

### 2.2 I/O 작업의 분할 가능성과 ForkJoinPool 부적합성의 정확한 이유

#### 2.2.1 I/O 작업이 실제로 분할되는 패턴

이전 절에서 "I/O 작업은 재귀적으로 분할되지 않는다"고 했지만, 이는 불완전한 설명이다. 실제 I/O 워크플로우는 복잡하며, 분할될 수 있다.

**패턴 1: 병렬 I/O 실행**

실제 서비스에서 흔한 패턴으로, 여러 I/O를 병렬로 실행하는 경우:

```java
// 실제 서비스에서 흔한 패턴
public ProductDetailResponse getProductDetail(Long productId) {
    CompletableFuture<Product> productFuture = 
        CompletableFuture.supplyAsync(() -> 
            productRepository.findById(productId), // DB I/O
            ioPool
        );
    
    CompletableFuture<List<Review>> reviewsFuture = 
        CompletableFuture.supplyAsync(() -> 
            reviewRepository.findByProductId(productId), // DB I/O
            ioPool
        );
    
    CompletableFuture<Stock> stockFuture = 
        CompletableFuture.supplyAsync(() -> 
            inventoryClient.getStock(productId), // HTTP I/O
            ioPool
        );
    
    CompletableFuture<Price> priceFuture = 
        CompletableFuture.supplyAsync(() -> 
            pricingClient.getPrice(productId), // HTTP I/O
            ioPool
        );
    
    // 모든 I/O 완료 대기
    CompletableFuture.allOf(productFuture, reviewsFuture, stockFuture, priceFuture)
        .join();
    
    return buildResponse(
        productFuture.join(),
        reviewsFuture.join(),
        stockFuture.join(),
        priceFuture.join()
    );
}
```

이 코드는 명확히 작업을 분할하고 있다. 하나의 "상품 상세 조회" 작업이 4개의 하위 I/O 작업으로 나뉘었다.

**패턴 2: 재귀적 I/O (페이지네이션, 계층 구조 탐색)**

```java
// 재귀적 I/O의 실제 예시: 조직도 전체 조회
public CompletableFuture<Organization> loadOrganizationTree(Long orgId, ExecutorService pool) {
    return CompletableFuture.supplyAsync(() -> {
        // DB에서 현재 조직 정보 조회 - I/O
        Organization org = orgRepository.findById(orgId);
        return org;
    }, pool)
    .thenCompose(org -> {
        // 하위 조직 ID 목록 조회 - I/O
        List<Long> childIds = orgRepository.findChildIds(orgId);
        
        // 각 하위 조직을 재귀적으로 조회 - 작업 분할
        List<CompletableFuture<Organization>> childFutures = childIds.stream()
            .map(childId -> loadOrganizationTree(childId, pool)) // 재귀 호출
            .collect(Collectors.toList());
        
        // 모든 하위 조직 완료 대기
        return CompletableFuture.allOf(childFutures.toArray(new CompletableFuture[0]))
            .thenApply(v -> {
                List<Organization> children = childFutures.stream()
                    .map(CompletableFuture::join)
                    .collect(Collectors.toList());
                org.setChildren(children);
                return org;
            });
    });
}
```

이것은 재귀적으로 분할되는 I/O 작업이다: 부모 조직 조회 → 하위 조직 목록 조회 → 각 하위에 대해 재귀 호출.

#### 2.2.2 ForkJoinPool으로 I/O 작업 분할을 시도할 때의 실제 문제

위의 조직도 조회를 ForkJoinPool로 구현한다면:

```java
class OrganizationLoadTask extends RecursiveTask<Organization> {
    private final Long orgId;
    
    @Override
    protected Organization compute() {
        // 1단계: 현재 조직 정보 조회 - I/O 블로킹
        Organization org = orgRepository.findById(orgId); // 여기서 스레드 블로킹
        
        // 2단계: 하위 조직 ID 조회 - I/O 블로킹  
        List<Long> childIds = orgRepository.findChildIds(orgId); // 여기서도 블로킹
        
        // 3단계: 하위 작업들 fork
        List<OrganizationLoadTask> childTasks = childIds.stream()
            .map(OrganizationLoadTask::new)
            .collect(Collectors.toList());
        
        // 4단계: fork 및 join
        childTasks.forEach(ForkJoinTask::fork);
        
        List<Organization> children = childTasks.stream()
            .map(ForkJoinTask::join) // 각 하위 작업 완료 대기 - 블로킹
            .collect(Collectors.toList());
        
        org.setChildren(children);
        return org;
    }
}

// 실행
ForkJoinPool pool = new ForkJoinPool(4);
Organization result = pool.invoke(new OrganizationLoadTask(rootOrgId));
```

**이 코드의 실제 동작 분석:**

```
시간 0ms: Worker 1이 OrganizationLoadTask(root) 실행 시작
시간 1ms: Worker 1이 orgRepository.findById(root) 호출
시간 1ms: Worker 1이 커널 I/O 대기 상태 진입 (JDBC 소켓 읽기 대기)
         Worker 1의 deque: [비어있음]
         
시간 50ms: DB에서 root 조직 정보 응답 도착
시간 50ms: 커널이 Worker 1을 Ready Queue에 삽입
시간 51ms: 스케줄러가 Worker 1에게 CPU 할당
시간 51ms: Worker 1이 orgRepository.findChildIds(root) 호출
시간 51ms: Worker 1이 다시 커널 I/O 대기 상태 진입
         Worker 1의 deque: [비어있음]
         
시간 100ms: 하위 조직 ID 목록 [child1, child2, child3] 응답 도착
시간 101ms: Worker 1이 3개의 하위 작업 생성 및 fork
         Worker 1의 deque: [Task(child1), Task(child2), Task(child3)]
         
시간 102ms: Worker 2가 Work-Stealing으로 Task(child3)를 steal
         Worker 1의 deque: [Task(child1), Task(child2)]
         Worker 2가 Task(child3) 실행 시작 → DB I/O → 블로킹
         
시간 102ms: Worker 1이 childTasks.stream().map(ForkJoinTask::join) 실행
         첫 번째 join() 호출 → Task(child1)이 아직 fork되지 않았거나 실행 중
         Worker 1이 Task(child1) 실행 시작 → DB I/O → 블로킹
         
시간 150ms: Worker 1의 Task(child1) DB 응답 도착 → 하위 조직 없음 → 완료
시간 151ms: Worker 1이 두 번째 join() 호출 → Task(child2) 대기
         Worker 3이 Task(child2)를 steal하여 실행 시작 → DB I/O → 블로킹
         
시간 200ms: Worker 3의 Task(child2) 완료
시간 200ms: Worker 1의 join() 반환
시간 200ms: Worker 1이 세 번째 join() 호출 → Task(child3) 대기
         Worker 2가 아직 Task(child3) 실행 중 (DB I/O 블로킹)
         Worker 1은 Task(child3) 완료를 기다리며 블로킹

시간 250ms: Worker 2의 Task(child3) 완료
시간 250ms: Worker 1의 join() 반환 → 최종 결과 조합 후 반환
```

**구조적 문제 분석:**

**문제 1: 블로킹 시 Work-Stealing 무용화**

```
Worker 1이 DB I/O 블로킹 중:
- Worker 1의 deque: [비어있음] (이미 fork한 작업들은 다른 Worker가 가져감)
- Worker 2-4는 steal할 작업이 없으므로 유휴 상태
- Worker 1이 블로킹에서 깨어나기 전까지 새로운 작업 생성 불가
```

ForkJoinPool의 work-stealing은 **"Worker가 빠르게 작업을 처리하면서 계속 새로운 하위 작업을 생성한다"**는 전제 위에 설계되었다. 

CPU-bound 작업에서는:
```
Worker 1이 QuickSort 실행:
- 배열 분할 (1ms) → Task(left), Task(right) 생성 및 fork
- deque에 Task(left) push
- Task(right) 직접 실행 → 또 분할 → 새로운 작업들 fork
- 이 과정이 빠르게 반복되므로 deque가 계속 채워짐
- Worker 2-4가 steal할 기회가 풍부
```

하지만 I/O-bound 작업에서는:
```
Worker 1이 OrganizationLoadTask 실행:
- DB 조회 (100ms 블로킹) → 하위 작업 생성 (1ms) → fork
- deque가 채워지는 속도 << 다른 Worker가 steal하는 속도
- 대부분의 시간을 블로킹으로 보내므로 새 작업 생성 빈도 매우 낮음
```

**문제 2: join()에서의 부모 스레드 블로킹**

```java
childTasks.forEach(ForkJoinTask::fork); // 하위 작업들 제출
List<Organization> children = childTasks.stream()
    .map(ForkJoinTask::join) // 여기서 부모 Worker가 블로킹됨
    .collect(Collectors.toList());
```

ForkJoinPool의 `join()` 구현은 **"대기 중인 작업이 있으면 현재 스레드가 그걸 처리한다"**는 로직을 포함한다:

```java
// ForkJoinTask.join()의 단순화된 로직
public final V join() {
    int s = doJoin(); // 작업 상태 확인
    if (s != NORMAL) {
        // 작업이 아직 완료되지 않음
        // 다른 작업을 처리하면서 대기 (work-helping)
        ForkJoinPool.helpStealer(...);
    }
    return getRawResult();
}
```

**CPU-bound 작업에서 이 메커니즘이 효율적인 이유:**

join() 대기 중 부모 Worker가 다른 작업을 처리한다 (deque에서 pop 또는 steal). 처리하는 작업들도 빠르게 완료되므로 (CPU-bound) 곧 join 대상 작업도 완료된다. CPU 유휴 시간이 최소화된다.

**I/O-bound 작업에서 이 메커니즘이 비효율적인 이유:**

join() 대기 중 다른 작업을 가져와도, 그 작업도 곧 I/O 블로킹에 진입한다. 결국 부모 Worker도 자식 작업이 I/O에서 반환되기를 기다리며 블로킹된다. 이는 일반적인 "스레드가 블로킹되어 기다린다"와 동일한 상황이다.

**문제 3: 스레드 수 부족으로 인한 병렬성 제약**

조직도 조회 시나리오:
- Root 조직 아래 10개 부서, 각 부서 아래 평균 5개 팀 = 총 60개 조직
- ForkJoinPool(4 스레드) 사용

**실제 실행 흐름:**

```
시간 0-100ms: 4개 Worker가 각각 조직 1-4 조회 시작, 모두 I/O 블로킹
시간 100ms: Worker 1의 조직 1 조회 완료, 하위 5개 팀 조회 시작
시간 100ms: Worker 1이 또 I/O 블로킹, 나머지 Worker도 여전히 블로킹
...
총 소요 시간: 약 60개 조직 × 100ms / 4개 스레드 = 1500ms
```

**ThreadPoolExecutor(20 스레드) 사용 시:**

```
시간 0-100ms: 20개 Worker가 각각 조직 1-20 조회 시작, 모두 I/O 블로킹
시간 100ms: 20개 Worker가 동시에 깨어나 다음 조직들 조회 시작
...
총 소요 시간: 약 60개 조직 × 100ms / 20개 스레드 = 300ms
```

**5배 차이의 근본 원인:** I/O 블로킹 시간 동안 충분한 수의 병렬 작업을 실행할 수 있는 스레드가 없다.

#### 2.2.3 정확한 결론: I/O 작업 분할의 존재와 ForkJoinPool 부적합성의 실제 이유

**I/O 작업은 분할될 수 있다** (이전 설명 수정):
- 병렬 I/O 실행 (여러 API 동시 호출)
- 재귀적 I/O (계층 구조 탐색, 페이지네이션)
- 파이프라인 처리 (I/O → 계산 → I/O)

**그러나 ForkJoinPool이 부적합한 실제 이유:**

1. **블로킹 지속성**: 분할된 각 하위 작업도 대부분 I/O 블로킹을 포함하므로, Worker 스레드들이 대부분의 시간을 커널 대기 상태로 보낸다.

2. **작업 생성 빈도**: I/O 블로킹 시간(수십~수백 ms) >> 작업 분할 시간(수 ms)이므로, deque가 빠르게 고갈되고 work-stealing이 작동할 기회가 적다.

3. **스레드 수 제약**: CPU 코어 수(4개)로 제한된 스레드로는 수십 개의 I/O 작업을 동시에 대기시킬 수 없어 병렬성이 부족하다.

4. **join() 블로킹**: 부모 작업이 자식 작업을 기다릴 때, 자식들도 I/O 블로킹 중이므로 부모도 결국 블로킹되어 work-helping 메커니즘이 무의미하다.

### 2.3 ThreadPoolExecutor의 내부 구조: 독립 작업의 순차 처리에 최적화

#### 2.3.1 ThreadPoolExecutor의 핵심 데이터 구조

ThreadPoolExecutor는 **단일 공유 BlockingQueue**를 사용한다.

**구조 다이어그램:**

```
ThreadPoolExecutor 구조:

           Shared Work Queue (BlockingQueue)
           ┌─────────────────────────┐
           │ Task 1 → Task 2 → ... → Task N │
           └─────────────────────────┘
                 ↑         ↑         ↑
                 │         │         │
         ┌───────┴────┬────┴────┬────┴──────┐
         │            │         │           │
    Worker 1      Worker 2  Worker 3    Worker N
    (Thread)      (Thread)  (Thread)    (Thread)
```

#### 2.3.2 작업 처리 메커니즘

**의사 코드:**

```java
// ThreadPoolExecutor의 핵심 로직
void runWorker() {
    while (true) {
        Runnable task = workQueue.take(); // BlockingQueue에서 가져오기 (블로킹)
        
        if (task == null) {
            break; // 종료 신호
        }
        
        task.run(); // 작업 실행
    }
}
```

**BlockingQueue의 동작:**

- `take()`: 큐가 비어있으면 작업이 추가될 때까지 스레드를 블로킹한다.
- `put()`: 큐가 가득 차면 공간이 생길 때까지 블로킹한다.
- **FIFO 순서**: 제출된 순서대로 작업을 처리한다.

#### 2.3.3 ThreadPoolExecutor가 I/O-Bound 작업에 적합한 이유

**I/O 작업의 특성:**
- 각 작업은 독립적 (서로 다른 API 엔드포인트 호출)
- 작업 분할이 빈번하지 않음 (HTTP 요청 하나 = 작업 하나)
- 실행 시간이 길고 대부분 I/O 대기 (수십~수백 ms)
- 작업 간 데이터 지역성 없음

**ThreadPoolExecutor의 장점:**

**장점 1: 스레드 수 유연성**

```java
ExecutorService ioPool = Executors.newFixedThreadPool(20);
// 또는
ThreadPoolExecutor ioPool = new ThreadPoolExecutor(
    10,  // corePoolSize: 기본 스레드 수
    100, // maximumPoolSize: 최대 스레드 수
    60L, TimeUnit.SECONDS, // 유휴 스레드 유지 시간
    new LinkedBlockingQueue<>(1000) // 작업 큐
);
```

CPU 코어 수와 무관하게 스레드 수를 설정할 수 있다. I/O-bound 작업은 대부분 시간을 커널 대기에 소비하므로, 스레드 수 >> CPU 코어 수가 최적이다. 예를 들어 4코어 CPU에서 20개 스레드를 사용하면, 일부가 I/O 블로킹되어도 다른 스레드들이 새 작업을 처리할 수 있다.

**장점 2: 단순한 작업 큐 관리**

모든 작업이 단일 큐에 FIFO 순서로 들어간다. 작업 분할이 없으므로, deque의 복잡한 push/pop/steal 메커니즘이 불필요하다. BlockingQueue의 `take()`가 자동으로 유휴 스레드를 깨운다.

**장점 3: I/O 블로킹에 강건함**

```
시간 0ms: Worker 1-20이 각각 HTTP 요청 1-20을 시작
시간 1ms: 모든 Worker가 커널 I/O 대기 상태 진입
시간 50ms: Worker 1-5의 HTTP 응답 도착 → 작업 완료
시간 51ms: Worker 1-5가 즉시 새로운 작업 21-25를 큐에서 가져와 시작
시간 100ms: Worker 6-10의 응답 도착 → 작업 26-30 시작
...

결과: 충분한 스레드 풀 덕분에 I/O 블로킹이 전체 처리량에 미치는 영향 최소화
```

**장점 4: 컨텍스트 스위칭 비용의 재해석**

CPU-bound 작업에서는 컨텍스트 스위칭이 순수 오버헤드다 (모든 스레드가 CPU를 원함). I/O-bound 작업에서는 대부분의 스레드가 I/O 대기 중이므로, 실제로 CPU를 놓고 경쟁하는 스레드는 소수다. 따라서 많은 스레드를 사용해도 컨텍스트 스위칭 비용이 크지 않다.

#### 2.3.4 ThreadPoolExecutor가 CPU-Bound 작업에 부적합한 이유

CPU-Bound 작업을 ThreadPoolExecutor로 처리하는 경우:

```java
ExecutorService cpuPool = Executors.newFixedThreadPool(4);

// 큰 배열을 정렬하는 작업 제출
cpuPool.submit(() -> {
    Arrays.sort(largeArray1); // CPU 집약적
});
cpuPool.submit(() -> {
    Arrays.sort(largeArray2);
});
// ...
```

**구조적 문제:**

**문제 1: 작업 분할 불가**

각 정렬 작업은 단일 스레드에서 순차 처리된다. 큰 배열을 여러 Worker가 협력하여 처리할 수 없다 (work-stealing 없음). ForkJoinPool이라면 재귀적 분할로 모든 코어를 활용할 수 있었을 작업이다.

**문제 2: 부하 불균형**

```
시간 0ms: Worker 1이 정렬 작업 1 시작 (예상 1000ms)
시간 0ms: Worker 2가 정렬 작업 2 시작 (예상 100ms)
시간 100ms: Worker 2가 작업 2 완료, 작업 3 시작 (예상 50ms)
시간 150ms: Worker 2가 작업 3 완료, 유휴 상태
시간 1000ms: Worker 1이 작업 1 완료

결과: Worker 2가 850ms 동안 유휴 (work-stealing이 있었다면 작업 1의 일부를 가져갈 수 있었음)
```

**문제 3: 캐시 지역성 상실**

공유 큐에서 FIFO로 작업을 가져오므로, 관련된 작업들이 다른 Worker에 분산될 수 있다. 분할 정복 알고리즘에서 부모 작업과 자식 작업이 같은 Worker에서 실행되지 않으면, 캐시 미스가 증가한다.

**문제 4: 오버헤드 증가**

공유 큐 접근 시 항상 동기화가 필요하다 (BlockingQueue의 락). ForkJoinPool의 per-worker deque는 대부분 락 없이 동작하므로 더 빠르다.

### 2.4 구체적 비교: 동일 작업을 다른 풀에서 처리할 때의 차이

#### 2.4.1 시나리오 1: 100만 개 정수 배열 정렬 (CPU-Bound)

**ForkJoinPool 사용:**

```java
ForkJoinPool pool = new ForkJoinPool(4); // 4코어 CPU
pool.invoke(new SortTask(array, 0, 1_000_000));

// 내부 동작:
// Level 0: [0, 1M] → [0, 500K], [500K, 1M]  (2개 작업)
// Level 1: [0, 250K], [250K, 500K], [500K, 750K], [750K, 1M]  (4개 작업)
// Level 2: 8개 작업...
// 최종적으로 4개 Worker가 모두 활용되어 병렬 처리
```

**성능:**
- 4개 Worker가 지속적으로 CPU를 100% 사용
- Work-stealing으로 부하 균형 유지
- 캐시 지역성 덕분에 메모리 접근 효율 상승
- 예상 시간: 단일 스레드 대비 ~3.5배 속도 향상 (이상적으로는 4배, 오버헤드 고려)

**ThreadPoolExecutor 사용:**

```java
ExecutorService pool = Executors.newFixedThreadPool(4);
pool.submit(() -> Arrays.sort(array, 0, 1_000_000));

// 내부 동작:
// Worker 1이 전체 배열을 혼자 정렬
// Worker 2-4는 유휴 상태
```

**성능:**
- 1개 Worker만 CPU 사용, 나머지 3개 코어는 유휴
- 예상 시간: 단일 스레드와 동일 (병렬화 이점 없음)

#### 2.4.2 시나리오 2: 50개 HTTP API 호출 (I/O-Bound)

**ThreadPoolExecutor 사용:**

```java
ExecutorService pool = Executors.newFixedThreadPool(20);
List<CompletableFuture<String>> futures = new ArrayList<>();

for (int i = 0; i < 50; i++) {
    futures.add(CompletableFuture.supplyAsync(() -> 
        httpClient.send(request), pool
    ));
}

// 내부 동작:
// 시간 0-10ms: 20개 Worker가 요청 1-20 전송 후 I/O 블로킹
// 시간 100ms: 일부 응답 도착, 해당 Worker들이 요청 21-50 처리 시작
// 시간 200ms: 모든 요청 완료
```

**성능:**
- 20개 요청이 동시에 진행 (네트워크 병렬화)
- I/O 블로킹 시간 동안 다른 Worker가 새 요청 처리
- 예상 시간: ~200ms (단일 스레드라면 50 × 100ms = 5초)

**ForkJoinPool 사용:**

```java
ForkJoinPool pool = new ForkJoinPool(4); // 기본값
List<CompletableFuture<String>> futures = new ArrayList<>();

for (int i = 0; i < 50; i++) {
    futures.add(CompletableFuture.supplyAsync(() -> 
        httpClient.send(request), pool
    ));
}

// 내부 동작:
// 시간 0-10ms: 4개 Worker가 요청 1-4 전송 후 모두 I/O 블로킹
// 시간 100ms: 요청 1 응답 도착, Worker 1이 요청 5 시작
// 시간 200ms: 요청 2 응답 도착, Worker 2가 요청 6 시작
// ...
// 시간 1200ms: 모든 요청 완료
```

**성능:**
- 최대 4개 요청만 동시 진행 (스레드 부족)
- Work-stealing 무의미 (작업 분할 없음)
- 예상 시간: ~1.2초 (ThreadPoolExecutor 대비 6배 느림)

### 2.5 정량적 분석: 최적 스레드 수 계산

#### 2.5.1 CPU-Bound 작업의 최적 스레드 수

**이론적 모델:**
- CPU 사용률 ≈ 100%인 작업
- 컨텍스트 스위칭이 순수 오버헤드
- 최적 스레드 수 = CPU 코어 수 (또는 코어 수 + 1~2)

**근거:**
- N개 코어에 N개 스레드: 각 스레드가 독립적인 코어에서 실행, 컨텍스트 스위칭 최소
- N개 코어에 2N개 스레드: 스케줄러가 계속 스레드를 교체해야 하므로, 컨텍스트 스위칭 오버헤드 증가

**ForkJoinPool이 이를 자동화:**

```java
ForkJoinPool.commonPool(); // parallelism = Runtime.getRuntime().availableProcessors() - 1
```

#### 2.5.2 I/O-Bound 작업의 최적 스레드 수

**이론적 모델 (Little's Law):**

```
최적 스레드 수 = (요청 처리량 × 평균 응답 시간) / CPU 사용률

또는 더 직관적으로:
최적 스레드 수 = CPU 코어 수 × (1 + I/O 대기 시간 / CPU 사용 시간)
```

**예시:**
- 4코어 CPU
- HTTP 요청 1개 처리: CPU 사용 10ms, I/O 대기 90ms
- 최적 스레드 수 = 4 × (1 + 90/10) = 4 × 10 = 40

**실전적 접근:**
- 예상 동시 요청 수를 기준으로 설정 (예: 50개 API를 병렬 호출 → 스레드 50개)
- 부하 테스트로 최적값 찾기 (스레드 수를 늘려가며 처리량 측정)

**ThreadPoolExecutor가 이를 지원:**

```java
new ThreadPoolExecutor(
    10,  // corePoolSize: 기본 유지 스레드
    100, // maximumPoolSize: 부하 증가 시 확장 가능
    60L, TimeUnit.SECONDS,
    new LinkedBlockingQueue<>(1000)
);
```

### 2.6 ForkJoinPool의 본질: CPU 코어 유휴 최소화를 위한 동적 작업 분배

#### 2.6.1 핵심 문제: ThreadPoolExecutor의 정적 분할에서 발생하는 코어 유휴

**시나리오: 1억 개 정수 배열을 4코어 CPU에서 정렬**

**ThreadPoolExecutor 방식의 구조적 한계:**

```java
ExecutorService pool = Executors.newFixedThreadPool(4);
int[] array = new int[100_000_000];

// 개발자가 미리 작업을 4개로 분할해야 함
List<Future<?>> futures = new ArrayList<>();
int chunkSize = 25_000_000; // 1억 / 4코어

for (int i = 0; i < 4; i++) {
    int start = i * chunkSize;
    int end = start + chunkSize;
    futures.add(pool.submit(() -> {
        Arrays.sort(array, start, end); // 각 Worker가 2500만 개씩 정렬
    }));
}

// 모든 작업 완료 대기
futures.forEach(f -> f.get());
```

**실제 실행 흐름과 문제점:**

```
데이터 분포 상황:
- Chunk 1 [0 ~ 25M]: 이미 대부분 정렬된 데이터 (Best case: O(n))
- Chunk 2 [25M ~ 50M]: 완전 랜덤 데이터 (Average case: O(n log n))
- Chunk 3 [50M ~ 75M]: 완전 역순 데이터 (Worst case: O(n²) for QuickSort)
- Chunk 4 [75M ~ 100M]: 평균 난이도 데이터

시간 흐름:
시간 0ms    : Worker 1-4가 각각 Chunk 1-4 정렬 시작
시간 500ms  : Worker 1이 Chunk 1 완료 (이미 정렬된 데이터) → CPU 1 유휴
시간 2000ms : Worker 2가 Chunk 2 완료 → CPU 2 유휴
시간 2500ms : Worker 4가 Chunk 4 완료 → CPU 4 유휴
시간 8000ms : Worker 3이 Chunk 3 완료 (역순 데이터, 최악의 경우)

결과:
- 총 소요 시간: 8000ms
- CPU 1 유휴 시간: 7500ms (93.75% 유휴)
- CPU 2 유휴 시간: 6000ms (75% 유휴)
- CPU 4 유휴 시간: 5500ms (68.75% 유휴)
- CPU 3만 끝까지 100% 사용
```

**핵심 문제**: "큰 덩어리 단위로 작업을 수행시키면 CPU 코어에 분배하는 작업 자체가 줄어든다"

정확히 이것이다. 4개의 큰 덩어리로 고정 분할하면:
- 한 번 분배하면 끝 (재분배 메커니즘 없음)
- 각 덩어리의 실제 처리 시간이 다름 (데이터 특성에 따라)
- 먼저 끝난 Worker는 할 일이 없어도 다른 Worker를 도울 수 없음

#### 2.6.2 ForkJoinPool의 해결책: 재귀적 분할을 통한 동적 작업 생성

```java
class SortTask extends RecursiveAction {
    final int[] array;
    final int left, right;
    static final int THRESHOLD = 10_000; // 매우 작은 단위까지 분할
    
    @Override
    protected void compute() {
        int size = right - left;
        
        if (size <= THRESHOLD) {
            // 기저 사례: 충분히 작으면 직접 정렬
            Arrays.sort(array, left, right);
        } else {
            // 재귀 사례: 계속 분할
            int mid = left + size / 2;
            
            SortTask leftTask = new SortTask(array, left, mid);
            SortTask rightTask = new SortTask(array, mid, right);
            
            leftTask.fork();  // Worker의 deque에 push
            rightTask.compute(); // 현재 스레드에서 즉시 실행 (또 분할됨)
            leftTask.join();  // leftTask 완료 대기
        }
    }
}

ForkJoinPool pool = new ForkJoinPool(4);
pool.invoke(new SortTask(array, 0, 100_000_000));
```

**ForkJoinPool의 실제 작동 메커니즘:**

```
초기 상태:
Worker 1: [Task(0, 100M)]
Worker 2: []
Worker 3: []
Worker 4: []

=== 시간 0ms: Worker 1이 Task(0, 100M) 실행 시작 ===

Worker 1의 compute() 실행:
- size = 100M > THRESHOLD
- 분할: leftTask(0, 50M), rightTask(50M, 100M)
- leftTask.fork() → Worker 1의 deque head에 push
- rightTask.compute() 즉시 실행 (재귀 호출)

Worker 1의 deque: [Task(0, 50M)]  ← head
Worker 1의 스택: compute(50M, 100M) 실행 중

Worker 1이 rightTask(50M, 100M) 처리:
- size = 50M > THRESHOLD
- 분할: leftTask(50M, 75M), rightTask(75M, 100M)
- leftTask.fork() → deque에 push
- rightTask.compute() 즉시 실행

Worker 1의 deque: [Task(0, 50M), Task(50M, 75M)]  ← head
Worker 1의 스택: compute(75M, 100M) 실행 중

Worker 1이 rightTask(75M, 100M) 처리:
- size = 25M > THRESHOLD
- 계속 분할...

=== 시간 1ms: Worker 2가 유휴 상태 감지, Work-Stealing 시도 ===

Worker 2가 Worker 1의 deque tail에서 steal:
- Task(0, 50M)을 가져감 (가장 큰 작업)

Worker 1의 deque: [Task(50M, 75M)]  ← head
Worker 2가 Task(0, 50M) 실행 시작:
- 이것도 분할됨 → Task(0, 25M), Task(25M, 50M)

=== 시간 2ms: Worker 3, 4도 stealing 시작 ===

현재 상태:
Worker 1: Task(75M, 100M) 영역을 계속 분할하며 처리
         deque: [Task(87.5M, 93.75M), Task(93.75M, 100M), ...]
         
Worker 2: Task(0, 50M) 영역을 분할하며 처리
         deque: [Task(12.5M, 25M), Task(25M, 37.5M), ...]
         
Worker 3: Worker 1의 deque에서 Task(93.75M, 100M) steal
         이것도 분할 시작
         
Worker 4: Worker 2의 deque에서 Task(25M, 37.5M) steal
         이것도 분할 시작

=== 핵심: 작업이 계속 생성되므로 유휴 Worker가 항상 가져갈 작업이 있음 ===
```

**정리**: "작업을 세세하게 분배함으로써 CPU 코어를 극적으로 활용"

```
Worker 1이 처리하는 작업 흐름 (실제 세밀도):
compute(75M, 100M)
→ compute(87.5M, 100M)
→ compute(93.75M, 100M)  ← 이 시점에 Worker 3이 steal
→ compute(93.75M, 96.875M)
→ compute(93.75M, 95.3125M)
→ compute(93.75M, 94.53125M)
→ ... (계속 분할)
→ compute(93.75M, 93.76M) ← THRESHOLD 이하, 직접 정렬

한 Worker가 수천 개의 작은 작업을 생성하고 처리하며,
유휴 Worker들이 계속 큰 작업들을 훔쳐가므로
모든 CPU 코어가 지속적으로 활용됨
```

---

## 3. 결론: 스레드 풀 선택의 구조적 원칙

### 3.1 메커니즘 기반 선택 기준

#### 3.1.1 ForkJoinPool을 선택해야 하는 경우

**조건:**
1. 작업이 재귀적으로 분할 가능 (분할 정복 알고리즘)
2. 각 하위 작업의 실행 시간이 짧고 예측 가능
3. 작업 간 데이터 지역성이 존재
4. 블로킹 I/O가 없음 (순수 CPU-bound)

**내부 메커니즘 활용:**
- Per-worker deque의 LIFO/FIFO 분리로 캐시 지역성 최적화
- Work-stealing으로 동적 부하 분산
- 락 없는 동시성 제어로 오버헤드 최소화

#### 3.1.2 ThreadPoolExecutor를 선택해야 하는 경우

**조건:**
1. 작업이 독립적이고 분할되지 않음
2. 실행 시간이 길거나 예측 불가능
3. 블로킹 I/O가 포함됨
4. 스레드 수를 작업 특성에 맞게 조정해야 함

**내부 메커니즘 활용:**
- 단일 공유 큐로 단순한 작업 분배
- 유연한 스레드 수 설정 (corePoolSize, maximumPoolSize)
- BlockingQueue의 자동 스레드 깨우기

### 3.2 실전 프로젝트에서의 적용

#### 3.2.1 Kakao API 병렬 호출 구현

```java
// I/O 작업용: ThreadPoolExecutor 기반
ExecutorService ioExecutor = Executors.newFixedThreadPool(20);

List<CompletableFuture<KakaoResponse>> futures = apiEndpoints.stream()
    .map(endpoint -> CompletableFuture.supplyAsync(
        () -> kakaoApiClient.call(endpoint), // 블로킹 I/O
        ioExecutor // ThreadPoolExecutor 사용
    ))
    .collect(Collectors.toList());
```

**이 설계의 정당화:**

1. 각 API 호출은 독립적 (작업 분할 불필요)
2. 네트워크 I/O로 인한 긴 블로킹 시간 (수십~수백 ms)
3. 20개 스레드로 충분한 병렬성 확보 (4코어 CPU에서도 효과적)
4. ForkJoinPool(4 스레드)을 사용했다면 최대 4개 요청만 동시 처리, 처리 시간 5배 증가

#### 3.2.2 CPU-bound 처리 추가 시

만약 이후 CPU-bound 처리가 추가된다면:

```java
// API 응답 데이터를 복잡한 알고리즘으로 분석하는 경우
CompletableFuture<AnalysisResult> analysisResult = futures.stream()
    .map(future -> future.thenApplyAsync(
        response -> complexAnalysis(response), // CPU-bound
        ForkJoinPool.commonPool() // ForkJoinPool 사용
    ))
    ...
```

이렇게 작업 유형에 따라 스레드 풀을 분리하면, 각 메커니즘의 강점을 최대한 활용할 수 있다.

### 3.3 설계적 책임: 기술 백서에 포함해야 할 내용

#### 3.3.1 스레드 풀 선택의 근거

**기본 설명:**
"Kakao API 호출은 블로킹 I/O이므로 ThreadPoolExecutor를 사용하며, 스레드 수는 예상 동시 호출 수(20)로 설정했다"

**상세 근거:**
"ForkJoinPool을 사용할 경우 4개 스레드로 제한되어 처리 시간이 5배 이상 증가할 것으로 예상된다"

#### 3.3.2 메커니즘 수준의 설명

**내부 구조 분석:**
"ThreadPoolExecutor의 공유 큐 방식은 독립적인 I/O 작업 처리에 적합하며, 블로킹된 스레드가 있어도 다른 스레드들이 새 작업을 계속 처리할 수 있다"

#### 3.3.3 성능 측정 결과와의 연결

**실측 데이터:**
"실측 결과 ThreadPoolExecutor(20 스레드) 사용 시 평균 400ms, ForkJoinPool(4 스레드) 사용 시 예상 2000ms"

---

## 4. 요약 및 핵심 포인트

### 4.1 ForkJoinPool의 설계 목표

**목표: 모든 CPU 코어가 항상 유용한 작업을 하도록**

- 큰 덩어리 분할의 문제: 일부 코어가 먼저 끝나고 유휴 상태
- ForkJoinPool의 해결: 재귀적 분할로 작은 작업을 계속 생성 → 유휴 코어가 가져갈 작업이 항상 있음

**메커니즘:**

1. **Per-Worker Deque**: 각 코어가 독립적인 작업 큐 소유, 락 경쟁 최소화
2. **LIFO/FIFO 분리**: 캐시 최적화 + 부하 분산
3. **Work-Stealing**: 동적 부하 균형

### 4.2 ThreadPoolExecutor의 설계 목표

**목표: 독립적인 I/O 작업의 병렬 처리**

- I/O 블로킹 허용: 많은 스레드가 블로킹되어도 시스템 동작 가능
- 유연한 스레드 수: CPU 코어 수와 무관하게 I/O 병렬성 확보
- 단순한 큐 구조: 복잡한 work-stealing 불필요

### 4.3 CPU-Bound와 I/O-Bound의 구조적 차이

**CPU-Bound:**
- 작업이 빠르게 완료 (밀리초) → 자주 새 작업 가져오기 필요 → 락 경쟁이 병목
- 작업이 재귀적 분할 가능 → 작은 작업들이 계속 생성 → work-stealing 활발
- 블로킹 없음 → Worker가 항상 CPU 사용 → 캐시 지역성 유지

**I/O-Bound:**
- 블로킹 시간이 길어 (수백ms) 작업 생성 빈도가 낮음 → work-stealing 기회 적음
- 스레드가 커널 대기 상태로 오래 머물러 CPU 유휴 → 코어 수만큼의 스레드로는 부족
- 분할해도 하위 작업들이 모두 블로킹 → per-worker deque의 이점 활용 못함

### 4.4 최종 결론

ForkJoinPool과 ThreadPoolExecutor의 선택은 단순한 베스트 프랙티스가 아니라, 각 스레드 풀이 최적화한 작업 패턴과 실제 작업의 특성 간의 구조적 적합성에서 비롯된다. ForkJoinPool은 CPU 코어 유휴를 최소화하기 위한 동적 작업 분배 메커니즘을 가지며, ThreadPoolExecutor는 I/O 블로킹을 허용하면서 충분한 병렬성을 확보하는 단순한 구조를 가진다. 이 차이를 이해하는 것이 올바른 스레드 풀 선택의 핵심이다.

---

## 5. 참고 문헌 및 추가 학습 자료

### 5.1 ForkJoinPool 설계 및 구현
- A Java Fork/Join Framework (Doug Lea, 2000)
- Java Concurrency in Practice (Brian Goetz, 2006)

### 5.2 Work-Stealing 알고리즘
- Scheduling Multithreaded Computations by Work Stealing (Blumofe & Leiserson, 1999)

### 5.3 CPU 캐시 및 메모리 계층
- Computer Architecture: A Quantitative Approach (Hennessy & Patterson)
- What Every Programmer Should Know About Memory (Ulrich Drepper, 2007)

### 5.4 스레드 풀 설계
- Java 8 in Action (Raoul-Gabriel Urma et al.)
- The Java Virtual Machine Specification

---

## 문서 버전 정보
- 버전: 1.0
- 작성일: 2025-11-17
- 분석 대상: ForkJoinPool vs ThreadPoolExecutor 구조 비교
- 키워드: ForkJoinPool, ThreadPoolExecutor, Work-Stealing, CPU-Bound, I/O-Bound, Per-Worker Deque, Cache Locality