## 0. 컴퓨터 아키텍처의 근본 제약과 프로그래밍 모델의 진화

### 0.1 하드웨어의 역사적 제약: "CPU는 한 번에 한 가지 일만 한다"

초기 컴퓨터의 CPU(중앙 처리 장치)는 코어가 하나였고, 이는 물리적으로 한 순간에 단 하나의 명령어만 처리할 수 있음을 의미한다. 이는 폰 노이만 아키텍처의 핵심 제약이며, 제어권이 "레지스터의 순차적인 나열"이라는 동작 원리에 기반한다.

아리한 하드웨어 위에서 프로그래밍이 동작하는 방식을 상상해보자:

- **프로그램은 명령어들의 순차적인 나열이다**: CPU는 **Instruction Pointer**(명령어 포인터)라는 것을 가지고 있으며, 이는 다음에 실행할 명령어의 메모리 주소를 가리킨다.
- **명령어를 하나 실행하면, 명령어 포인터는 자동으로 다음 주소로 이동한다**: 이것이 순차 실행의 본질이다.
- **명령어를 하나 실행하면, 명령어 포인터는 자동으로 다음 주소로 넘어간다**: 이렇게 코드는 위에서 아래로 흐른다.

여기서 한계(서브루틴 호출, 즉 "함수 호출")가 생긴다. 함수를 호출하면 현재 실행 중인 코드를 멈추고, 다른 코드로 점프해야 한다. 그리고 그 함수가 끝나면 다시 원래 위치로 돌아와야 한다.

### 0.2 콜 스택(Call Stack): 동기-블록킹이 "기본값"인 이유

프로그래밍은 복잡한 문제를 논리적 절차로 나누어 해결하는 과정이며, 이때 가장 중요한 것은 **"코드는 위에서 아래로 흐른다"**는 과정이다. 이 때 가장 중요한 것은 예측 가능성이다. 코드는 위에서 아래로 흐른다는 것, 그냥 보면 이해할 수 있다.

#### 0.2.1 콜 스택의 동작 방식

```java
int a = 1;
int b = 2;
int sum = add(a, b);  // "재어감"이 add 함수로 넘어간다
System.out.println(sum);  // add 함수 완료 후 여기로 돌아온다
```

만약 `add(a, b)`를 호출했을 때 제어권이 넘어가지 않고, `add`의 함수와 `System.out.println(sum)`이 동시에 실행된다고 가정해보자. 그럼 `sum`이라는 변수는 언제 어떤 값이 들어있을까? 알 수 없다. 이처럼 "제어권"을 넘겨주고 작업이 완료될 때까지 기다리는 동기-블로킹은 동작이 간결하고 가장에게 이해하기 쉬운 모델이다.

이 LIFO(Last-In, First-Out) 구조인 "콜 스택(Call Stack)"이 하드웨어 수준에서 지원된다:

1. 함수를 호출하면, 해당 함수로의 스택 프레임이 생성된다
2. 함수가 끝나면 그 공간은 자동으로 해제되고 이전 위치로 돌아간다
3. 코드를 이해하기가 쉽고, 자원 관리가 간단하며, 디버깅하기가 편리하다

결론: 제어흐름 모델은 **"콜 스택(Call Stack)"**이라는 하드웨어가 자연스럽게 지원하는 메커니즘을 사용하며, 그 대가로 동기-블록킹이 "기본값"이 된다.

### 0.3 제어 흐름의 진화: "기본" vs "고급"

#### 0.3.1 기본(Default) 제어 흐름: 동기-블록킹

하나의 함수가 완료될 때까지 기다리고 다음으로 진행하는 방식이다. 하드웨어(CPU, 콜 스택)가 자연스럽게 지원하는 모델이며, 코드는 위에서 아래로 순차적으로 흐른다. 각 함수 호출마다 완료를 기다린다. 이것이 모든 프로그래밍 언어의 기본 동작이다.

#### 0.3.2 고급(Advanced) 제어 흐름: 비동기-논블록킹

기본 모델의 한계(I/O 작업에서의 대기 시간)를 극복하기 위해 등장했다. 작업을 다른 실행 주체(Thread, Process)에 위임하고 즉시 반환한다. Main Thread는 다른 작업을 계속할 수 있다. 

하지만 이는 복잡성을 수반한다:
- OS 수준의 Thread/Process 관리가 필요하다
- Thread 간 통신이 복잡하다
- Race Condition 등 동시성 버그가 발생할 수 있다
- 디버깅이 어렵고 예측하기 힘들다

따라서 "제어권"이라는 개념이 흐트러지고, 두뇌가 처리해야 할 복잡도가 기하급수적으로 증가한다. 오히려 플러그램의 복잡성은 늘어나지만, 성능과 처리량이 개선되는 트레이드오프가 있다.

### 0.4 프로그래밍 모델의 진화 계층

복잡성을 감수하고 성능을 개선하려면, 새로운 추상화가 필요하다:

1. **하드웨어 수준**: 멍령어 포인터를 직접 조작, 순차 실행만 가능
2. **기본 추상화**: 함수 호출과 콜 스택, 동기-블록킹이 기본
3. **멀티스레드**: Thread를 생성하여 병렬 실행, 하지만 동기화 복잡
4. **비동기 추상화**: CompletableFuture, Promise 등으로 비동기 작업 관리
5. **반응형 스트림**: Event Loop와 Non-blocking I/O로 더 높은 효율성

### 0.5 현재 프로젝트의 맥락에서의 의미

카카오 API 15개를 순차 호출하는 현재 구조는 "기본" 제어 흐름(동기-블록킹)을 사용한다. 각 API 호출마다 Thread가 응답을 기다리며 WAITING 상태로 전환되어, 총 2,681ms의 시간이 소요된다.

CompletableFuture를 통한 "고급" 제어 흐름으로 전환하면:
- 15개 Thread가 동시에 각자의 API를 호출한다
- 대부분의 시간을 WAITING 상태로 보내므로 Context Switching 오버헤드는 미미하다
- 병렬 처리로 전체 시간이 대폭 단축된다

이는 단순히 "빠르게 만드는 기법"이 아니라, 하드웨어의 근본적 제약(CPU는 한 번에 한 가지)을 프로그래밍 모델의 진화(여러 Thread가 각자 일함)로 극복하는 아키텍처적 전환이다.

---
# R-04 비동기 병목 개선 기술 백서

## 1. 서론: 문제 정의 및 개선 필요성

### 1.1 현재 시스템의 한계

현재 구현된 코드는 Thread-per-Request 모델의 근본적 한계에 직면해 있다. Spring MVC의 Servlet Thread는 외부 API 응답을 기다리는 동안 Blocking 상태로 유휴 시간을 소비하며, 이는 시스템의 처리량(Throughput)을 근본적으로 제한한다. 15개의 카카오 API를 순차 호출하는 현재 구조는 네트워크 I/O 대기 시간이 직렬로 누적되는 전형적인 안티패턴이며, 동시 사용자 수가 증가할 때 가용 Thread Pool이 고갈되어 시스템 전체가 응답 불가 상태에 빠질 수 있다.

### 1.2 비동기 프로그래밍의 아키텍처적 의의

비동기 프로그래밍은 단순히 "빠르게 만드는 기법"이 아니라, Thread라는 유한한 시스템 자원을 I/O 대기 시간으로부터 해방시켜, 동일한 하드웨어에서 더 많은 동시 요청을 처리할 수 있게 하는 자원 활용 전략의 근본적 전환이다. 이러한 개선은 "시스템의 처리 용량 한계를 아키텍처 수준에서 극복"하는 접근 방식을 증명하는 것이다.

---

## 2. 핵심 학습 주제 및 기술 개념

### 2.1 Java 비동기 프로그래밍의 근본 메커니즘

#### 2.1.1 CompletableFuture의 설계 목적

CompletableFuture는 Java 8에서 도입된 비동기 연산의 표준 추상화다. 이것은 callback hell을 피하면서도 비동기 작업의 조합(composition), 예외 처리, 완료 후 후속 작업 체이닝을 선언적으로 표현할 수 있게 해주는 Monad 패턴의 구현체다. 단순히 "빠르게 만드는 도구"가 아니라, 비동기 작업의 생명주기를 제어 가능한 객체로 추상화하는 설계 패턴이다.

#### 2.1.2 핵심 개념

- **supplyAsync() vs runAsync()**: 반환값 유무에 따른 비동기 작업 생성
- **thenApply() vs thenCompose()**: 변환(map) vs 평탄화(flatMap)의 차이
- **allOf() vs anyOf()**: 여러 비동기 작업의 완료 조건 제어
- **exceptionally() vs handle()**: 예외 처리 전략의 차이
- **join() vs get()**: Blocking 방식의 결과 수집과 예외 래핑 방식

#### 2.1.3 구글링 키워드

```
java completablefuture tutorial
completablefuture chaining explained
completablefuture exception handling best practices
completablefuture supplyAsync vs thenApply
java async programming patterns
completablefuture allOf collect results
```

#### 2.1.4 검증 방법

- 15개 카테고리를 `List<CompletableFuture<...>>`로 생성
- `CompletableFuture.allOf()`로 모든 작업 완료 대기
- 각 Future의 `join()`으로 결과 수집
- try-catch 대신 `exceptionally()`로 실패한 카테고리 처리

---

### 2.2 Thread Pool과 Executor Framework

#### 2.2.1 기본 Thread Pool의 부적합성

`supplyAsync()`는 기본적으로 `ForkJoinPool.commonPool()`을 사용하는데, 이는 CPU 코어 수만큼의 Thread만 생성하는 공용 풀이다. 하지만 현재 작업은 CPU-bound가 아니라 I/O-bound(네트워크 대기)이므로, 이 기본 풀은 부적합하다. 카카오 API 15개를 동시에 호출하려면 최소 15개의 Thread가 필요하지만, commonPool은 CPU 코어 수(예: 6개)만큼만 Thread를 유지하므로, 결국 일부 작업은 대기 큐에서 순차 실행된다.

커스텀 Executor를 설정하지 않으면, 비동기화를 해도 원하는 병렬도(parallelism)를 달성할 수 없다. 이는 I/O-bound 작업에서 Thread Pool 크기를 결정하는 Little's Law와 Amdahl's Law의 실무적 적용이다.

#### 2.2.2 핵심 개념

- **ThreadPoolExecutor vs ForkJoinPool**: I/O-bound vs CPU-bound 작업에 적합한 풀
- **corePoolSize vs maximumPoolSize**: 상시 유지 Thread vs 최대 확장 가능 Thread
- **queueCapacity**: Bounded queue를 통한 Backpressure 제어
- **RejectedExecutionHandler**: 큐가 가득 찰 때의 정책(CallerRunsPolicy, AbortPolicy)

#### 2.2.3 구글링 키워드

```
java custom executor completablefuture
threadpoolexecutor vs forkjoinpool
executor service for io bound tasks
spring boot async executor configuration
completablefuture custom thread pool
calculating optimal thread pool size
```

#### 2.2.4 검증 방법

```java
ExecutorService executor = Executors.newFixedThreadPool(15);
CompletableFuture.supplyAsync(() -> searchPlacesByCategory(...), executor);
```

---

### 2.3 Thread Safety와 Concurrent Collections

#### 2.3.1 동시성 환경에서의 데이터 무결성 문제

현재 코드에서 `Map<String, List<Map<String, Object>>> results = new HashMap<>();`는 여러 Thread가 동시에 `results.put()`을 호출하면 데이터 유실, NPE, 또는 무한 루프가 발생할 수 있는 Race Condition을 내포한다. HashMap은 Thread-safe하지 않으며, 동기화 없이 멀티 Thread 환경에서 사용하면 내부 해시 충돌 처리 과정에서 linked list가 순환 구조를 형성하여 CPU가 100%까지 치솟는 심각한 버그를 유발할 수 있다.

이는 단순히 "동시성 버그"가 아니라, Java Memory Model(JMM)에서 정의한 happens-before 관계가 보장되지 않아 발생하는 가시성(visibility) 문제다. 한 Thread가 쓴 값을 다른 Thread가 읽지 못하거나, 중간 상태를 읽어 데이터 불일치가 발생한다.

#### 2.3.2 핵심 개념

- **ConcurrentHashMap의 내부 구조**: Segment-based locking → CAS(Compare-And-Swap) 기반 lock-free 알고리즘
- **volatile 키워드의 의미**: CPU 캐시 무효화와 메모리 가시성 보장
- **synchronized vs Lock vs Atomic 클래스**: 동기화 방식의 성능 차이
- **Collections.synchronizedMap() vs ConcurrentHashMap**: 전체 잠금 vs 세그먼트 잠금

#### 2.3.3 구글링 키워드

```
java concurrenthashmap vs hashmap
thread safety in java explained
java memory model happens-before
race condition example java
concurrent collections java tutorial
cas lock free algorithm
```

#### 2.3.4 검증 방법

```java
Map<String, List<Map<String, Object>>> results = new ConcurrentHashMap<>();
```

---

### 2.4 Spring @Async와 비동기 설정

#### 2.4.1 Spring AOP 기반 비동기 메커니즘

Spring의 `@Async`는 메서드 호출을 별도 Thread에서 실행하도록 프록시를 생성하는 AOP 기반 추상화다. 하지만 이는 같은 클래스 내부 메서드 호출(`this.method()`)에서는 작동하지 않으며, 반드시 외부에서 Spring Bean을 통해 호출해야 프록시가 개입한다. 또한 `@EnableAsync`를 설정하지 않으면 단순히 동기 호출로 동작하며, 커스텀 Executor를 지정하지 않으면 `SimpleAsyncTaskExecutor`가 사용되어 매 호출마다 새 Thread를 생성하는 비효율이 발생한다.

@Async는 단순한 어노테이션이 아니라, Spring AOP의 프록시 패턴과 TaskExecutor 추상화가 결합된 프레임워크 수준의 비동기 지원 메커니즘이다. 이를 올바르게 사용하려면 Spring의 Bean 생명주기와 프록시 생성 과정을 이해해야 한다.

#### 2.4.2 핵심 개념

- **@EnableAsync의 역할**: `AsyncConfigurer` 인터페이스 활성화
- **Proxy 기반 AOP의 한계**: self-invocation 문제
- **TaskExecutor vs Executor**: Spring 추상화 vs Java 표준 인터페이스
- **@Async의 예외 전파**: `AsyncUncaughtExceptionHandler` 설정

#### 2.4.3 구글링 키워드

```
spring boot async configuration
spring @async not working self invocation
spring async exception handling
spring taskexecutor vs executor
@enableasync custom executor
spring async best practices
```

#### 2.4.4 검증 방법

```java
@Configuration
@EnableAsync
public class AsyncConfig implements AsyncConfigurer {
    @Override
    public Executor getAsyncExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(15);
        executor.setMaxPoolSize(20);
        executor.setQueueCapacity(50);
        executor.initialize();
        return executor;
    }
}
```

---

### 2.5 성능 측정과 부하 테스트

#### 2.5.1 정량적 검증의 필요성

비동기화를 적용했다고 해서 자동으로 성능이 개선되는 것은 아니다. 잘못된 Thread Pool 크기, Executor 설정 오류, 카카오 API의 Rate Limit, 또는 Redis 연결 풀 고갈 등 새로운 병목이 발생할 수 있다. 개선 효과를 정량적으로 증명하지 못하면, 포트폴리오에서 "시도는 했지만 효과는 모르는" 무의미한 변경으로 간주된다.

성능 측정은 단순히 "빨라졌다"를 확인하는 것이 아니라, 시스템의 처리량(TPS), 응답 시간(Latency), 자원 사용률(CPU, Thread, Memory)의 변화를 통해 개선의 아키텍처적 효과를 입증하는 과정이다.

#### 2.5.2 핵심 개념

- **Throughput vs Latency**: 처리량과 지연 시간의 트레이드오프
- **Percentile 분석**: P50, P95, P99의 의미와 중요성
- **Little's Law**: `L = λW` (동시 요청 수 = 도착률 × 평균 처리 시간)
- **JMeter의 Thread Group 설정**: Ramp-up, Duration, Loop Count
- **VisualVM / JConsole**: Thread 상태 모니터링 (RUNNABLE, WAITING, BLOCKED)

#### 2.5.3 구글링 키워드

```
jmeter load testing tutorial
how to measure api performance
calculating throughput and latency
percentile vs average response time
thread pool monitoring java
visualvm thread analysis
little's law explained
```

#### 2.5.4 검증 방법

- JMeter로 동시 사용자 50명, 5분간 부하 테스트
- 개선 전/후 평균 응답 시간 비교: 2,695ms → ?ms
- Thread Pool 상태 모니터링: Active Thread, Queue Size

---

## 3. 비동기 vs 멀티스레드 개념 구분

### 3.1 오해의 핵심: "비동기 = 단일 Thread"가 아니다

비동기 프로그래밍과 멀티스레드는 자주 혼동되는 개념이다. 특히 "비동기로 하나의 스레드에 여러 작업을 할당"한다는 표현은 Node.js의 Event Loop 모델과 Java의 CompletableFuture 모델을 혼동한 것이다.

### 3.2 Node.js의 Single-threaded Event Loop

```javascript
// JavaScript - 실제로 단일 메인 Thread
async function fetchAll() {
    const [addr, amenity, rate] = await Promise.all([
        fetch('/address'),    // I/O 작업을 libuv에 위임
        fetch('/amenity'),    // I/O 작업을 libuv에 위임
        fetch('/rate')        // I/O 작업을 libuv에 위임
    ]);
}
// 메인 Thread는 I/O 완료 이벤트를 Event Loop에서 처리
// 실제 네트워크 I/O는 OS의 커널 Thread가 수행
```

### 3.3 Java의 Multi-threaded Executor

```java
// Java - 실제로 여러 Thread가 생성됨
ExecutorService executor = Executors.newFixedThreadPool(15);

CompletableFuture<List<...>> future1 = CompletableFuture.supplyAsync(() -> {
    return searchPlacesByCategory("CS2", ...);  // Thread 1에서 실행
}, executor);

CompletableFuture<List<...>> future2 = CompletableFuture.supplyAsync(() -> {
    return searchPlacesByCategory("FD6", ...);  // Thread 2에서 실행
}, executor);

// 이것은 15개의 Worker Thread를 생성하고, 각 Thread가 각자의 작업을 실행하는 것이다
```

### 3.4 Java에서 CompletableFuture는 멀티 Thread다

#### 3.4.1 실제 동작 방식

CompletableFuture.supplyAsync()를 15번 호출하면:

```java
ExecutorService executor = Executors.newFixedThreadPool(15);

for (String category : categories) {
    CompletableFuture.supplyAsync(() -> {
        return searchPlacesByCategory(category, ...);
    }, executor);
}
```

실제로 일어나는 일:

1. Executor가 15개의 Worker Thread를 생성 (pool-1-thread-1, pool-1-thread-2, ..., pool-1-thread-15)
2. 각 supplyAsync() 호출은 별도의 Thread에 작업을 제출
3. Thread 1이 "CS2" 카테고리 API를 호출하고 응답을 기다리며 Blocking
4. Thread 2가 "FD6" 카테고리 API를 호출하고 응답을 기다리며 Blocking
5. ...
6. 15개의 Thread가 동시에 각자의 네트워크 I/O를 기다림

이것은 "하나의 Thread에 여러 작업"이 아니라, "여러 Thread가 각자의 작업을 동시에 실행"하는 것이다.

### 3.5 "비동기"의 진짜 의미

"비동기"는 호출자(Caller)의 관점에서 정의된다.

#### 3.5.1 동기 (Synchronous)

```java
// Main Thread가 직접 실행
List<...> result1 = searchPlacesByCategory("CS2");  // Main Thread Blocked
List<...> result2 = searchPlacesByCategory("FD6");  // Main Thread Blocked
List<...> result3 = searchPlacesByCategory("CE7");  // Main Thread Blocked

// Main Thread는 각 작업이 끝날 때까지 대기
// 총 시간 = T1 + T2 + T3
```

#### 3.5.2 비동기 (Asynchronous)

```java
// Worker Thread에 작업을 위임
CompletableFuture<List<...>> future1 = CompletableFuture.supplyAsync(() -> searchPlacesByCategory("CS2"));
CompletableFuture<List<...>> future2 = CompletableFuture.supplyAsync(() -> searchPlacesByCategory("FD6"));
CompletableFuture<List<...>> future3 = CompletableFuture.supplyAsync(() -> searchPlacesByCategory("CE7"));

// Main Thread는 즉시 반환되고 다른 작업 수행 가능
// Worker Thread들이 백그라운드에서 작업 수행
// 총 시간 = max(T1, T2, T3) (병렬 실행 시)
```

"비동기"의 핵심은:

- **Main Thread가 Blocking되지 않는다** (Non-blocking)
- **작업을 다른 Thread에 위임한다** (Delegation)
- **결과는 나중에 받는다** (Future)

### 3.6 Blocking vs Non-blocking 구분

#### 3.6.1 Blocking (차단)

Thread가 작업 완료를 기다리며 멈춰 있는 상태.

```java
// 동기 + Blocking
String response = httpClient.get("https://api.kakao.com/...");
// 이 Thread는 응답이 올 때까지 아무것도 못함 (WAITING 상태)
```

#### 3.6.2 Non-blocking (비차단)

Thread가 작업 완료를 기다리지 않고 즉시 반환.

```java
// 비동기 + Non-blocking
CompletableFuture<String> future = CompletableFuture.supplyAsync(() ->
    httpClient.get("https://api.kakao.com/...")
);
// Main Thread는 즉시 다음 라인 실행 가능
// Worker Thread가 네트워크 I/O를 기다림
```

### 3.7 Synchronous vs Asynchronous 구분

#### 3.7.1 Synchronous (동기)

호출자가 작업을 직접 실행하고 완료를 기다림.

```java
List<...> result = searchPlacesByCategory("CS2");
// 이 라인이 끝나야 다음 라인 실행 가능
```

#### 3.7.2 Asynchronous (비동기)

호출자가 작업을 다른 실행 주체에 위임하고 즉시 반환.

```java
CompletableFuture<List<...>> future = CompletableFuture.supplyAsync(() ->
    searchPlacesByCategory("CS2")
);
// 즉시 다음 라인 실행 가능
// 나중에 future.join()으로 결과 수집
```

### 3.8 Java의 전통적 Thread.start()와의 차이

#### 3.8.1 Thread.start() 방식 (Low-level)

```java
Thread thread1 = new Thread(() -> {
    List<...> result = searchPlacesByCategory("CS2");
    // 결과를 어떻게 Main Thread로 전달할 것인가? (문제 발생)
});

Thread thread2 = new Thread(() -> {
    List<...> result = searchPlacesByCategory("FD6");
});

thread1.start();
thread2.start();

thread1.join();  // Main Thread가 Blocking
thread2.join();  // Main Thread가 Blocking

// 문제:
// 1. 결과를 어떻게 받을 것인가? (공유 변수? 위험함)
// 2. 예외는 어떻게 처리할 것인가?
// 3. 15개 Thread를 일일이 관리해야 함
```

#### 3.8.2 CompletableFuture 방식 (High-level)

```java
ExecutorService executor = Executors.newFixedThreadPool(15);

List<CompletableFuture<List<...>>> futures = new ArrayList<>();

for (String category : categories) {
    CompletableFuture<List<...>> future = CompletableFuture
        .supplyAsync(() -> searchPlacesByCategory(category), executor)
        .exceptionally(ex -> {
            log.error("Failed", ex);
            return Collections.emptyList();
        });
    
    futures.add(future);
}

// 모든 작업 완료 대기
CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();

// 결과 수집
List<List<...>> results = futures.stream()
    .map(CompletableFuture::join)
    .collect(Collectors.toList());

// 장점:
// 1. 결과 수집이 안전하고 간단함
// 2. 예외 처리가 선언적임
// 3. Thread Pool이 자동 관리됨
```

### 3.9 핵심 요약

| 구분 | Node.js | Java CompletableFuture |
|------|---------|------------------------|
| 메인 Thread | 단일 Thread | Servlet Thread (요청당 1개) |
| 작업 실행 | OS 커널 Thread | Worker Thread Pool |
| 동시성 모델 | Event Loop | Thread Pool |
| I/O 대기 | Non-blocking | Blocking (하지만 Worker Thread에서) |
| 적합한 경우 | I/O-heavy, 단순 API | CPU/I/O 혼재, 복잡한 로직 |

현재 구현에서:

- **Thread.start() 방식을 사용하면**: 결과 수집과 예외 처리가 복잡해진다.
- **CompletableFuture를 사용하면**: 코드가 선언적이고 안전해진다.
- **I/O-bound 작업이므로**: Thread Pool 크기는 동시 I/O 수(15개)에 맞춰야 한다.

---

## 4. 비동기의 정확한 의미와 동작 메커니즘

### 4.1 "비동기"의 핵심 개념

"비동기"는 호출자(Caller)의 관점에서 정의된다.

#### 4.1.1 동기 (Synchronous) 방식

```java
// Main Thread가 직접 실행
List<...> result1 = searchPlacesByCategory("CS2");  // Main Thread Blocked
List<...> result2 = searchPlacesByCategory("FD6");  // Main Thread Blocked
List<...> result3 = searchPlacesByCategory("CE7");  // Main Thread Blocked

// Main Thread는 각 작업이 끝날 때까지 대기
// 총 시간 = T1 + T2 + T3
```

#### 4.1.2 비동기 (Asynchronous) 방식

```java
// Worker Thread에 작업을 위임
CompletableFuture<List<...>> future1 = CompletableFuture.supplyAsync(() -> searchPlacesByCategory("CS2"));
CompletableFuture<List<...>> future2 = CompletableFuture.supplyAsync(() -> searchPlacesByCategory("FD6"));
CompletableFuture<List<...>> future3 = CompletableFuture.supplyAsync(() -> searchPlacesByCategory("CE7"));

// Main Thread는 즉시 반환되고 다른 작업 수행 가능
// Worker Thread들이 백그라운드에서 작업 수행
// 총 시간 = max(T1, T2, T3) (병렬 실행 시)

// 나중에 결과 필요할 때 수집
CompletableFuture.allOf(future1, future2, future3).join();  // 이때만 Main Thread가 대기
```

"비동기"의 핵심:

- **Main Thread(또는 요청 처리 Thread)가 작업 완료를 기다리지 않고 즉시 반환된다**
- **실제 작업은 별도의 Worker Thread에서 수행된다**
- **Main Thread는 "나중에" 결과를 수집할 수 있다**

### 4.2 Thread 직접 생성 vs CompletableFuture

#### 4.2.1 방법 1: Thread 직접 생성

```java
// Thread 직접 생성 방식
List<Thread> threads = new ArrayList<>();
Map<String, List<...>> results = new ConcurrentHashMap<>();

for (String category : categories) {
    Thread thread = new Thread(() -> {
        List<...> places = searchPlacesByCategory(category, ...);
        results.put(category, places);
    });
    thread.start();
    threads.add(thread);
}

// 모든 Thread 완료 대기
for (Thread thread : threads) {
    thread.join();  // Main Thread가 각 Thread의 종료를 기다림
}
```

**문제점:**

- **Thread 생성 비용**: 매 요청마다 15개의 새 Thread를 생성/소멸하면 Context Switching 오버헤드가 크다
- **자원 제어 불가**: 동시 요청 100개가 오면 1,500개의 Thread가 생성되어 시스템 붕괴
- **예외 처리 복잡**: 각 Thread의 예외를 Main Thread에서 잡을 수 없음
- **결과 수집 번거로움**: thread.join() + ConcurrentHashMap 수동 관리

#### 4.2.2 방법 2: Thread Pool + CompletableFuture (권장)

```java
// Thread Pool은 한 번만 생성
ExecutorService executor = Executors.newFixedThreadPool(15);

Map<String, List<...>> results = new ConcurrentHashMap<>();
List<CompletableFuture<Void>> futures = new ArrayList<>();

for (String category : categories) {
    CompletableFuture<Void> future = CompletableFuture.supplyAsync(() -> {
        return searchPlacesByCategory(category, ...);
    }, executor)
    .thenAccept(places -> results.put(category, places))
    .exceptionally(ex -> {
        log.error("Category {} failed", category, ex);
        results.put(category, Collections.emptyList());
        return null;
    });
    
    futures.add(future);
}

// 모든 작업 완료 대기
CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();
```

**장점:**

- **Thread 재사용**: 15개 Thread Pool이 모든 요청에 재사용됨 (생성/소멸 비용 없음)
- **자원 제어**: corePoolSize=15, maxPoolSize=20으로 최대 Thread 수 제한
- **선언적 예외 처리**: exceptionally(), handle()로 각 작업의 예외를 우아하게 처리
- **조합 가능**: thenApply(), thenCompose(), allOf() 등으로 복잡한 비동기 흐름 구성

### 4.3 진짜 Single-threaded 비동기: Reactive Streams

#### 4.3.1 Spring WebFlux + Reactor 예시

```java
// WebClient는 Netty의 Event Loop를 사용 (Non-blocking I/O)
Flux.fromIterable(categories)
    .flatMap(category -> 
        webClient.get()
            .uri("https://dapi.kakao.com/...?category={}", category)
            .retrieve()
            .bodyToMono(String.class)
            .map(response -> Map.entry(category, parseResponse(response)))
    )
    .collectMap(Map.Entry::getKey, Map.Entry::getValue)
    .block();
```

**동작 방식:**

- Netty의 Event Loop Thread(보통 CPU 코어 수만큼, 예: 8개)가 모든 I/O를 처리
- HTTP 요청을 보내고 **즉시 반환** (Thread Blocking 없음)
- OS의 epoll/kqueue가 네트워크 I/O 완료를 감지하면 Event Loop에 알림
- Event Loop Thread가 응답을 처리하고 다음 작업 실행

**이것이 진짜 "적은 Thread로 많은 작업"을 처리하는 방식이다.**

### 4.4 현재 상황에서의 결론: CompletableFuture + ThreadPoolExecutor가 정답

**이유:**

1. **Spring MVC 환경**: 이미 Spring MVC를 사용 중이며, WebFlux로 전환하는 것은 너무 큰 변경이다.
2. **WebClient의 Blocking 특성**: `WebClient.block()`을 사용하면 어차피 Thread가 Blocking된다.
3. **학습 곡선**: Reactive Programming은 명령형 → 선언형 패러다임 전환으로 학습 비용이 매우 크다.
4. **I/O Bound 특성**: 15개 네트워크 요청은 Thread Pool 15개로 충분히 병렬 처리 가능하다.

### 4.5 올바른 이해

**질문**: "Thread 하나로 여러 작업 vs Thread 여러 개 생성?"

**답**: 둘 다 틀렸다.

**정답**: "Thread Pool(15개)을 미리 생성하고, CompletableFuture로 작업을 제출하여 재사용"

**Thread Pool이 하는 일:**

1. 15개의 Worker Thread를 한 번만 생성
2. 각 요청마다 15개 카테고리 작업을 이 Pool에 제출
3. Thread는 작업 완료 후 다시 Pool로 반환되어 다음 요청에 재사용
4. 100개 요청이 와도 Thread는 15개만 유지 (+ Queue에서 대기)

**이것은:**

- Thread 직접 생성보다 효율적 (재사용)
- Single-threaded Event Loop보다 구현 간단 (기존 코드 유지)
- I/O Bound 작업에 충분한 병렬성 제공

### 4.6 비교 항목

| 비교 항목 | Thread 직접 생성 | CompletableFuture + Pool | Reactive (WebFlux) |
|----------|-----------------|-------------------------|-------------------|
| Thread 수 | 요청당 15개 생성 | Pool 15개 재사용 | Event Loop 8개 |
| 자원 효율 | ❌ 낮음 | ✅ 높음 | ✅ 매우 높음 |
| 학습 난이도 | 낮음 | 중간 | 높음 |
| 기존 코드 호환 | ✅ 가능 | ✅ 가능 | ❌ 전면 수정 필요 |
| 동시 요청 처리 | 제한 없음 (위험) | ✅ Queue로 제어 | ✅ Backpressure |
| 적합한 상황 | 없음 | ✅ I/O Bound + MVC | CPU/I/O 모두, 대규모 |

**현재 프로젝트 정답**: CompletableFuture + ThreadPoolExecutor (15 core, 20 max)

이것은 "하나의 Thread에 여러 작업"이 아니라, "15개의 Worker Thread가 각각 1개씩 카테고리를 담당하며, Main Thread는 결과만 수집"하는 구조다. Thread 직접 생성보다 효율적이고, Reactive보다 간단하며, I/O Bound 작업에 최적화되어 있다.

---

## 5. OS 스케줄러와 Context Switching의 실체

### 5.1 질문의 핵심: "결국 비동기도 Thread 여러 개 만드는 거면 느려지는 거 아닌가?"

작업을 수행하는 주체인 Thread 자체가 15개 생성되고, CPU 코어가 8개라면, 여러 Thread들이 CPU를 경합하면서 Context Switching이 발생하여 오히려 성능이 저하되는 것 아닌가?

### 5.2 CPU-bound vs I/O-bound의 근본적 차이

#### 5.2.1 CPU-bound 작업: 우려가 맞는 경우

**예시: 암호화 연산**

```java
// 15개의 암호화 작업을 동시에 실행
for (int i = 0; i < 15; i++) {
    CompletableFuture.supplyAsync(() -> {
        return performHeavyEncryption(data);  // CPU 100% 사용
    }, executor);
}
```

**CPU 관점에서 일어나는 일:**

```
CPU Core 1: [Thread-1 암호화] → [Thread-2 암호화] → [Thread-3 암호화] ...
CPU Core 2: [Thread-4 암호화] → [Thread-5 암호화] → [Thread-6 암호화] ...
...
CPU Core 8: [Thread-13 암호화] → [Thread-14 암호화] → [Thread-15 암호화]
```

**문제점:**

- 15개 Thread가 모두 CPU 연산을 수행하려고 경합
- 8개 코어에서 15개 Thread를 돌리려면 Context Switching 발생
- **이 경우 멀티 Thread는 오히려 성능을 떨어뜨릴 수 있다**
- Context Switching 오버헤드 > 병렬 실행 이득

**CPU-bound에서의 최적 Thread 수:**

```
최적 Thread 수 = CPU 코어 수 (8개)
```

이 경우 Thread를 15개 만드는 것은 비효율적이다. 우려가 정확하다.

#### 5.2.2 I/O-bound 작업: 우려가 틀린 경우

**예시: 네트워크 API 호출 (현재 상황)**

```java
// 15개의 네트워크 요청을 동시에 실행
for (String category : categories) {
    CompletableFuture.supplyAsync(() -> {
        return webClient.get()
                .uri("https://api.kakao.com/...")
                .retrieve()
                .bodyToMono(String.class)
                .block();  // 네트워크 응답 대기
    }, executor);
}
```

**CPU 관점에서 실제로 일어나는 일:**

```
시간축 →

Thread-1: [HTTP 요청 전송 1ms] → [네트워크 대기 100ms] → [응답 처리 1ms]
Thread-2: [HTTP 요청 전송 1ms] → [네트워크 대기 100ms] → [응답 처리 1ms]
...
Thread-15: [HTTP 요청 전송 1ms] → [네트워크 대기 100ms] → [응답 처리 1ms]

CPU 사용: 각 Thread당 2ms만 CPU 사용, 나머지 100ms는 CPU 놀고 있음
```

**핵심 차이:**

1. HTTP 요청을 보내는 순간, Thread는 **WAITING 상태**로 전환된다.
2. OS는 이 Thread를 실행 큐에서 제거한다 (Context Switching 대상이 아님).
3. CPU는 다른 프로세스/Thread를 실행한다.
4. 네트워크 카드가 응답을 받으면 하드웨어 인터럽트 발생.
5. OS는 WAITING 상태의 Thread를 RUNNABLE로 전환하고 실행 큐에 재삽입.

**오해:**

"15개 Thread가 CPU를 차지하기 위해 Context Switching 경합한다"

**실제:**

"15개 Thread 중 대부분(14개)은 WAITING 상태로 CPU 경합에서 제외되어 있고, 실제로 CPU를 쓰는 Thread는 1~2개뿐이다."

### 5.3 Thread 상태 전환의 실체

#### 5.3.1 Java Thread의 상태

```java
public enum State {
    NEW,          // Thread 생성, 아직 시작 안됨
    RUNNABLE,     // 실행 가능 (CPU 대기 큐에 있음)
    BLOCKED,      // synchronized 락 대기
    WAITING,      // 무기한 대기 (notify, I/O 완료 대기)
    TIMED_WAITING,// 시간 제한 대기 (sleep, timeout)
    TERMINATED    // 실행 완료
}
```

#### 5.3.2 카카오 API 호출 시 Thread 상태

```java
Thread-1: RUNNABLE (1ms) → WAITING (100ms) → RUNNABLE (1ms) → TERMINATED
                ↑                  ↑                ↑
            HTTP 요청 전송    네트워크 대기      응답 처리
```

**WAITING 상태의 의미:**

- OS 스케줄러는 WAITING 상태의 Thread를 **실행 대상에서 제외**한다.
- CPU 슬라이스를 할당받지 않는다.
- Context Switching 대상이 아니다.
- **메모리만 차지하고, CPU는 전혀 소비하지 않는다.**

**실제 CPU 경합:**

```
8개 CPU 코어
15개 Thread 중 13개는 WAITING (네트워크 대기)
실제로 RUNNABLE 상태인 Thread: 1~2개

→ Context Switching 오버헤드는 거의 없음
→ CPU 코어는 대부분 유휴 상태 (다른 프로세스 실행 가능)
```

### 5.4 순차 vs 병렬의 실제 차이

#### 5.4.1 순차 실행 (현재 코드)

```java
// Main Thread의 시간선
for (String category : categories) {
    List<Place> places = searchPlacesByCategory(category);
    // Main Thread가 네트워크 응답을 기다리며 WAITING 상태
}
```

**타임라인:**

```
Main Thread:
[CS2 요청 1ms] → [WAITING 100ms] → [처리 1ms] →
[FD6 요청 1ms] → [WAITING 100ms] → [처리 1ms] →
[CE7 요청 1ms] → [WAITING 100ms] → [처리 1ms] →
...
총 시간: (1 + 100 + 1) × 15 = 1,530ms

Main Thread가 WAITING인 동안: 다른 HTTP 요청을 처리할 수 없음
→ 서버 처리량(TPS) 감소
```

**CPU 사용률:**

- Main Thread가 WAITING일 때 CPU는 놀고 있음 (다른 프로세스는 실행 가능)
- 실제 CPU 사용: 15 × 2ms = 30ms (총 시간의 1.96%)

#### 5.4.2 병렬 실행 (개선 후)

```java
// Main Thread가 15개 Worker Thread에 작업 위임
List<CompletableFuture<...>> futures = new ArrayList<>();
for (String category : categories) {
    futures.add(CompletableFuture.supplyAsync(() -> 
        searchPlacesByCategory(category), executor
    ));
}
CompletableFuture.allOf(...).join();
```

**타임라인:**

```
Main Thread:
[15개 작업 제출 5ms] → [WAITING: Worker들 완료 대기] → [결과 수집 5ms]

Worker Thread-1: [CS2 요청 1ms] → [WAITING 100ms] → [처리 1ms]
Worker Thread-2: [FD6 요청 1ms] → [WAITING 100ms] → [처리 1ms]
...
Worker Thread-15: [PM9 요청 1ms] → [WAITING 100ms] → [처리 1ms]

총 시간: max(모든 Worker의 시간) ≈ 110ms

15개 Worker가 동시에 WAITING 상태
→ CPU는 여전히 놀고 있음 (다른 요청 처리 가능)
```

**CPU 사용률:**

- 실제 CPU 사용: 15 × 2ms = 30ms (순차와 동일)
- 하지만 총 소요 시간: 1,530ms → 110ms (14배 단축)

**핵심:**

- CPU 사용량은 동일하다 (30ms)
- 벽시계 시간(Wall-clock time)이 줄어든다 (1,530ms → 110ms)
- Main Thread가 빨리 해방되어 다음 요청을 받을 수 있다

### 5.5 질문에 대한 직접 답변

#### "결국 비동기가 아니지 않나?"

**틀렸다.** 비동기의 목적은 **"CPU를 더 쓰는 것"이 아니라, "Thread를 더 효율적으로 쓰는 것"**이다.

**비유:**

- **순차 실행**: 택배 기사 1명이 15개 물건을 순차적으로 배달 (각 배달지마다 왕복)
- **병렬 실행**: 택배 기사 15명이 동시에 출발 (각자 1개씩 배달)

**주장:**

"택배 기사 15명이 도로를 나눠 써야 하니까, 결국 교통 체증으로 더 느려지는 거 아닌가?"

**반박:**

"택배 기사들은 대부분의 시간을 고객이 문을 열 때까지 기다리는 데 쓴다. 도로(CPU)는 거의 사용하지 않는다. 15명이 동시에 기다리는 것이 1명이 15번 방문하는 것보다 훨씬 빠르다."

### 5.6 Context Switching 오버헤드는 실제로 무시 가능한가?

#### 5.6.1 우려가 현실화되는 경우

**잘못된 Thread Pool 설정:**

```java
ExecutorService executor = Executors.newFixedThreadPool(1000);  // ❌ 너무 많음
```

**문제:**

- 1,000개 Thread가 모두 생성되어 메모리 낭비 (각 Thread당 1MB 스택)
- RUNNABLE 상태인 Thread가 많으면 OS 스케줄러 부담 증가
- Context Switching 빈도 증가

#### 5.6.2 올바른 설정 (I/O-bound)

```java
ExecutorService executor = Executors.newFixedThreadPool(15);  // ✅ 적정
```

**이유:**

- 15개 카테고리 × 1개 Thread = 15개 동시 실행
- 대부분의 시간을 WAITING 상태로 보냄
- RUNNABLE 상태인 Thread: 1~2개 (HTTP 요청 전송/응답 처리 시)
- Context Switching 횟수: 최소

**Little's Law 기반 계산:**

```
최적 Thread 수 = (요청 처리 시간 / CPU 실제 사용 시간) × CPU 코어 수

예시:
- 요청 처리 시간: 100ms
- CPU 실제 사용: 2ms (나머지 98ms는 I/O 대기)
- CPU 코어: 8개

최적 Thread 수 = (100 / 2) × 8 = 400개

→ 15개는 매우 보수적인 수치 (Context Switching 걱정 불필요)
```

### 5.7 Main Thread의 역할

**오해:**

"Main Thread에서 하위 작업을 비동기로 동작시키는..."

**실제:**

Main Thread는 실제로 작업을 수행하지 않는다.

```java
// ===== Main Thread (http-nio-8185-exec-2) =====
public ExternalApiResult performExternalApiCalls(request) {
    
    // 1. Executor에 작업 제출 (0.1ms)
    CompletableFuture<...> future1 = CompletableFuture.supplyAsync(() -> {
        return searchPlacesByCategory("CS2");  // Worker가 실행
    }, executor);
    
    // 2. Main Thread는 즉시 다음 줄 실행 (Blocking 없음)
    CompletableFuture<...> future2 = CompletableFuture.supplyAsync(() -> {
        return searchPlacesByCategory("FD6");  // Worker가 실행
    }, executor);
    
    // ... 15개 작업 제출 (총 1ms)
    
    // 3. Main Thread는 모든 Worker의 완료를 기다림 (WAITING)
    CompletableFuture.allOf(futures...).join();  // Blocking
    
    // 4. Worker들이 완료되면 Main Thread가 RUNNABLE로 전환되어 결과 수집
    return result;
}
```

**Main Thread의 상태 변화:**

```
RUNNABLE (작업 제출 1ms) →
WAITING (Worker 완료 대기 110ms) →
RUNNABLE (결과 수집 5ms) →
TERMINATED
```

**Worker Thread-1의 상태:**

```
RUNNABLE (HTTP 요청 전송 1ms) →
WAITING (네트워크 대기 100ms) →
RUNNABLE (응답 처리 1ms) →
IDLE (Pool로 반환, 다음 작업 대기)
```

### 5.8 정리: 왜 병렬화가 효과적인가

#### CPU 관점

**순차 실행:**

```
CPU 사용: 30ms / 1,530ms = 1.96%
CPU 유휴: 98.04% (다른 프로세스 실행 가능)
```

**병렬 실행:**

```
CPU 사용: 30ms / 110ms = 27.3%
CPU 유휴: 72.7% (다른 프로세스 실행 가능)
```

**결론:**

- CPU 사용량은 동일 (30ms)
- 병렬 실행이 CPU를 더 효율적으로 활용 (유휴 시간 단축)
- Context Switching 오버헤드는 무시 가능 (대부분 WAITING 상태)

#### Thread 관점 (실제 병목)

**순차 실행의 문제:**

```
Spring MVC Thread Pool: 200개 (기본값)
1개 요청이 Main Thread를 1,530ms 동안 점유
동시 요청 100개가 오면: 100개 Thread가 모두 Blocking
→ Thread Pool 고갈
→ 새로운 요청은 대기 큐에서 지연
→ 시스템 응답 불가
```

**병렬 실행의 효과:**

```
1개 요청이 Main Thread를 110ms만 점유 (14배 빠름)
동시 요청 100개가 와도: Thread Pool이 빠르게 회전
→ TPS (Transactions Per Second) 증가
→ 시스템 처리량 향상
```

### 5.9 최종 답변

#### 질문 1: "결국 15개 Thread가 CPU를 경합하는 거 아닌가?"

**아니다.** 15개 Thread 중 13~14개는 항상 WAITING 상태(네트워크 대기)로 CPU 경합에서 제외되어 있다. 실제로 RUNNABLE 상태인 Thread는 1~2개뿐이며, 8개 CPU 코어에 충분히 여유가 있다.

#### 질문 2: "Context Switching으로 오히려 느려지는 거 아닌가?"

**아니다.** I/O-bound 작업에서는 Thread가 대부분의 시간을 WAITING 상태로 보내므로, Context Switching 빈도가 낮다. 실제 측정 시 Context Switching 오버헤드는 전체 시간의 1% 미만이다.

#### 질문 3: "Main 입장에서만 비동기지, 결국 비동기가 아니지 않나?"

**비동기의 정의를 오해했다.** 비동기는 "CPU를 안 쓰는 것"이 아니라, "호출자(Main Thread)를 빨리 해방시켜 다른 요청을 받을 수 있게 하는 것"이다. Worker Thread는 실제로 동기적으로 작동하지만, Main Thread 관점에서는 비동기다.

#### 질문 4: "그럼 왜 빨라지는가?"

**벽시계 시간(Wall-clock time)이 줄어들기 때문이다.** 15개 작업을 순차 실행하면 1,530ms가 걸리지만, 병렬 실행하면 110ms만 걸린다. CPU 사용량은 동일하지만, Thread가 네트워크를 기다리는 시간을 중첩시켜 전체 시간을 단축한다.

### 5.10 진짜 Single-threaded 비동기: 비교

"하나의 Thread로 여러 작업"은 **Event Loop 모델**이다.

**Node.js / Netty / Reactor Netty:**

```
Event Loop Thread (1개)
→ HTTP 요청 15개를 OS 커널에 위임 (Non-blocking I/O)
→ Event Loop는 즉시 다음 작업 실행
→ OS가 응답 도착을 알리면 Callback 실행

CPU 관점: Thread 1개만 사용
처리 시간: 여전히 110ms (네트워크 병렬 대기)
```

**Event Loop와 CompletableFuture의 차이:**

- **Event Loop**: Thread 1개, Non-blocking I/O
- **CompletableFuture**: Thread 15개, Blocking I/O

**결과는 동일**: 둘 다 110ms에 완료

**하지만:**

- Event Loop는 구현 복잡도가 높음 (Reactive Programming)
- CompletableFuture는 기존 Blocking 코드 유지 가능

**현재 상황**: CompletableFuture가 정답이다. Thread 15개 정도는 Context Switching 걱정이 없고, 기존 코드 구조를 유지할 수 있다.

---

## 6. 개념 이해 검증 및 정밀 교정

### 6.1 이해 검증

다음은 비동기 프로그래밍에 대한 일반적인 이해를 검증하고 교정하는 과정이다.

#### 6.1.1 ✅ 정확하게 이해한 부분

**1. "비동기는 Main Thread가 결과를 받아서 처리하는 주체일 뿐"**

정확하다. Main Thread는 작업 자체를 수행하지 않고, Worker Thread에 작업을 위임한 뒤 결과만 수집한다.

```java
// Main Thread의 실제 역할
CompletableFuture<A> future1 = CompletableFuture.supplyAsync(() -> taskA(), executor);
// ↑ Main Thread는 Executor에 작업 제출만 하고 즉시 반환

CompletableFuture<B> future2 = CompletableFuture.supplyAsync(() -> taskB(), executor);
// ↑ Main Thread는 또 제출만 하고 즉시 반환

future1.join();  // ← 여기서만 Main Thread가 대기 (WAITING)
future2.join();

// Main Thread는 작업 A, B를 직접 수행하지 않았다
// Worker Thread-1이 taskA()를, Worker Thread-2가 taskB()를 수행했다
```

**2. "하나의 Thread가 여러 작업을 돌려받아가며 실행하는 것이 아니다"**

정확하다. Java의 CompletableFuture는 **1 Thread : 1 Task** 모델이다.

Thread-1이 작업 A를 실행하는 동안, 작업 B를 동시에 실행하지 않는다. 작업 A가 끝나면 Thread-1은 Pool로 반환되고, 다음 작업 C를 받아 실행한다.

배제된 "하나의 Thread가 여러 작업을 번갈아 실행"하는 모델:

- **Node.js Event Loop**: 단일 Thread가 비선점형 스케줄링으로 여러 작업 번갈아 실행
- **Green Thread / Coroutine**: OS Thread가 아닌 언어 레벨 경량 Thread (Kotlin Coroutine, Go Goroutine)
- Java의 CompletableFuture는 이런 방식이 아니다.

**3. "CPU 코어보다 Thread가 많으면 Context Switching 발생"**

맞다. 하지만 중요한 조건이 있다: **RUNNABLE 상태인 Thread가 코어보다 많을 때만**.

```
상황 A: CPU-bound 작업 (우려가 현실화)
- Thread 15개가 모두 암호화 연산 수행 (RUNNABLE)
- CPU 8개 코어에서 실행하려면 Context Switching 불가피
- 오버헤드 발생

상황 B: I/O-bound 작업 (현재 상황)
- Thread 15개 중 13개는 네트워크 대기 (WAITING)
- RUNNABLE 상태: 2개만
- CPU 8개 코어에 충분한 여유
- Context Switching 거의 없음
```

**4. "I/O 작업이면 Context Switching 비용이 작업 시간 대비 크지 않다"**

정확하다. 이것이 핵심이다.

**Context Switching 비용:**

- TLB flush, 레지스터 저장/복원: 약 1~5μs (마이크로초)

**I/O 대기 시간:**

- 네트워크 응답: 100ms = 100,000μs

**비율:**

```
Context Switching: 5μs
I/O 대기: 100,000μs

비용 비율: 5 / 100,000 = 0.005% (무시 가능)
```

"어차피 I/O로 넘어가야 되니, 그 전에 Context Switching 하든 안 하든 영향 미미"가 정확하다.

#### 6.1.2 ⚠️ 교정이 필요한 부분

**오해 1: "비동기라는 건 한 개의 Thread가 여러 개의 작업을 수행"**

**정밀하게 수정:**

"비동기"의 정의는 **호출 모델**에 대한 것이지, Thread 개수에 대한 것이 아니다.

```
동기 (Synchronous):
호출자가 작업 완료까지 대기 (Blocking)

Main Thread: [작업 A 실행] ---- 완료 대기 ---- [결과 받음] [작업 B 실행] ----
             ↑ Main이 직접 실행하거나, 다른 Thread에 맡기고 완료까지 대기

비동기 (Asynchronous):
호출자가 즉시 반환, 나중에 결과 수신

Main Thread: [작업 A 위임] [작업 B 위임] ... [나중에 결과 수집]
Worker-1:                  [작업 A 실행] ---- [완료]
Worker-2:                         [작업 B 실행] ---- [완료]
```

비동기는 "Thread 개수"가 아니라, "호출자가 대기하지 않는다"는 실행 모델이다.

**오해 2: "하나의 Thread가 여러 작업을 돌려받아가며"**

이 표현이 애매하다. 두 가지 해석이 가능:

**해석 A: "Thread-1이 작업 A와 작업 B를 동시에 실행?"**

→ 불가능. Thread는 한 번에 1개 작업만 실행 (1 Thread : 1 Task at a time)

**해석 B: "Thread-1이 작업 A 완료 후, 작업 B를 다시 받아서 실행?"**

→ 가능. 이것이 Thread Pool의 재사용 메커니즘.

```java
// Thread Pool에서 Thread 재사용
ExecutorService executor = Executors.newFixedThreadPool(3);

// 5개 작업 제출
for (int i = 0; i < 5; i++) {
    executor.submit(() -> doWork());
}

// 실제 동작:
// Thread-1: [작업 1] → Pool 반환 → [작업 4] → Pool 반환
// Thread-2: [작업 2] → Pool 반환 → [작업 5] → Pool 반환
// Thread-3: [작업 3] → Pool 반환 → 대기
```

배제하려던 것은 **"동시에 여러 작업"**이고, 이는 맞다. 하지만 **"순차적으로 여러 작업"**은 일어난다.

**오해 3: "CPU 버스트만 존재하는 스레드들"**

**용어 정정:**

"CPU 버스트"는 **작업(Task)의 특성**이지, **Thread의 특성**이 아니다.

```
CPU-bound Task: 암호화, 압축, 이미지 처리
→ 이 작업을 실행하는 Thread는 대부분의 시간을 RUNNABLE 상태로 보냄

I/O-bound Task: 네트워크 요청, 파일 읽기
→ 이 작업을 실행하는 Thread는 대부분의 시간을 WAITING 상태로 보냄
```

**더 정확한 표현:**

"CPU-bound 작업을 수행하는 Thread들이 많으면 Context Switching 부담이 크지만, I/O-bound 작업을 수행하는 Thread들은 대부분 WAITING 상태라 Context Switching 빈도가 낮다."

### 6.2 정밀한 메커니즘: Context Switching 발생 조건

#### 6.2.1 OS 스케줄러의 실제 동작

**1. Runnable Queue (실행 대기 큐)**

```
CPU Core 1: [Thread-1 실행 중]
CPU Core 2: [Thread-2 실행 중]
...
CPU Core 8: [Thread-8 실행 중]

Runnable Queue: [Thread-9] [Thread-10] [Thread-11] ← Context Switching 대상
                ↑ 이들이 CPU를 기다리며 경합

Waiting Queue: [Thread-12] [Thread-13] [Thread-14] [Thread-15]
               ↑ I/O 대기 중, CPU 경합에서 제외
```

**2. Context Switching 발생 조건**

```c
// OS 스케줄러 의사코드
if (runnable_queue.size() > cpu_core_count) {
    // Time-slice 만료 시 Context Switching
    context_switch();  // 1~5μs 소요
}

if (thread.state == WAITING) {
    // WAITING Thread는 Runnable Queue에서 제거
    // Context Switching 대상이 아님
}
```

**3. 카카오 API 상황**

```
시작 시점 (0ms):
- 15개 Thread가 동시에 RUNNABLE로 시작
- Runnable Queue: [T1, T2, T3, ..., T15]
- CPU 8개는 8개 Thread 실행
- 나머지 7개는 Queue에서 대기 → Context Switching 발생 (여기서만!)

1ms 후:
- 8개 Thread가 HTTP 요청을 전송하고 WAITING으로 전환
- Runnable Queue: [T9, T10, T11, ..., T15] (7개)
- CPU 8개 중 7개만 사용
- Context Switching 중단

2ms 후:
- 7개 Thread도 HTTP 요청을 전송하고 WAITING으로 전환
- Runnable Queue: [] (비어있음)
- CPU 8개 모두 유휴
- Context Switching 없음

100ms 후 (응답 도착):
- 네트워크 카드가 하드웨어 인터럽트 발생
- OS가 Thread-1을 WAITING → RUNNABLE로 전환
- Thread-1이 응답 처리 (1ms)
- 다시 TERMINATED로 전환
```

**결론:**

- Context Switching은 처음 1~2ms에만 발생 (15개가 동시에 시작할 때)
- 나머지 100ms 동안은 모두 WAITING 상태
- 총 Context Switching 비용: 약 10~20μs
- 전체 시간 대비: 0.01~0.02% (무시 가능)

### 6.3 CPU 버스트 vs I/O 버스트의 정확한 비교

#### 6.3.1 CPU-bound: 우려가 현실화

```java
// 암호화 작업 (CPU 100% 사용)
for (int i = 0; i < 15; i++) {
    CompletableFuture.supplyAsync(() -> {
        return performHeavyCrypto(data);  // 100ms CPU 연산
    }, executor);
}
```

**Thread 상태:**

```
Thread-1: [RUNNABLE 100ms] → TERMINATED
Thread-2: [RUNNABLE 100ms] → TERMINATED
...
Thread-15: [RUNNABLE 100ms] → TERMINATED

전체 15개가 RUNNABLE 상태 유지
→ CPU 8개 코어에서 15개 Thread 경합
→ 지속적인 Context Switching
→ 오버헤드 누적: 약 5ms~10ms
```

**최적 설정:**

```java
// CPU-bound는 코어 수만큼만 Thread 생성
ExecutorService executor = Executors.newFixedThreadPool(8);  // ✅
```

#### 6.3.2 I/O-bound: Context Switching 무시 가능

```java
// 네트워크 요청 (CPU 2% 사용)
for (int i = 0; i < 15; i++) {
    CompletableFuture.supplyAsync(() -> {
        return kakaoApi.search(category);  // 1ms CPU + 100ms I/O
    }, executor);
}
```

**Thread 상태:**

```
Thread-1: [RUNNABLE 1ms] → [WAITING 100ms] → [RUNNABLE 1ms] → TERMINATED
Thread-2: [RUNNABLE 1ms] → [WAITING 100ms] → [RUNNABLE 1ms] → TERMINATED
...
Thread-15: [RUNNABLE 1ms] → [WAITING 100ms] → [RUNNABLE 1ms] → TERMINATED

대부분의 시간(98ms)을 WAITING 상태로 보냄
→ RUNNABLE 상태인 Thread: 평균 1~2개
→ CPU 8개 코어에 충분한 여유
→ Context Switching 거의 없음
```

**최적 설정:**

```java
// I/O-bound는 더 많은 Thread 허용
// Little's Law: Thread 수 = (응답 시간 / CPU 시간) × 코어 수
// = (100ms / 2ms) × 8 = 400개까지 가능
ExecutorService executor = Executors.newFixedThreadPool(15);  // ✅ 보수적
```

### 6.4 표현 교정

**원래 표현:**

"cpu 가동 시간 -> i/o 작업으로 어차피 넘어가야 되니 그 전에 문맥교환이 이루어지면 이 또한 비용이지만 어차피 그건 진행될 것"

**정밀하게 수정:**

"CPU 가동 시간(1ms) → I/O 대기(100ms) 구간에서, CPU 가동 중에 Context Switching이 발생해도 비용(5μs)이 작업 시간 대비 매우 작고(0.005%), I/O 대기 중에는 Thread가 WAITING 상태라 Context Switching 자체가 발생하지 않으므로, 전체 성능에 미치는 영향이 무시 가능하다."

**놓친 핵심:**

- "어차피 진행될 것"이 아니라, "대부분 진행되지 않는다"
- I/O 대기 중(100ms)에는 Thread가 WAITING 상태라 OS 스케줄러가 아예 관여하지 않음
- Context Switching은 CPU 가동 구간(1ms)에서만 발생 가능
- 그마저도 CPU 코어에 여유가 있으면 발생하지 않음

### 6.5 최종 정리: 이해 검증

#### ✅ 완벽하게 이해한 부분

1. "비동기는 Main Thread가 결과를 받아서 처리하는 주체일 뿐" → 정확
2. "하나의 Thread가 여러 작업을 동시에 돌리는 게 아니다" → 정확
3. "CPU 코어보다 Thread가 많으면 Context Switching 발생" → 정확 (단, RUNNABLE 상태일 때만)
4. "I/O 작업이면 Context Switching 비용이 작다" → 정확

#### ⚠️ 미세하게 교정한 부분

**1. "비동기 = 한 개의 Thread가 여러 작업 수행"**

**수정**: "비동기 = 호출자가 대기하지 않고, Worker에 위임"

Thread 개수는 비동기 정의와 무관

**2. "CPU 버스트만 존재하는 스레드들"**

**수정**: "CPU-bound 작업을 수행하는 Thread들"

CPU 버스트는 작업의 특성, Thread의 특성이 아님

**3. "어차피 그건 진행될 것"**

**수정**: "I/O 대기 중에는 Context Switching 자체가 발생하지 않음"

WAITING 상태 Thread는 스케줄링 대상 제외

### 6.6 핵심 통찰

**핵심 질문:**

"결국 비동기도 Thread 여러 개 만드는 거면, Context Switching으로 느려지는 거 아닌가?"

**정답:**

"I/O-bound 작업에서는 Thread가 대부분 WAITING 상태라 Context Switching이 거의 발생하지 않으므로, Thread를 많이 만들어도 오버헤드가 무시 가능하다. CPU-bound 작업에서는 우려가 맞지만, 네트워크 API 호출은 I/O-bound이므로 Thread 15개는 전혀 문제없다."

### 6.7 실전 검증 방법

직접 확인하고 싶다면:

```java
// VisualVM 또는 JConsole로 Thread 상태 모니터링
while (true) {
    ThreadInfo[] threads = ManagementFactory.getThreadMXBean()
        .getThreadInfo(ManagementFactory.getThreadMXBean().getAllThreadIds());
    
    int runnable = 0, waiting = 0, blocked = 0;
    for (ThreadInfo info : threads) {
        if (info != null) {
            switch (info.getThreadState()) {
                case RUNNABLE: runnable++; break;
                case WAITING: waiting++; break;
                case BLOCKED: blocked++; break;
            }
        }
    }
    
    System.out.printf("RUNNABLE: %d, WAITING: %d, BLOCKED: %d\n", 
                      runnable, waiting, blocked);
    Thread.sleep(10);
}

// 결과:
// RUNNABLE: 2, WAITING: 13, BLOCKED: 0  ← 대부분 WAITING
// RUNNABLE: 1, WAITING: 14, BLOCKED: 0
// RUNNABLE: 3, WAITING: 12, BLOCKED: 0
```

이것을 실제로 측정하면, 이해가 정확하다는 것을 눈으로 확인할 수 있다.

---

## 7. 검색 키워드 및 학습 자료

### 7.1 핵심 개념별 검색 전략

#### 7.1.1 A. Thread 상태와 OS 스케줄링

**검색 키워드:**

```
java thread states RUNNABLE WAITING
operating system thread scheduling
context switching overhead measurement
io bound vs cpu bound thread pool sizing
little's law thread pool calculation
blocking io vs non-blocking io

# 한글
자바 스레드 상태 전환
운영체제 스레드 스케줄링
문맥 교환 비용
CPU bound IO bound 차이
```

**필독 문서:**

1. **Java Thread States 공식 문서**
   - `Thread.State` enum Javadoc
   - URL: `https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.State.html`
   - **핵심**: RUNNABLE, WAITING, BLOCKED의 정확한 정의

2. **"Java Concurrency in Practice" (책)**
   - Chapter 8: Applying Thread Pools
   - Chapter 11: Performance and Scalability
   - **핵심**: Thread Pool 크기 계산 공식, Little's Law 적용

3. **Linux Thread Scheduling 문서**
   - `man 7 sched`
   - Completely Fair Scheduler (CFS) 설명
   - **핵심**: WAITING 상태 Thread가 스케줄링에서 제외되는 메커니즘

**구글 검색 예시:**

```
site:stackoverflow.com "WAITING vs RUNNABLE" java
site:docs.oracle.com thread pool sizing
"context switching" "io bound" measurement
```

#### 7.1.2 B. CompletableFuture 내부 동작

**검색 키워드:**

```
completablefuture internal implementation
completablefuture thread pool default
forkjoinpool commonpool vs custom executor
completablefuture supplyAsync source code

# 한글
CompletableFuture 내부 구조
ForkJoinPool 동작 원리
```

**필독 문서:**

1. **OpenJDK Source Code**
   - java.util.concurrent.CompletableFuture 소스
   - URL: https://github.com/openjdk/jdk/blob/master/src/java.base/share/classes/java/util/concurrent/CompletableFuture.java
   - **핵심**: asyncSupplyStage() 메서드가 실제로 Thread를 어떻게 할당하는지

2. **Baeldung: Guide to CompletableFuture**
   - URL: https://www.baeldung.com/java-completablefuture
   - **핵심**: 실전 예제와 Best Practice

**OpenJDK 소스 직접 확인:**

```java
// CompletableFuture.java 실제 코드
static <U> CompletableFuture<U> asyncSupplyStage(Executor e,
                                                  Supplier<U> f) {
    if (f == null) throw new NullPointerException();
    CompletableFuture<U> d = new CompletableFuture<U>();
    e.execute(new AsyncSupply<U>(d, f));  // ← Executor에 작업 제출
    return d;
}

// 이 코드를 보면 "실제로 Executor가 Thread를 할당한다"는 것을 증명
```

#### 7.1.3 C. 비동기 모델의 종류

**검색 키워드:**

```
async programming models comparison
blocking io vs non-blocking io
reactor netty event loop
spring webflux vs spring mvc threading

# 한글
동기 비동기 블로킹 논블로킹 차이
Reactor Netty 이벤트 루프
```

**필독 문서:**

1. **IBM Developer: Blocking vs Non-blocking I/O**
   - URL: `https://developer.ibm.com/articles/l-async/`
   - **핵심**: Blocking I/O의 근본적 한계

2. **Spring WebFlux Documentation**
   - URL: `https://docs.spring.io/spring-framework/docs/current/reference/html/web-reactive.html`
   - **핵심**: Reactor 기반 Non-blocking I/O 모델

3. **"Reactive Programming with RxJava" (책)**
   - Chapter 3: Operators and Transformations
   - **핵심**: Event Loop 기반 비동기 vs Thread Pool 기반 비동기

### 7.2 학습 로드맵

#### Phase 1: OS 기초 (2-3일)

```
1. Thread 상태 전환 (RUNNABLE/WAITING/BLOCKED)
   → Java Thread States Javadoc 정독
   → VisualVM으로 실제 상태 관찰

2. Context Switching 측정
   → Linux perf 또는 JMH Benchmark 실습
   → 실제 비용 측정 (μs 단위)

3. I/O-bound vs CPU-bound
   → Little's Law 공식 이해
   → Thread Pool 크기 계산 실습
```

#### Phase 2: CompletableFuture 심화 (3-4일)

```
1. OpenJDK 소스 코드 읽기
   → asyncSupplyStage() 메서드 분석
   → ForkJoinPool vs Custom Executor 차이

2. 실습: Thread 상태 모니터링
   → ThreadMXBean으로 상태 추적
   → WAITING 비율 측정

3. 성능 측정
   → JMH로 순차 vs 병렬 Benchmark
   → 실제 개선 효과 정량화
```

#### Phase 3: 다른 비동기 모델 비교 (2-3일)

```
1. Reactor Netty (Non-blocking I/O)
   → Event Loop 모델 이해
   → Netty의 epoll/kqueue 메커니즘

2. Kotlin Coroutine (Structured Concurrency)
   → suspend function의 내부 동작
   → CPS (Continuation-Passing Style)

3. 실습: 같은 작업을 3가지 방식으로 구현
   → CompletableFuture (Blocking I/O + Thread Pool)
   → WebClient (Non-blocking I/O + Event Loop)
   → Coroutine (suspend + Dispatcher)
```

---

## 8. 라이브러리별 비동기 구현 차이

### 8.1 비동기 모델의 3가지 근본 패러다임

#### 8.1.1 모델 A: Thread Pool + Blocking I/O (현재 상황)

**구현체:**

- Java CompletableFuture + Executor
- Spring MVC + @Async
- Python ThreadPoolExecutor

**동작 원리:**

```java
ExecutorService executor = Executors.newFixedThreadPool(15);

CompletableFuture.supplyAsync(() -> {
    return webClient.get()
        .uri("https://api.kakao.com/...")
        .retrieve()
        .bodyToMono(String.class)
        .block();  // ← Thread가 여기서 Blocking (WAITING)
}, executor);

// Thread 상태:
// RUNNABLE (1ms) → WAITING (100ms) → RUNNABLE (1ms)
```

**설명이 적용되는 범위:**

- ✅ "15개 Thread 생성"
- ✅ "대부분 WAITING 상태"
- ✅ "Context Switching 무시 가능"

#### 8.1.2 모델 B: Event Loop + Non-blocking I/O

**구현체:**

- Node.js (libuv Event Loop)
- Netty (EventLoopGroup)
- Spring WebFlux (Reactor Netty)

**동작 원리:**

```java
// Reactor Netty의 실제 동작
WebClient client = WebClient.create();

Flux.fromIterable(categories)
    .flatMap(category -> 
        client.get()
            .uri("https://api.kakao.com/...?category={}", category)
            .retrieve()
            .bodyToMono(String.class)  // ← Non-blocking, 즉시 반환
    )
    .collectList()
    .block();

// Event Loop Thread 상태:
// 항상 RUNNABLE (절대 WAITING 되지 않음)
// HTTP 요청을 OS 커널에 위임하고 즉시 다음 작업 실행
```

**동작 메커니즘:**

```
Event Loop Thread (1개):
[HTTP 요청 1 → OS 위임] [HTTP 요청 2 → OS 위임] ... [HTTP 요청 15 → OS 위임]
      ↓ (네트워크 카드가 응답 받음)
[OS가 Event Loop에 알림: "요청 3 응답 도착"]
[Event Loop: Callback 실행 → 응답 처리]

Thread 개수: 1개 (또는 CPU 코어 수, 예: 8개)
Thread 상태: 항상 RUNNABLE
I/O 대기: OS 커널이 담당 (epoll, kqueue, IOCP)
```

**설명이 적용되지 않는 부분:**

- ❌ "15개 Thread 생성" → Thread는 8개만 (CPU 코어 수)
- ❌ "WAITING 상태" → Event Loop Thread는 절대 WAITING 안 됨
- ✅ "Context Switching 무시 가능" → Thread가 적어서 더욱 적음

#### 8.1.3 모델 C: Coroutine / Green Thread (경량 Thread)

**구현체:**

- Kotlin Coroutine
- Go Goroutine
- Erlang Process

**동작 원리:**

```kotlin
// Kotlin Coroutine
runBlocking {
    val results = categories.map { category ->
        async(Dispatchers.IO) {
            client.get("https://api.kakao.com/...?category=$category")
        }
    }.awaitAll()
}

// Coroutine은 OS Thread가 아니라 언어 레벨 추상화
// 1개 OS Thread에서 10,000개 Coroutine 실행 가능
```

**동작 메커니즘:**

```
OS Thread 1: [Coroutine-1 실행] [suspend] [Coroutine-2 실행] [suspend] ...
              ↑ I/O 대기 시점에 Thread를 다른 Coroutine에 양보

Coroutine-1: [HTTP 요청] → suspend → [응답 받으면 resume]
Coroutine-2: [HTTP 요청] → suspend → [응답 받으면 resume]
...
Coroutine-15: ## 10. 직접 확인해야 할 문서

### 10.1 CompletableFuture는 실제로 Thread를 어떻게 생성하나?

**읽어야 할 소스:**

```
https://github.com/openjdk/jdk/blob/master/src/java.base/share/classes/java/util/concurrent/CompletableFuture.java

핵심 메서드:
- asyncSupplyStage() (line 1687)
- AsyncSupply.run() (line 1754)
- ForkJoinPool.commonPool() (line 301)
```

**확인할 내용:**

```java
// CompletableFuture.java (실제 OpenJDK 코드)
public static <U> CompletableFuture<U> supplyAsync(Supplier<U> supplier,
                                                    Executor executor) {
    return asyncSupplyStage(executor, supplier);
}

static <U> CompletableFuture<U> asyncSupplyStage(Executor e,
                                                  Supplier<U> f) {
    CompletableFuture<U> d = new CompletableFuture<U>();
    e.execute(new AsyncSupply<U>(d, f));  // ← 이 줄이 핵심
    return d;
}

// AsyncSupply는 Runnable 구현체
static final class AsyncSupply<T> extends ForkJoinTask<Void> implements Runnable {
    public void run() {
        // ← 이 메서드가 실제로 Worker Thread에서 실행됨
        T t = fn.get();
    }
}
```

**이 코드가 증명하는 것:**

- `executor.execute()`가 실제로 Thread에 작업을 제출
- Worker Thread가 `run()` 메서드를 실행
- Supplier의 `get()`이 Worker Thread에서 호출됨

### 10.2 Reactor Netty는 어떻게 Thread 1개로 여러 I/O를 처리하나?

**읽어야 할 문서:**

```
https://projectreactor.io/docs/netty/release/reference/index.html

핵심 개념:
- EventLoop (Netty의 핵심)
- ChannelPipeline (이벤트 처리 파이프라인)
- epoll/kqueue (OS 레벨 Non-blocking I/O)
```

**확인할 내용:**

```java
// Reactor Netty의 실제 동작 (의사코드)
EventLoop eventLoop = new NioEventLoop();

eventLoop.register(socketChannel1);  // HTTP 요청 1 등록
eventLoop.register(socketChannel2);  // HTTP 요청 2 등록
...

// Event Loop Thread의 실제 루프
while (true) {
    Set<SelectionKey> readyKeys = selector.select();  // OS에게 "준비된 채널 알려줘"
    
    for (SelectionKey key : readyKeys) {
        if (key.isReadable()) {
            // 응답이 도착한 Channel에 대해서만 Callback 실행
            channel.read();
            callback.onComplete();
        }
    }
}
```

**이 코드가 증명하는 것:**

- Thread는 selector.select()에서 대기 (하지만 WAITING 아님, RUNNABLE 유지)
- OS가 응답 도착을 알려주면 해당 Callback만 실행
- 1개 Thread가 여러 Socket을 동시에 관리

### 10.3 Thread 상태를 실제로 관찰하는 방법

**JMH Benchmark 작성:**

```java
@State(Scope.Benchmark)
public class ThreadStateBenchmark {
    
    private ExecutorService executor = Executors.newFixedThreadPool(15);
    
    @Benchmark
    public void measureThreadStates() throws Exception {
        ThreadMXBean threadMXBean = ManagementFactory.getThreadMXBean();
        
        List<CompletableFuture<Void>> futures = new ArrayList<>();
        for (int i = 0; i < 15; i++) {
            futures.add(CompletableFuture.runAsync(() -> {
                try {
                    // HTTP 요청 시뮬레이션
                    Thread.sleep(100);  // I/O 대기
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                }
            }, executor));
        }
        
        // Thread 상태 측정
        Thread.sleep(50);  // 작업 진행 중에 측정
        long[] threadIds = threadMXBean.getAllThreadIds();
        ThreadInfo[] threadInfos = threadMXBean.getThreadInfo(threadIds);
        
        int runnable = 0, waiting = 0;
        for (ThreadInfo info : threadInfos) {
            if (info != null && info.getThreadName().startsWith("pool-")) {
                if (info.getThreadState() == Thread.State.RUNNABLE) runnable++;
                if (info.getThreadState() == Thread.State.TIMED_WAITING) waiting++;
            }
        }
        
        System.out.printf("RUNNABLE: %d, WAITING: %d\n", runnable, waiting);
        // 예상 결과: RUNNABLE: 0~2, WAITING: 13~15
        
        CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();
    }
}
```