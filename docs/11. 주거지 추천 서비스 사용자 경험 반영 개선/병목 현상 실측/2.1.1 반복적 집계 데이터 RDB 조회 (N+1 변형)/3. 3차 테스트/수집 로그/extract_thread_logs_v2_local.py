#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ìŠ¤ë ˆë“œë³„ ì„±ëŠ¥ ë¡œê·¸ íŒŒì‹± ë° CSV ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ (v2.0 - ë¡œì»¬ ë²„ì „)
- Java ì½”ë“œ ë³€ê²½ì‚¬í•­ ë°˜ì˜ (Chunking ë²„ì „)
- Bottleneck Resolved: 2.1.1 (Chunking) íŒ¨í„´ ëŒ€ì‘
- RDB Chunk Sum ë° ë™ì  í˜¸ì¶œ íšŸìˆ˜ ì²˜ë¦¬
- ë¡œì»¬ í™˜ê²½ ì‹¤í–‰ìš© (ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©)
"""

import re
import csv
import os
import sys
from collections import defaultdict

def parse_log_file(file_path):
    """
    ë¡œê·¸ íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ìŠ¤ë ˆë“œë³„ ì„±ëŠ¥ ë°ì´í„° ì¶”ì¶œ
    Java ì½”ë“œ ë³€ê²½ì‚¬í•­ ë°˜ì˜ ë²„ì „
    """
    
    # ìŠ¤ë ˆë“œë³„ ë°ì´í„° ì €ì¥
    thread_data = defaultdict(list)
    
    # ===== ì—…ë°ì´íŠ¸ëœ ì •ê·œ í‘œí˜„ì‹ íŒ¨í„´ë“¤ =====
    thread_pattern = r'(\d{2}:\d{2}:\d{2}\.\d{3})\s+\[([^\]]+)\]\s+(\w+)'
    
    # Bottleneck íŒ¨í„´ - (Chunking) í¬í•¨ ì—¬ë¶€ ì˜µì…˜ì²˜ë¦¬
    bottleneck_pattern = r'=== \[Bottleneck Resolved: ([\d.]+)(?:\s+\([^)]+\))?\] ==='
    
    # ê¸°ë³¸ íŒ¨í„´ë“¤
    total_time_pattern = r'1\. ì´ ì†Œìš” ì‹œê°„: (\d+)ms'
    
    # RDB íŒ¨í„´ - Bulk ë˜ëŠ” Chunk Sum ëª¨ë‘ ëŒ€ì‘
    rdb_time_pattern = r'2\. RDB ì¡°íšŒ ì‹œê°„ \((?:Bulk|Chunk Sum)\): (\d+)ms \(ì „ì²´ì˜ ([\d.]+)%\)'
    
    # RDB í˜¸ì¶œ íšŸìˆ˜ - ë‹¤ì–‘í•œ í˜•íƒœ ëŒ€ì‘
    rdb_count_pattern = r'3\. RDB í˜¸ì¶œ íšŸìˆ˜: (\d+)íšŒ'
    
    # ìƒíƒœ íŒ¨í„´
    status_pattern = r'ì „ì„¸ ì§€ì—­êµ¬ ì¶”ì²œ ìš”ì²­ ì™„ë£Œ - ìƒíƒœ: (\w+)'
    
    # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ìŠ¤ë ˆë“œ ì •ë³´
    current_thread_info = {}
    current_thread = None
    current_timestamp = None
    current_log_level = None
    
    print(f"ğŸ“‚ íŒŒì‹± ì¤‘: {os.path.basename(file_path)}")
    
    # íŒŒì¼ ì½ê¸°
    with open(file_path, 'r', encoding='utf-8') as file:
        for line_num, line in enumerate(file, 1):
            line = line.strip()
            
            # ë¹ˆ ì¤„ ë˜ëŠ” ì¤‘ê°„ ìƒëµ í‘œì‹œ ê±´ë„ˆë›°ê¸°
            if not line or '[ì¤‘ê°„ ìƒëµ]' in line:
                continue
            
            # ìŠ¤ë ˆë“œ ì •ë³´ ì¶”ì¶œ
            thread_match = re.match(thread_pattern, line)
            if thread_match:
                current_timestamp = thread_match.group(1)
                current_thread = thread_match.group(2)
                current_log_level = thread_match.group(3)
            
            # Bottleneck Resolved ì„¹ì…˜ ì‹œì‘ (Chunking í¬í•¨ ë²„ì „ë„ ì²˜ë¦¬)
            if 'Bottleneck Resolved' in line and current_thread:
                bottleneck_match = re.search(bottleneck_pattern, line)
                if bottleneck_match:
                    version = bottleneck_match.group(1)
                    
                    # (Chunking) ë“±ì˜ ì¶”ê°€ ì •ë³´ ê°ì§€
                    if '(Chunking)' in line:
                        version_suffix = 'Chunking'
                    else:
                        version_suffix = 'Standard'
                    
                    # ìƒˆë¡œìš´ ì„±ëŠ¥ ì¸¡ì • ì‹œì‘
                    current_thread_info[current_thread] = {
                        'timestamp': current_timestamp,
                        'log_level': current_log_level,
                        'version': version,
                        'version_type': version_suffix,
                        'line_number': line_num
                    }
            
            # ì´ ì†Œìš” ì‹œê°„
            elif 'ì´ ì†Œìš” ì‹œê°„' in line and current_thread:
                total_match = re.search(total_time_pattern, line)
                if total_match and current_thread in current_thread_info:
                    current_thread_info[current_thread]['total_time_ms'] = int(total_match.group(1))
            
            # RDB ì¡°íšŒ ì‹œê°„ (Bulk ë˜ëŠ” Chunk Sum)
            elif 'RDB ì¡°íšŒ ì‹œê°„' in line and current_thread:
                rdb_match = re.search(rdb_time_pattern, line)
                if rdb_match and current_thread in current_thread_info:
                    current_thread_info[current_thread]['rdb_time_ms'] = int(rdb_match.group(1))
                    current_thread_info[current_thread]['rdb_percentage'] = float(rdb_match.group(2))
                    
                    # RDB íƒ€ì… êµ¬ë¶„
                    if 'Chunk Sum' in line:
                        current_thread_info[current_thread]['rdb_type'] = 'Chunk Sum'
                    else:
                        current_thread_info[current_thread]['rdb_type'] = 'Bulk'
            
            # RDB í˜¸ì¶œ íšŸìˆ˜
            elif 'RDB í˜¸ì¶œ íšŸìˆ˜' in line and current_thread:
                count_match = re.search(rdb_count_pattern, line)
                if count_match and current_thread in current_thread_info:
                    current_thread_info[current_thread]['rdb_call_count'] = int(count_match.group(1))
                    
                    # ì¶”ê°€ ì„¤ëª… ì •ë³´ íŒŒì‹±
                    if 'Chunk ë‹¨ìœ„ ì‹¤í–‰' in line:
                        current_thread_info[current_thread]['execution_type'] = 'Chunk'
                    elif 'ê¸°ì¡´ 25íšŒ -> 1íšŒ ê°œì„ ' in line:
                        current_thread_info[current_thread]['execution_type'] = 'Optimized'
                    else:
                        current_thread_info[current_thread]['execution_type'] = 'Standard'
                    
                    # ì´ ì‹œì ì—ì„œ í•˜ë‚˜ì˜ ì™„ì„±ëœ ë°ì´í„° ì„¸íŠ¸
                    thread_data[current_thread].append(dict(current_thread_info[current_thread]))
            
            # ìƒíƒœ ì •ë³´
            elif 'ì „ì„¸ ì§€ì—­êµ¬ ì¶”ì²œ ìš”ì²­ ì™„ë£Œ' in line and current_thread:
                status_match = re.search(status_pattern, line)
                if status_match and current_thread in current_thread_info:
                    # ê°€ì¥ ìµœê·¼ ë ˆì½”ë“œì— ìƒíƒœ ì¶”ê°€
                    if thread_data[current_thread]:
                        thread_data[current_thread][-1]['status'] = status_match.group(1)
                        thread_data[current_thread][-1]['completion_timestamp'] = current_timestamp
    
    return thread_data

def save_to_csv(thread_data, output_file):
    """
    íŒŒì‹±ëœ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ (í™•ì¥ëœ í•„ë“œ í¬í•¨)
    """
    
    # CSV í—¤ë” (ìƒˆë¡œìš´ í•„ë“œ ì¶”ê°€)
    headers = [
        'thread_name',
        'timestamp',
        'log_level',
        'version',
        'version_type',        # ì¶”ê°€: Standard/Chunking
        'total_time_ms',
        'rdb_time_ms',
        'rdb_percentage',
        'rdb_type',           # ì¶”ê°€: Bulk/Chunk Sum
        'rdb_call_count',
        'execution_type',     # ì¶”ê°€: Standard/Optimized/Chunk
        'status',
        'completion_timestamp',
        'line_number'
    ]
    
    # ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í‰íƒ„í™”
    all_records = []
    
    for thread_name, records in thread_data.items():
        for record in records:
            row = {
                'thread_name': thread_name,
                'timestamp': record.get('timestamp', ''),
                'log_level': record.get('log_level', ''),
                'version': record.get('version', ''),
                'version_type': record.get('version_type', ''),
                'total_time_ms': record.get('total_time_ms', ''),
                'rdb_time_ms': record.get('rdb_time_ms', ''),
                'rdb_percentage': record.get('rdb_percentage', ''),
                'rdb_type': record.get('rdb_type', ''),
                'rdb_call_count': record.get('rdb_call_count', ''),
                'execution_type': record.get('execution_type', ''),
                'status': record.get('status', ''),
                'completion_timestamp': record.get('completion_timestamp', ''),
                'line_number': record.get('line_number', '')
            }
            all_records.append(row)
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    all_records.sort(key=lambda x: (x['timestamp'], x['thread_name']))
    
    # CSV íŒŒì¼ ì‘ì„±
    with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=headers)
        writer.writeheader()
        writer.writerows(all_records)
    
    print(f"   ğŸ’¾ ì €ì¥: {output_file}")
    return len(all_records)

def generate_summary_statistics(thread_data, summary_file):
    """
    ìŠ¤ë ˆë“œë³„ ìš”ì•½ í†µê³„ë¥¼ ìƒì„±í•˜ì—¬ ë³„ë„ CSVë¡œ ì €ì¥
    """
    
    summary_headers = [
        'thread_name',
        'execution_count',
        'version_types',      # ì¶”ê°€: ì‚¬ìš©ëœ ë²„ì „ íƒ€ì…ë“¤
        'avg_total_time_ms',
        'min_total_time_ms',
        'max_total_time_ms',
        'avg_rdb_time_ms',
        'min_rdb_time_ms',
        'max_rdb_time_ms',
        'avg_rdb_percentage',
        'min_rdb_percentage',
        'max_rdb_percentage',
        'avg_rdb_call_count',
        'min_rdb_call_count',
        'max_rdb_call_count',
        'rdb_types_used'      # ì¶”ê°€: ì‚¬ìš©ëœ RDB íƒ€ì…ë“¤
    ]
    
    summary_data = []
    
    for thread_name, records in thread_data.items():
        if not records:
            continue
            
        # ìœ íš¨í•œ ë°ì´í„°ë§Œ í•„í„°ë§
        valid_records = [r for r in records if 'total_time_ms' in r and 'rdb_time_ms' in r]
        
        if not valid_records:
            continue
        
        total_times = [r['total_time_ms'] for r in valid_records]
        rdb_times = [r['rdb_time_ms'] for r in valid_records]
        rdb_percentages = [r['rdb_percentage'] for r in valid_records]
        rdb_call_counts = [r.get('rdb_call_count', 0) for r in valid_records]
        
        # ê³ ìœ í•œ íƒ€ì… ì •ë³´ ìˆ˜ì§‘
        version_types = list(set(r.get('version_type', 'Unknown') for r in valid_records))
        rdb_types = list(set(r.get('rdb_type', 'Unknown') for r in valid_records))
        
        summary = {
            'thread_name': thread_name,
            'execution_count': len(valid_records),
            'version_types': ', '.join(version_types),
            'avg_total_time_ms': round(sum(total_times) / len(total_times), 2),
            'min_total_time_ms': min(total_times),
            'max_total_time_ms': max(total_times),
            'avg_rdb_time_ms': round(sum(rdb_times) / len(rdb_times), 2),
            'min_rdb_time_ms': min(rdb_times),
            'max_rdb_time_ms': max(rdb_times),
            'avg_rdb_percentage': round(sum(rdb_percentages) / len(rdb_percentages), 2),
            'min_rdb_percentage': min(rdb_percentages),
            'max_rdb_percentage': max(rdb_percentages),
            'avg_rdb_call_count': round(sum(rdb_call_counts) / len(rdb_call_counts), 2) if rdb_call_counts else 0,
            'min_rdb_call_count': min(rdb_call_counts) if rdb_call_counts else 0,
            'max_rdb_call_count': max(rdb_call_counts) if rdb_call_counts else 0,
            'rdb_types_used': ', '.join(rdb_types)
        }
        
        summary_data.append(summary)
    
    # ìŠ¤ë ˆë“œ ì´ë¦„ìœ¼ë¡œ ì •ë ¬
    summary_data.sort(key=lambda x: x['thread_name'])
    
    # CSV íŒŒì¼ ì‘ì„±
    with open(summary_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=summary_headers)
        writer.writeheader()
        writer.writerows(summary_data)
    
    print(f"   ğŸ“Š ìš”ì•½: {summary_file}")
    return len(summary_data)

def find_log_files(directory="."):
    """
    í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ë¡œê·¸ íŒŒì¼ ì°¾ê¸°
    """
    log_files = []
    patterns = [
        "*ìŠ¤ë ˆë“œ*ë¡œê·¸*.txt",
        "*thread*log*.txt",
        "*.log",
        "*.txt"
    ]
    
    # í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  txt íŒŒì¼ ì°¾ê¸°
    for file in os.listdir(directory):
        if file.endswith('.txt') or file.endswith('.log'):
            # ìŠ¤ë ˆë“œ ë˜ëŠ” thread í‚¤ì›Œë“œê°€ í¬í•¨ëœ íŒŒì¼ ìš°ì„ 
            if 'ìŠ¤ë ˆë“œ' in file or 'thread' in file.lower() or 'ë¡œê·¸' in file or 'log' in file.lower():
                log_files.append(os.path.join(directory, file))
    
    return log_files

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ë¡œì»¬ í™˜ê²½ìš©
    """
    
    # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ìœ„ì¹˜
    script_dir = os.path.dirname(os.path.abspath(__file__))
    os.chdir(script_dir)
    
    print("=" * 80)
    print("ìŠ¤ë ˆë“œ ì„±ëŠ¥ ë¡œê·¸ íŒŒì‹± ë„êµ¬ v2.0 (ë¡œì»¬ ë²„ì „)")
    print("=" * 80)
    print(f"ğŸ“ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    print("-" * 80)
    
    # ë¡œê·¸ íŒŒì¼ ì°¾ê¸°
    log_files = []
    
    # 1. ëª…ë ¹í–‰ ì¸ìë¡œ íŒŒì¼ì´ ì§€ì •ëœ ê²½ìš°
    if len(sys.argv) > 1:
        for arg in sys.argv[1:]:
            if os.path.exists(arg):
                log_files.append(arg)
            else:
                print(f"âš ï¸  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {arg}")
    
    # 2. ì¸ìê°€ ì—†ìœ¼ë©´ í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ìë™ íƒìƒ‰
    if not log_files:
        log_files = find_log_files()
        
        if not log_files:
            print("âŒ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            print("\nì‚¬ìš©ë²•:")
            print("  1. ë°©ë²• 1: python extract_thread_logs_v2_local.py [ë¡œê·¸íŒŒì¼ëª…]")
            print("  2. ë°©ë²• 2: ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”ì— .txt ë¡œê·¸ íŒŒì¼ì„ ë‘ê³  ì‹¤í–‰")
            print("\nì˜ˆì‹œ:")
            print("  python extract_thread_logs_v2_local.py 3ì°¨_í…ŒìŠ¤íŠ¸_50ê°œ_ìŠ¤ë ˆë“œ_í…ŒìŠ¤íŠ¸_ê²°ê³¼_ë¡œê·¸.txt")
            return
        
        print(f"\nğŸ” ë°œê²¬ëœ ë¡œê·¸ íŒŒì¼:")
        for i, file in enumerate(log_files, 1):
            print(f"   {i}. {os.path.basename(file)}")
        
        # ì‚¬ìš©ì í™•ì¸
        if len(log_files) > 1:
            print(f"\nì´ {len(log_files)}ê°œ íŒŒì¼ì„ ëª¨ë‘ ì²˜ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ", end='')
            response = input().strip().lower()
            if response and response not in ['y', 'yes', '']:
                print("ì²˜ë¦¬ ì·¨ì†Œ")
                return
    
    print("\n" + "=" * 80)
    
    # ê° ë¡œê·¸ íŒŒì¼ ì²˜ë¦¬
    for log_file_path in log_files:
        if not os.path.exists(log_file_path):
            print(f"âš ï¸  íŒŒì¼ ì—†ìŒ: {log_file_path}")
            continue
        
        # íŒŒì¼ëª…ì—ì„œ ì¶œë ¥ ì´ë¦„ ìƒì„±
        base_name = os.path.basename(log_file_path)
        name_without_ext = os.path.splitext(base_name)[0]
        output_csv_path = f'{name_without_ext}_v2_data.csv'
        summary_csv_path = f'{name_without_ext}_v2_summary.csv'
        
        print(f"\nğŸ“‹ ì²˜ë¦¬ ì¤‘: {base_name}")
        print("-" * 40)
        
        try:
            # ë¡œê·¸ íŒŒì¼ íŒŒì‹±
            thread_data = parse_log_file(log_file_path)
            
            # íŒŒì‹± ê²°ê³¼ ì¶œë ¥
            total_threads = len(thread_data)
            total_records = sum(len(records) for records in thread_data.values())
            
            if total_records == 0:
                print("   âš ï¸  íŒŒì‹±ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            print(f"\nâœ… íŒŒì‹± ì™„ë£Œ:")
            print(f"   - ìŠ¤ë ˆë“œ ìˆ˜: {total_threads}")
            print(f"   - ì´ ë ˆì½”ë“œ ìˆ˜: {total_records}")
            
            # ë²„ì „ íƒ€ì… í†µê³„
            all_version_types = set()
            all_rdb_types = set()
            for records in thread_data.values():
                for record in records:
                    all_version_types.add(record.get('version_type', 'Unknown'))
                    all_rdb_types.add(record.get('rdb_type', 'Unknown'))
            
            print(f"   - ê°ì§€ëœ ë²„ì „: {', '.join(all_version_types)}")
            print(f"   - RDB íƒ€ì…: {', '.join(all_rdb_types)}")
            
            # CSV ì €ì¥
            print(f"\nğŸ“ íŒŒì¼ ìƒì„±:")
            records_saved = save_to_csv(thread_data, output_csv_path)
            summary_count = generate_summary_statistics(thread_data, summary_csv_path)
            
            # ê°„ë‹¨í•œ í†µê³„
            all_total_times = []
            all_rdb_times = []
            all_rdb_percentages = []
            all_rdb_call_counts = []
            
            for records in thread_data.values():
                for record in records:
                    if 'total_time_ms' in record:
                        all_total_times.append(record['total_time_ms'])
                    if 'rdb_time_ms' in record:
                        all_rdb_times.append(record['rdb_time_ms'])
                    if 'rdb_percentage' in record:
                        all_rdb_percentages.append(record['rdb_percentage'])
                    if 'rdb_call_count' in record:
                        all_rdb_call_counts.append(record['rdb_call_count'])
            
            print(f"\nğŸ“Š í†µê³„ ìš”ì•½:")
            if all_total_times:
                print(f"   ì´ ì²˜ë¦¬ ì‹œê°„: í‰ê·  {sum(all_total_times)/len(all_total_times):.2f}ms")
                print(f"                 (ìµœì†Œ {min(all_total_times)}ms / ìµœëŒ€ {max(all_total_times)}ms)")
            
            if all_rdb_times:
                print(f"   RDB ì¡°íšŒ ì‹œê°„: í‰ê·  {sum(all_rdb_times)/len(all_rdb_times):.2f}ms")
                print(f"                  (ìµœì†Œ {min(all_rdb_times)}ms / ìµœëŒ€ {max(all_rdb_times)}ms)")
            
            if all_rdb_percentages:
                print(f"   RDB ì‹œê°„ ë¹„ìœ¨: í‰ê·  {sum(all_rdb_percentages)/len(all_rdb_percentages):.2f}%")
            
            if all_rdb_call_counts:
                print(f"   RDB í˜¸ì¶œ íšŸìˆ˜: í‰ê·  {sum(all_rdb_call_counts)/len(all_rdb_call_counts):.2f}íšŒ")
                print(f"                  (ìµœì†Œ {min(all_rdb_call_counts)}íšŒ / ìµœëŒ€ {max(all_rdb_call_counts)}íšŒ)")
        
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 80)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("=" * 80)

if __name__ == "__main__":
    main()
